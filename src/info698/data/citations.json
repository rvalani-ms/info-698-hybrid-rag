{
    "https://openalex.org/W4385245566": {
        "https://openalex.org/W3138516171": {
            "title": "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows",
            "openalex_id": "https://openalex.org/W3138516171",
            "cited_by_count": 20564,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4390494008",
                "https://openalex.org/W4285411112",
                "https://openalex.org/W3135697610",
                "https://openalex.org/W2922442631",
                "https://openalex.org/W2171299904",
                "https://openalex.org/W2168523118",
                "https://openalex.org/W2085033728",
                "https://openalex.org/W2053596378",
                "https://openalex.org/W1647606319",
                "https://openalex.org/W1522196789"
            ],
            "references": [
                "https://openalex.org/W1522301498",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1928278792",
                "https://openalex.org/W2010315761",
                "https://openalex.org/W2086161653",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2112796928"
            ],
            "authors": [
                "Ze Liu",
                "Yutong Lin",
                "Yue Cao",
                "Han Hu",
                "Yixuan Wei",
                "Zheng Zhang",
                "Stephen Lin",
                "Baining Guo"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00986",
            "concepts": [
                "Transformer",
                "Computer science",
                "Segmentation",
                "Artificial intelligence",
                "Computation",
                "Pixel",
                "Architecture",
                "Image segmentation",
                "Computer vision",
                "Algorithm",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Art",
                "Visual arts"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/2103.14030",
            "abstract": "This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with Shifted windows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at https://github.com/microsoft/Swin-Transformer."
        },
        "https://openalex.org/W2963420686": {
            "title": "Squeeze-and-Excitation Networks",
            "openalex_id": "https://openalex.org/W2963420686",
            "cited_by_count": 11917,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W4396701345",
                "https://openalex.org/W4396696052",
                "https://openalex.org/W4391913857",
                "https://openalex.org/W4391375266",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2390279801",
                "https://openalex.org/W2376932109",
                "https://openalex.org/W2358668433",
                "https://openalex.org/W2001405890"
            ],
            "references": [
                "https://openalex.org/W1026270304",
                "https://openalex.org/W139960808",
                "https://openalex.org/W1514535095",
                "https://openalex.org/W1568165162",
                "https://openalex.org/W1581066146",
                "https://openalex.org/W1665214252",
                "https://openalex.org/W1677182931",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1861492603"
            ],
            "authors": [
                "Jie Hu",
                "Li Shen",
                "Samuel Albanie",
                "Gang Sun",
                "Enhua Wu"
            ],
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "doi": "https://doi.org/10.1109/tpami.2019.2913372",
            "concepts": [
                "Computer science",
                "Artificial intelligence"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/1709.01507",
            "abstract": "The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the \"Squeeze-and-Excitation\" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251 percent, surpassing the winning entry of 2016 by a relative improvement of -25 percent. Models and code are available at https://github.com/hujie-frank/SENet."
        },
        "https://openalex.org/W4312933868": {
            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
            "openalex_id": "https://openalex.org/W4312933868",
            "cited_by_count": 7793,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W49967185",
                "https://openalex.org/W425542480",
                "https://openalex.org/W3178025616",
                "https://openalex.org/W3035059915",
                "https://openalex.org/W2946160871",
                "https://openalex.org/W2907830442",
                "https://openalex.org/W2131831293",
                "https://openalex.org/W2060947339",
                "https://openalex.org/W2017457812",
                "https://openalex.org/W1995073329"
            ],
            "references": [
                "https://openalex.org/W1583912456",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1909320841",
                "https://openalex.org/W1959608418",
                "https://openalex.org/W2031342017",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2125389028",
                "https://openalex.org/W2129069237"
            ],
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01042",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Pixel",
                "Inference",
                "Inpainting",
                "Image translation",
                "Computer vision",
                "Image (mathematics)"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2112.10752",
            "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
        },
        "https://openalex.org/W2955058313": {
            "title": "Dual Attention Network for Scene Segmentation",
            "openalex_id": "https://openalex.org/W2955058313",
            "cited_by_count": 6081,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W4385222618",
                "https://openalex.org/W4286681602",
                "https://openalex.org/W3209312100",
                "https://openalex.org/W3167388719",
                "https://openalex.org/W2907527313",
                "https://openalex.org/W2159686533",
                "https://openalex.org/W2100576949",
                "https://openalex.org/W2069133146",
                "https://openalex.org/W2017545316",
                "https://openalex.org/W1663079876"
            ],
            "references": [
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1909234690",
                "https://openalex.org/W1961891967",
                "https://openalex.org/W2031489346",
                "https://openalex.org/W2125215748",
                "https://openalex.org/W2340897893",
                "https://openalex.org/W2412782625",
                "https://openalex.org/W2558580397",
                "https://openalex.org/W2560023338"
            ],
            "authors": [
                "Jun Fu",
                "Jing Liu",
                "Haijie Tian",
                "Yong Li",
                "Yongjun Bao",
                "Zhiwei Fang",
                "Hanqing Lu"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr.2019.00326",
            "concepts": [
                "Computer science",
                "Pascal (unit)",
                "Segmentation",
                "Artificial intelligence",
                "Attention network",
                "Context (archaeology)",
                "Channel (broadcasting)",
                "Dual (grammatical number)",
                "Feature (linguistics)",
                "Pattern recognition (psychology)",
                "Fusion mechanism",
                "Representation (politics)",
                "Fusion",
                "Art",
                "Paleontology",
                "Computer network",
                "Linguistics",
                "Philosophy",
                "Literature",
                "Lipid bilayer fusion",
                "Politics",
                "Political science",
                "Law",
                "Biology",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/1809.02983",
            "abstract": "In this paper, we address the scene segmentation task by capturing rich contextual dependencies based on the self-attention mechanism. Unlike previous works that capture contexts by multi-scale features fusion, we propose a Dual Attention Networks (DANet) to adaptively integrate local features with their global dependencies. Specifically, we append two types of attention modules on top of traditional dilated FCN, which model the semantic interdependencies in spatial and channel dimensions respectively. The position attention module selectively aggregates the features at each position by a weighted sum of the features at all positions. Similar features would be related to each other regardless of their distances. Meanwhile, the channel attention module selectively emphasizes interdependent channel maps by integrating associated features among all channel maps. We sum the outputs of the two attention modules to further improve feature representation which contributes to more precise segmentation results. We achieve new state-of-the-art segmentation performance on three challenging scene segmentation datasets, i.e., Cityscapes, PASCAL Context and COCO Stuff dataset. In particular, a Mean IoU score of 81.5% on Cityscapes test set is achieved without using coarse data."
        },
        "https://openalex.org/W2911489562": {
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "openalex_id": "https://openalex.org/W2911489562",
            "cited_by_count": 5660,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W4387356431",
                "https://openalex.org/W4308171175",
                "https://openalex.org/W3185751515",
                "https://openalex.org/W3111301126",
                "https://openalex.org/W3045642779",
                "https://openalex.org/W2963862093",
                "https://openalex.org/W2572241437",
                "https://openalex.org/W2527712604",
                "https://openalex.org/W2489933339",
                "https://openalex.org/W2057069926"
            ],
            "references": [
                "https://openalex.org/W1981208470",
                "https://openalex.org/W2047782770",
                "https://openalex.org/W2052217781",
                "https://openalex.org/W2071879021",
                "https://openalex.org/W2100627415",
                "https://openalex.org/W2136437513",
                "https://openalex.org/W2149369282",
                "https://openalex.org/W2153579005",
                "https://openalex.org/W2154142897",
                "https://openalex.org/W2168041406"
            ],
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang"
            ],
            "venue": "Bioinformatics",
            "doi": "https://doi.org/10.1093/bioinformatics/btz682",
            "concepts": [
                "Biomedical text mining",
                "Computer science",
                "Artificial intelligence",
                "Natural language processing",
                "Language model",
                "Named-entity recognition",
                "Relationship extraction",
                "Text mining",
                "Text corpus",
                "Representation (politics)",
                "F1 score",
                "Domain (mathematical analysis)",
                "Source code",
                "Information extraction",
                "Mathematical analysis",
                "Mathematics",
                "Management",
                "Politics",
                "Political science",
                "Law",
                "Economics",
                "Task (project management)",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/1901.08746",
            "abstract": "Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert."
        },
        "https://openalex.org/W4313156423": {
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "openalex_id": "https://openalex.org/W4313156423",
            "cited_by_count": 4756,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4390516098",
                "https://openalex.org/W4285411112",
                "https://openalex.org/W4235240664",
                "https://openalex.org/W3081694532",
                "https://openalex.org/W2965083567",
                "https://openalex.org/W2757182831",
                "https://openalex.org/W2389214306",
                "https://openalex.org/W2171299904",
                "https://openalex.org/W2085033728",
                "https://openalex.org/W1838576100"
            ],
            "references": [
                "https://openalex.org/W1533861849",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2025768430",
                "https://openalex.org/W2063971957",
                "https://openalex.org/W2102409316",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2134670479",
                "https://openalex.org/W2138621090"
            ],
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01553",
            "concepts": [
                "Computer science",
                "Scalability",
                "Artificial intelligence",
                "Encoder",
                "Masking (illustration)",
                "Representation (politics)",
                "Image (mathematics)",
                "Feature learning",
                "Pixel",
                "Pattern recognition (psychology)",
                "Computer vision",
                "Art",
                "Database",
                "Politics",
                "Political science",
                "Law",
                "Visual arts",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/2111.06377",
            "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3\u00d7 or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior."
        },
        "https://openalex.org/W4312443924": {
            "title": "A ConvNet for the 2020s",
            "openalex_id": "https://openalex.org/W4312443924",
            "cited_by_count": 4351,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W97075385",
                "https://openalex.org/W4235240664",
                "https://openalex.org/W2965083567",
                "https://openalex.org/W2389214306",
                "https://openalex.org/W2357523926",
                "https://openalex.org/W2095886385",
                "https://openalex.org/W2089704382",
                "https://openalex.org/W1983399550",
                "https://openalex.org/W1838576100",
                "https://openalex.org/W1522196789"
            ],
            "references": [
                "https://openalex.org/W1536680647",
                "https://openalex.org/W1665214252",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2056695679",
                "https://openalex.org/W2077513643",
                "https://openalex.org/W2086161653",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2102605133",
                "https://openalex.org/W2108598243"
            ],
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01167",
            "concepts": [
                "Transformer",
                "Computer science",
                "Artificial intelligence",
                "Segmentation",
                "Scalability",
                "Object detection",
                "Machine learning",
                "Image segmentation",
                "Computer vision",
                "Pattern recognition (psychology)",
                "Engineering",
                "Electrical engineering",
                "Voltage",
                "Database"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets."
        },
        "https://openalex.org/W3177318507": {
            "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
            "openalex_id": "https://openalex.org/W3177318507",
            "cited_by_count": 3690,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W65617392",
                "https://openalex.org/W63071447",
                "https://openalex.org/W4231964008",
                "https://openalex.org/W2888625260",
                "https://openalex.org/W2589098947",
                "https://openalex.org/W2385621972",
                "https://openalex.org/W2351267244",
                "https://openalex.org/W2275988210",
                "https://openalex.org/W2169928498",
                "https://openalex.org/W1529400504"
            ],
            "references": [
                "https://openalex.org/W1969852690",
                "https://openalex.org/W2054685200",
                "https://openalex.org/W2130942839",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2319170717",
                "https://openalex.org/W2549483845",
                "https://openalex.org/W2604847698",
                "https://openalex.org/W2613328025",
                "https://openalex.org/W2757354914",
                "https://openalex.org/W2762309767"
            ],
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "doi": "https://doi.org/10.1609/aaai.v35i12.17325",
            "concepts": [
                "Transformer",
                "Computer science",
                "Encoder",
                "Sequence (biology)",
                "Dependency (UML)",
                "Algorithm",
                "Artificial intelligence",
                "Engineering",
                "Voltage",
                "Biology",
                "Electrical engineering",
                "Genetics",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17325/17132",
            "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem."
        },
        "https://openalex.org/W3131500599": {
            "title": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions",
            "openalex_id": "https://openalex.org/W3131500599",
            "cited_by_count": 3392,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W3155393898",
                "https://openalex.org/W2993291555",
                "https://openalex.org/W2349160795",
                "https://openalex.org/W2159686533",
                "https://openalex.org/W2135595438",
                "https://openalex.org/W2100576949",
                "https://openalex.org/W2100057527",
                "https://openalex.org/W2069133146",
                "https://openalex.org/W2017545316",
                "https://openalex.org/W1663079876"
            ],
            "references": [
                "https://openalex.org/W1533861849",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1745334888",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W2031489346",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2112796928"
            ],
            "authors": [
                "Wenhai Wang",
                "Enze Xie",
                "Xiang Li",
                "Deng-Ping Fan",
                "Kaitao Song",
                "Ding Liang",
                "Tong L\u00fc",
                "Ping Luo",
                "Ling Shao"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00061",
            "concepts": [
                "Computer science",
                "Transformer",
                "Convolutional neural network",
                "Artificial intelligence",
                "Pixel",
                "Image resolution",
                "Backbone network",
                "Pascal (unit)",
                "Computation",
                "Segmentation",
                "Pattern recognition (psychology)",
                "Computer vision",
                "Computer engineering",
                "Algorithm",
                "Voltage",
                "Computer network",
                "Physics",
                "Quantum mechanics",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2102.12122",
            "abstract": "Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research."
        },
        "https://openalex.org/W3159481202": {
            "title": "Emerging Properties in Self-Supervised Vision Transformers",
            "openalex_id": "https://openalex.org/W3159481202",
            "cited_by_count": 3288,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4390516098",
                "https://openalex.org/W4390062853",
                "https://openalex.org/W4389256085",
                "https://openalex.org/W4313644201",
                "https://openalex.org/W4285328440",
                "https://openalex.org/W4205302943",
                "https://openalex.org/W2561132942",
                "https://openalex.org/W2384362569",
                "https://openalex.org/W2181948922",
                "https://openalex.org/W2142795561"
            ],
            "references": [
                "https://openalex.org/W1821462560",
                "https://openalex.org/W2020308406",
                "https://openalex.org/W2031489346",
                "https://openalex.org/W2086161653",
                "https://openalex.org/W2100664567",
                "https://openalex.org/W2117539524",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2134670479",
                "https://openalex.org/W2144796873",
                "https://openalex.org/W2148809531"
            ],
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 Je\u01f5ou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00951",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Machine learning",
                "Transformer",
                "Segmentation",
                "Supervised learning",
                "Encoder",
                "Pattern recognition (psychology)",
                "Artificial neural network",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://hal.science/hal-03323359/document",
            "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base."
        },
        "https://openalex.org/W4323655724": {
            "title": "ChatGPT for good? On opportunities and challenges of large language models for education",
            "openalex_id": "https://openalex.org/W4323655724",
            "cited_by_count": 3094,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4362576712",
                "https://openalex.org/W4312355418",
                "https://openalex.org/W2387560707",
                "https://openalex.org/W2384329035",
                "https://openalex.org/W2373380871",
                "https://openalex.org/W2370820329",
                "https://openalex.org/W2370554813",
                "https://openalex.org/W2363525455",
                "https://openalex.org/W2348562106",
                "https://openalex.org/W2314810092"
            ],
            "references": [
                "https://openalex.org/W1583837637",
                "https://openalex.org/W2783539613",
                "https://openalex.org/W2896457183",
                "https://openalex.org/W2937444804",
                "https://openalex.org/W2955088691",
                "https://openalex.org/W2958068682",
                "https://openalex.org/W2965373594",
                "https://openalex.org/W2969717756",
                "https://openalex.org/W2970597249",
                "https://openalex.org/W3085139254"
            ],
            "authors": [
                "Enkelejda Kasneci",
                "Kathrin Se\u00dfler",
                "Stefan K\u00fcchemann",
                "Maria Bannert",
                "Daryna Dementieva",
                "Frank Fischer",
                "Urs Gasser",
                "Georg Groh",
                "Stephan G\u00fcnnemann",
                "Eyke H\u00fcllermeier",
                "Stephan Krusche",
                "Gitta Kutyniok",
                "Tilman Michaeli",
                "Claudia Nerdel",
                "J\u00fcrgen Pfeffer",
                "Oleksandra Poquet",
                "Michael Sailer",
                "Albrecht Schmidt",
                "Tina Seidel",
                "Matthias Stadler",
                "J. Weller",
                "Jochen K\u00fchn",
                "Gjergji Kasneci"
            ],
            "venue": "Learning and Individual Differences",
            "doi": "https://doi.org/10.1016/j.lindif.2023.102274",
            "concepts": [
                "Curriculum",
                "Computer science",
                "Field (mathematics)",
                "Engineering ethics",
                "Knowledge management",
                "Management science",
                "Psychology",
                "Pedagogy",
                "Engineering",
                "Mathematics",
                "Pure mathematics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.lindif.2023.102274"
        },
        "https://openalex.org/W3170841864": {
            "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
            "openalex_id": "https://openalex.org/W3170841864",
            "cited_by_count": 2910,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4286681602",
                "https://openalex.org/W4242339654",
                "https://openalex.org/W3209312100",
                "https://openalex.org/W2349160795",
                "https://openalex.org/W2159686533",
                "https://openalex.org/W2100576949",
                "https://openalex.org/W2069133146",
                "https://openalex.org/W2017545316",
                "https://openalex.org/W1663079876",
                "https://openalex.org/W1522196789"
            ],
            "references": [
                "https://openalex.org/W1610060839",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1745334888",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1923697677",
                "https://openalex.org/W2124592697",
                "https://openalex.org/W2125215748",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2340897893"
            ],
            "authors": [
                "Sixiao Zheng",
                "Jiachen Lu",
                "Hengshuang Zhao",
                "Xiatian Zhu",
                "Zekun Luo",
                "Yabiao Wang",
                "Yanwei Fu",
                "Jianfeng Feng",
                "Tao Xiang",
                "Philip H. S. Torr",
                "Li Zhang"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr46437.2021.00681",
            "concepts": [
                "Computer science",
                "Segmentation",
                "Encoder",
                "Transformer",
                "Artificial intelligence",
                "Pascal (unit)",
                "Image segmentation",
                "Computer vision",
                "Pattern recognition (psychology)",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Programming language",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2012.15840",
            "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission."
        },
        "https://openalex.org/W2884001105": {
            "title": "Recent Trends in Deep Learning Based Natural Language Processing [Review Article]",
            "openalex_id": "https://openalex.org/W2884001105",
            "cited_by_count": 2623,
            "publication_year": 2018,
            "related_works": [
                "https://openalex.org/W54497855",
                "https://openalex.org/W4380075502",
                "https://openalex.org/W3125814499",
                "https://openalex.org/W3121970507",
                "https://openalex.org/W2565703248",
                "https://openalex.org/W217960748",
                "https://openalex.org/W2110028391",
                "https://openalex.org/W2090827041",
                "https://openalex.org/W2032233321",
                "https://openalex.org/W187246281"
            ],
            "references": [
                "https://openalex.org/W10957333",
                "https://openalex.org/W1423339008",
                "https://openalex.org/W1486649854",
                "https://openalex.org/W1499864241",
                "https://openalex.org/W1514535095",
                "https://openalex.org/W1517386993",
                "https://openalex.org/W1518951372",
                "https://openalex.org/W1591706642",
                "https://openalex.org/W1597941798",
                "https://openalex.org/W1614298861"
            ],
            "authors": [
                "Tom Young",
                "Devamanyu Hazarika",
                "Soujanya Poria",
                "Erik Cambria"
            ],
            "venue": "IEEE Computational Intelligence Magazine",
            "doi": "https://doi.org/10.1109/mci.2018.2840738",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Deep learning",
                "Context (archaeology)",
                "Variety (cybernetics)",
                "Natural language processing",
                "Machine learning",
                "Biology",
                "Paleontology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP."
        },
        "https://openalex.org/W2981689412": {
            "title": "CCNet: Criss-Cross Attention for Semantic Segmentation",
            "openalex_id": "https://openalex.org/W2981689412",
            "cited_by_count": 2556,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W915438175",
                "https://openalex.org/W4321353415",
                "https://openalex.org/W4246352526",
                "https://openalex.org/W4230315250",
                "https://openalex.org/W2745001401",
                "https://openalex.org/W2378211422",
                "https://openalex.org/W2130974462",
                "https://openalex.org/W2121910908",
                "https://openalex.org/W2086519370",
                "https://openalex.org/W2028665553"
            ],
            "references": [
                "https://openalex.org/W1548976575",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1923697677",
                "https://openalex.org/W2122122381",
                "https://openalex.org/W2124592697",
                "https://openalex.org/W2167222293",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2267186426"
            ],
            "authors": [
                "Zilong Huang",
                "Xinggang Wang",
                "Lichao Huang",
                "Chang Huang",
                "Yunchao Wei",
                "Wenyu Liu"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv.2019.00069",
            "concepts": [
                "Computer science",
                "Benchmark (surveying)",
                "Pixel",
                "Block (permutation group theory)",
                "Code (set theory)",
                "Artificial intelligence",
                "Segmentation",
                "Set (abstract data type)",
                "Image segmentation",
                "Semantics (computer science)",
                "Pattern recognition (psychology)",
                "Programming language",
                "Mathematics",
                "Geometry",
                "Geodesy",
                "Geography"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/1811.11721",
            "abstract": "Full-image dependencies provide useful contextual information to benefit visual understanding problems. In this work, we propose a Criss-Cross Network (CCNet) for obtaining such contextual information in a more effective and efficient way. Concretely, for each pixel, a novel criss-cross attention module in CCNet harvests the contextual information of all the pixels on its criss-cross path. By taking a further recurrent operation, each pixel can finally capture the full-image dependencies from all pixels. Overall, CCNet is with the following merits: 1) GPU memory friendly. Compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less GPU memory usage. 2) High computational efficiency. The recurrent criss-cross attention significantly reduces FLOPs by about 85% of the non-local block in computing full-image dependencies. 3) The state-of-the-art performance. We conduct extensive experiments on popular semantic segmentation benchmarks including Cityscapes, ADE20K, and instance segmentation benchmark COCO. In particular, our CCNet achieves the mIoU score of 81.4 and 45.22 on Cityscapes test set and ADE20K validation set, respectively, which are the new state-of-the-art results. The source code is available at https://github.com/speedinghzl/CCNet."
        },
        "https://openalex.org/W4213019189": {
            "title": "A Survey on Vision Transformer",
            "openalex_id": "https://openalex.org/W4213019189",
            "cited_by_count": 2527,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4391266461",
                "https://openalex.org/W4321487865",
                "https://openalex.org/W4313906399",
                "https://openalex.org/W4312273141",
                "https://openalex.org/W4310274968",
                "https://openalex.org/W4293226380",
                "https://openalex.org/W2590798552",
                "https://openalex.org/W1810370127",
                "https://openalex.org/W1594946127",
                "https://openalex.org/W1587378402"
            ],
            "references": [
                "https://openalex.org/W1598730426",
                "https://openalex.org/W1783366411",
                "https://openalex.org/W1821462560",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W2011815965",
                "https://openalex.org/W2025768430",
                "https://openalex.org/W2027377866",
                "https://openalex.org/W2053619738",
                "https://openalex.org/W2064296540"
            ],
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Hanting Chen",
                "Xinghao Chen",
                "Jianyuan Guo",
                "Zhenhua Liu",
                "Yehui Tang",
                "An Xiao",
                "Chunjing Xu",
                "Yixing Xu",
                "Zhaohui Yang",
                "Yiman Zhang",
                "Dacheng Tao"
            ],
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "doi": "https://doi.org/10.1109/tpami.2022.3152247",
            "concepts": [
                "Transformer",
                "Computer science",
                "Artificial intelligence",
                "Convolutional neural network",
                "Inductive bias",
                "Machine vision",
                "Artificial neural network",
                "Machine learning",
                "Computer vision",
                "Engineering",
                "Multi-task learning",
                "Electrical engineering",
                "Systems engineering",
                "Voltage",
                "Task (project management)"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2012.12556",
            "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers."
        },
        "https://openalex.org/W4206706211": {
            "title": "Transformers in Vision: A Survey",
            "openalex_id": "https://openalex.org/W4206706211",
            "cited_by_count": 2278,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W97075385",
                "https://openalex.org/W4235240664",
                "https://openalex.org/W2965083567",
                "https://openalex.org/W2889616422",
                "https://openalex.org/W2389214306",
                "https://openalex.org/W2357523926",
                "https://openalex.org/W2095886385",
                "https://openalex.org/W2089704382",
                "https://openalex.org/W1983399550",
                "https://openalex.org/W1838576100"
            ],
            "references": [
                "https://openalex.org/W1673923490",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1895577753",
                "https://openalex.org/W1920022804",
                "https://openalex.org/W1933349210",
                "https://openalex.org/W1959608418",
                "https://openalex.org/W2064675550",
                "https://openalex.org/W2097073572",
                "https://openalex.org/W2101032778",
                "https://openalex.org/W2108598243"
            ],
            "authors": [
                "Salman Khan",
                "Muzammal Naseer",
                "Munawar Hayat",
                "Syed Waqas Zamir",
                "Fahad Shahbaz Khan",
                "Mubarak Shah"
            ],
            "venue": "ACM Computing Surveys",
            "doi": "https://doi.org/10.1145/3505244",
            "concepts": [
                "Computer science",
                "Transformer",
                "Segmentation",
                "Artificial intelligence",
                "Image processing",
                "Scalability",
                "Computer vision",
                "Database",
                "Physics",
                "Quantum mechanics",
                "Voltage",
                "Image (mathematics)"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/2101.01169",
            "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works."
        },
        "https://openalex.org/W2966926453": {
            "title": "Deformable ConvNets V2: More Deformable, Better Results",
            "openalex_id": "https://openalex.org/W2966926453",
            "cited_by_count": 2184,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W4321487865",
                "https://openalex.org/W4313906399",
                "https://openalex.org/W4293226380",
                "https://openalex.org/W4239306820",
                "https://openalex.org/W3019910406",
                "https://openalex.org/W2969228573",
                "https://openalex.org/W2964954556",
                "https://openalex.org/W2947043951",
                "https://openalex.org/W2811106690",
                "https://openalex.org/W2318112981"
            ],
            "references": [
                "https://openalex.org/W1536680647",
                "https://openalex.org/W1821462560",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2031489346",
                "https://openalex.org/W2102605133",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2117228865",
                "https://openalex.org/W2124386111",
                "https://openalex.org/W2134797427",
                "https://openalex.org/W2168356304"
            ],
            "authors": [
                "Xizhou Zhu",
                "Han Hu",
                "Stephen Lin",
                "Jifeng Dai"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr.2019.00953",
            "concepts": [
                "Computer science",
                "Focus (optics)",
                "Artificial intelligence",
                "Benchmark (surveying)",
                "Convolution (computer science)",
                "Feature (linguistics)",
                "Segmentation",
                "Object detection",
                "Convolutional neural network",
                "Feature extraction",
                "Pattern recognition (psychology)",
                "Artificial neural network",
                "Object (grammar)",
                "Computer vision",
                "Image segmentation",
                "Image (mathematics)",
                "Linguistics",
                "Philosophy",
                "Physics",
                "Geodesy",
                "Optics",
                "Geography"
            ],
            "type": "preprint",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/1811.11168",
            "abstract": "The superior performance of Deformable Convolutional Networks arises from its ability to adapt to the geometric variations of objects. Through an examination of its adaptive behavior, we observe that while the spatial support for its neural features conforms more closely than regular ConvNets to object structure, this support may nevertheless extend well beyond the region of interest, causing features to be influenced by irrelevant image content. To address this problem, we present a reformulation of Deformable ConvNets that improves its ability to focus on pertinent image regions, through increased modeling power and stronger training. The modeling power is enhanced through a more comprehensive integration of deformable convolution within the network, and by introducing a modulation mechanism that expands the scope of deformation modeling. To effectively harness this enriched modeling capability, we guide network training via a proposed feature mimicking scheme that helps the network to learn features that reflect the object focus and classification power of R-CNN features. With the proposed contributions, this new version of Deformable ConvNets yields significant performance gains over the original model and produces leading results on the COCO benchmark for object detection and instance segmentation."
        },
        "https://openalex.org/W2963367478": {
            "title": "Self-Attentive Sequential Recommendation",
            "openalex_id": "https://openalex.org/W2963367478",
            "cited_by_count": 2130,
            "publication_year": 2018,
            "related_works": [
                "https://openalex.org/W4390273403",
                "https://openalex.org/W4386781444",
                "https://openalex.org/W4225394202",
                "https://openalex.org/W3197542405",
                "https://openalex.org/W3092950680",
                "https://openalex.org/W2364370872",
                "https://openalex.org/W2294335174",
                "https://openalex.org/W2150182025",
                "https://openalex.org/W2097963413",
                "https://openalex.org/W2053269318"
            ],
            "references": [
                "https://openalex.org/W1485147275",
                "https://openalex.org/W1500188831",
                "https://openalex.org/W1514535095",
                "https://openalex.org/W1522301498",
                "https://openalex.org/W1690919088",
                "https://openalex.org/W1720514416",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1849277567",
                "https://openalex.org/W1899504021",
                "https://openalex.org/W1985854669"
            ],
            "authors": [
                "Wang-Cheng Kang",
                "Julian McAuley"
            ],
            "venue": "2021 IEEE International Conference on Data Mining (ICDM)",
            "doi": "https://doi.org/10.1109/icdm.2018.00035",
            "concepts": [
                "Computer science",
                "Recurrent neural network",
                "Artificial intelligence",
                "Recommender system",
                "Semantics (computer science)",
                "Context (archaeology)",
                "Machine learning",
                "Markov chain",
                "Hidden Markov model",
                "Feature (linguistics)",
                "Markov process",
                "Artificial neural network",
                "Paleontology",
                "Linguistics",
                "Philosophy",
                "Statistics",
                "Mathematics",
                "Biology",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/1808.09781",
            "abstract": "Sequential dynamics are a key feature of many modern recommender systems, which seek to capture the 'context' of users' activities on the basis of actions they have performed recently. To capture such patterns, two approaches have proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs). Markov Chains assume that a user's next action can be predicted on the basis of just their last (or last few) actions, while RNNs in principle allow for longer-term semantics to be uncovered. Generally speaking, MC-based methods perform best in extremely sparse datasets, where model parsimony is critical, while RNNs perform better in denser datasets where higher model complexity is affordable. The goal of our work is to balance these two goals, by proposing a self-attention based sequential model (SASRec) that allows us to capture long-term semantics (like an RNN), but, using an attention mechanism, makes its predictions based on relatively few actions (like an MC). At each time step, SASRec seeks to identify which items are 'relevant' from a user's action history, and use them to predict the next item. Extensive empirical studies show that our method outperforms various state-of-the-art sequential models (including MC/CNN/RNN-based approaches) on both sparse and dense datasets. Moreover, the model is an order of magnitude more efficient than comparable CNN/RNN-based models. Visualizations on attention weights also show how our model adaptively handles datasets with various density, and uncovers meaningful patterns in activity sequences."
        },
        "https://openalex.org/W3097777922": {
            "title": "Conformer: Convolution-augmented Transformer for Speech Recognition",
            "openalex_id": "https://openalex.org/W3097777922",
            "cited_by_count": 2089,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4372260258",
                "https://openalex.org/W2919798019",
                "https://openalex.org/W2759540840",
                "https://openalex.org/W2369791303",
                "https://openalex.org/W2361284596",
                "https://openalex.org/W2360069155",
                "https://openalex.org/W2293685972",
                "https://openalex.org/W2267589039",
                "https://openalex.org/W2169963286",
                "https://openalex.org/W2133280289"
            ],
            "references": [
                "https://openalex.org/W1494198834",
                "https://openalex.org/W1522301498",
                "https://openalex.org/W1828163288",
                "https://openalex.org/W1964175594",
                "https://openalex.org/W1995562189",
                "https://openalex.org/W2095705004",
                "https://openalex.org/W2112739286",
                "https://openalex.org/W2567070169",
                "https://openalex.org/W2892009249",
                "https://openalex.org/W2899423466"
            ],
            "authors": [
                "Anmol Gulati",
                "James Qin",
                "Chung\u2010Cheng Chiu",
                "Niki Parmar",
                "Yu Zhang",
                "Jiahui Yu",
                "Wei Han",
                "Shibo Wang",
                "Zhengdong Zhang",
                "Yonghui Wu",
                "Ruoming Pang"
            ],
            "venue": "Interspeech 2022",
            "doi": "https://doi.org/10.21437/interspeech.2020-3015",
            "concepts": [
                "Transformer",
                "Computer science",
                "Overlap\u2013add method",
                "Speech recognition",
                "Convolution (computer science)",
                "Artificial intelligence",
                "Natural language processing",
                "Mathematics",
                "Electrical engineering",
                "Fourier transform",
                "Engineering",
                "Voltage",
                "Artificial neural network",
                "Fourier analysis",
                "Mathematical analysis",
                "Fractional Fourier transform"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2005.08100",
            "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs).Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively.In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way.To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer.Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies.On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3%without using a language model and 1.9%/3.9%with an external language model on test/testother.We also observe competitive performance of 2.7%/6.3%with a small model of only 10M parameters."
        },
        "https://openalex.org/W4225672218": {
            "title": "Restormer: Efficient Transformer for High-Resolution Image Restoration",
            "openalex_id": "https://openalex.org/W4225672218",
            "cited_by_count": 2074,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W791927757",
                "https://openalex.org/W3153582293",
                "https://openalex.org/W3080537281",
                "https://openalex.org/W2905397092",
                "https://openalex.org/W2359633702",
                "https://openalex.org/W2289746762",
                "https://openalex.org/W2269775642",
                "https://openalex.org/W2182590612",
                "https://openalex.org/W2140617750",
                "https://openalex.org/W2031788393"
            ],
            "references": [
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1912194039",
                "https://openalex.org/W1916731006",
                "https://openalex.org/W1930824406",
                "https://openalex.org/W2056370875",
                "https://openalex.org/W2081418206",
                "https://openalex.org/W2121927366",
                "https://openalex.org/W2128254161",
                "https://openalex.org/W2150081556",
                "https://openalex.org/W2167191464"
            ],
            "authors": [
                "Syed Waqas Zamir",
                "Aditya Arora",
                "Salman Khan",
                "Munawar Hayat",
                "Fahad Shahbaz Khan",
                "Ming\u2013Hsuan Yang"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.00564",
            "concepts": [
                "Deblurring",
                "Computer science",
                "Artificial intelligence",
                "Image restoration",
                "Computer vision",
                "Transformer",
                "Pixel",
                "Image warping",
                "Convolutional neural network",
                "Image resolution",
                "Pattern recognition (psychology)",
                "Image processing",
                "Image (mathematics)",
                "Physics",
                "Quantum mechanics",
                "Voltage"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2111.09881",
            "abstract": "Since convolutional neural networks (CNNs) perform well at learning generalizable image priors from large-scale data, these models have been extensively applied to image restoration and related tasks. Recently, another class of neural architectures, Transformers, have shown significant performance gains on natural language and high-level vision tasks. While the Transformer model mitigates the shortcomings of CNNs (i.e., limited receptive field and inadaptability to input content), its computational complexity grows quadratically with the spatial resolution, therefore making it infeasible to apply to most image restoration tasks involving high-resolution images. In this work, we propose an efficient Transformer model by making several key designs in the building blocks (multi-head attention and feed-forward network) such that it can capture long-range pixel interactions, while still remaining applicable to large images. Our model, named Restoration Transformer (Restormer), achieves state-of-the-art results on several image restoration tasks, including image deraining, single-image motion deblurring, defocus deblurring (single-image and dual-pixel data), and image denoising (Gaussian grayscale/color denoising, and real image denoising). The source code and pre-trained models are available at https://github.com/swz30/Restormer."
        },
        "https://openalex.org/W3156636935": {
            "title": "SimCSE: Simple Contrastive Learning of Sentence Embeddings",
            "openalex_id": "https://openalex.org/W3156636935",
            "cited_by_count": 2006,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W60493759",
                "https://openalex.org/W4386437125",
                "https://openalex.org/W4308619659",
                "https://openalex.org/W4287121366",
                "https://openalex.org/W3213069564",
                "https://openalex.org/W3174759195",
                "https://openalex.org/W3167013339",
                "https://openalex.org/W2997229301",
                "https://openalex.org/W2794802664",
                "https://openalex.org/W2785325870"
            ],
            "references": [
                "https://openalex.org/W131533222",
                "https://openalex.org/W1486649854",
                "https://openalex.org/W1840435438",
                "https://openalex.org/W2014902591",
                "https://openalex.org/W2028175314",
                "https://openalex.org/W2037386840",
                "https://openalex.org/W2095705004",
                "https://openalex.org/W2114524997",
                "https://openalex.org/W2126400076",
                "https://openalex.org/W2133458109"
            ],
            "authors": [
                "Tianyu Gao",
                "Xingcheng Yao",
                "Danqi Chen"
            ],
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.552",
            "concepts": [
                "Artificial intelligence",
                "Computer science",
                "Natural language processing",
                "Sentence",
                "Simple (philosophy)",
                "Feature learning",
                "Pattern recognition (psychology)",
                "Unsupervised learning",
                "Philosophy",
                "Epistemology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://aclanthology.org/2021.emnlp-main.552.pdf",
            "abstract": "This paper presents SimCSE, a simple contrastive learning framework that greatly advances the state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework, by using \u201centailment\u201d pairs as positives and \u201ccontradiction\u201d pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3% and 81.6% Spearman\u2019s correlation respectively, a 4.2% and 2.2% improvement compared to previous best results. We also show\u2014both theoretically and empirically\u2014that contrastive learning objective regularizes pre-trained embeddings\u2019 anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available."
        },
        "https://openalex.org/W2963847595": {
            "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning",
            "openalex_id": "https://openalex.org/W2963847595",
            "cited_by_count": 1937,
            "publication_year": 2018,
            "related_works": [
                "https://openalex.org/W4390569940",
                "https://openalex.org/W4388685194",
                "https://openalex.org/W4388422664",
                "https://openalex.org/W4361193272",
                "https://openalex.org/W4312407344",
                "https://openalex.org/W4310278675",
                "https://openalex.org/W2963326959",
                "https://openalex.org/W2905433371",
                "https://openalex.org/W2888392564",
                "https://openalex.org/W1986582023"
            ],
            "references": [
                "https://openalex.org/W133115189",
                "https://openalex.org/W1497605902",
                "https://openalex.org/W1500693574",
                "https://openalex.org/W155346003",
                "https://openalex.org/W1673923490",
                "https://openalex.org/W1695152929",
                "https://openalex.org/W1787224781",
                "https://openalex.org/W1849277567",
                "https://openalex.org/W1899185266",
                "https://openalex.org/W1901367599"
            ],
            "authors": [
                "Leilani H. Gilpin",
                "David Bau",
                "Ben Z. Yuan",
                "Ayesha Bajwa",
                "Michael A. Specter",
                "Lalana Kagal"
            ],
            "venue": "2022 IEEE 9th International Conference on Data Science and Advanced Analytics (DSAA)",
            "doi": "https://doi.org/10.1109/dsaa.2018.00018",
            "concepts": [
                "Interpretability",
                "Computer science",
                "Artificial intelligence",
                "Data science",
                "Machine learning",
                "Artificial neural network",
                "Management science",
                "Economics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/1806.00069",
            "abstract": "There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence."
        },
        "https://openalex.org/W3034275286": {
            "title": "SuperGlue: Learning Feature Matching With Graph Neural Networks",
            "openalex_id": "https://openalex.org/W3034275286",
            "cited_by_count": 1931,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4327738859",
                "https://openalex.org/W4285277090",
                "https://openalex.org/W4252555497",
                "https://openalex.org/W3143197806",
                "https://openalex.org/W3121175838",
                "https://openalex.org/W3106568383",
                "https://openalex.org/W3016293053",
                "https://openalex.org/W2280422768",
                "https://openalex.org/W2123144113",
                "https://openalex.org/W1690653314"
            ],
            "references": [
                "https://openalex.org/W1744214816",
                "https://openalex.org/W1979931042",
                "https://openalex.org/W2003447360",
                "https://openalex.org/W2005433550",
                "https://openalex.org/W2013603106",
                "https://openalex.org/W2031487654",
                "https://openalex.org/W2033819227",
                "https://openalex.org/W2074617510",
                "https://openalex.org/W2081332440",
                "https://openalex.org/W2085261163"
            ],
            "authors": [
                "Paul-Edouard Sarlin",
                "Daniel DeTone",
                "Tomasz Malisiewicz",
                "Andrew Rabinovich"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr42600.2020.00499",
            "concepts": [
                "Heuristics",
                "Computer science",
                "Artificial intelligence",
                "Artificial neural network",
                "Graph",
                "Feature (linguistics)",
                "Differentiable function",
                "Matching (statistics)",
                "Context (archaeology)",
                "Feature matching",
                "Code (set theory)",
                "Scene graph",
                "Pattern recognition (psychology)",
                "Machine learning",
                "Feature extraction",
                "Theoretical computer science",
                "Rendering (computer graphics)",
                "Mathematics",
                "Mathematical analysis",
                "Paleontology",
                "Linguistics",
                "Philosophy",
                "Statistics",
                "Set (abstract data type)",
                "Biology",
                "Programming language",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/1911.11763",
            "abstract": "This paper introduces SuperGlue, a neural network that matches two sets of local features by jointly finding correspondences and rejecting non-matchable points. Assignments are estimated by solving a differentiable optimal transport problem, whose costs are predicted by a graph neural network. We introduce a flexible context aggregation mechanism based on attention, enabling SuperGlue to reason about the underlying 3D scene and feature assignments jointly. Compared to traditional, hand-designed heuristics, our technique learns priors over geometric transformations and regularities of the 3D world through end-to-end training from image pairs. SuperGlue outperforms other learned approaches and achieves state-of-the-art results on the task of pose estimation in challenging real-world indoor and outdoor environments. The proposed method performs matching in real-time on a modern GPU and can be readily integrated into modern SfM or SLAM systems. The code and trained weights are publicly available at github.com/magicleap/SuperGluePretrainedNetwork."
        },
        "https://openalex.org/W4212875960": {
            "title": "UNETR: Transformers for 3D Medical Image Segmentation",
            "openalex_id": "https://openalex.org/W4212875960",
            "cited_by_count": 1889,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4298287631",
                "https://openalex.org/W4225394202",
                "https://openalex.org/W3036642985",
                "https://openalex.org/W3032952384",
                "https://openalex.org/W3017902212",
                "https://openalex.org/W2982145560",
                "https://openalex.org/W2969450769",
                "https://openalex.org/W2964335273",
                "https://openalex.org/W2953061907",
                "https://openalex.org/W1847088711"
            ],
            "references": [
                "https://openalex.org/W1901129140",
                "https://openalex.org/W2285838348",
                "https://openalex.org/W2301358467",
                "https://openalex.org/W2412782625",
                "https://openalex.org/W2463818697",
                "https://openalex.org/W2515788890",
                "https://openalex.org/W2604785265",
                "https://openalex.org/W2604790786",
                "https://openalex.org/W2608631154",
                "https://openalex.org/W2618677231"
            ],
            "authors": [
                "Ali Hatamizadeh",
                "Yucheng Tang",
                "Vishwesh Nath",
                "Dong Yang",
                "Andriy Myronenko",
                "Bennett A. Landman",
                "Holger R. Roth",
                "Daguang Xu"
            ],
            "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
            "doi": "https://doi.org/10.1109/wacv51458.2022.00181",
            "concepts": [
                "Computer science",
                "Segmentation",
                "Encoder",
                "Transformer",
                "Artificial intelligence",
                "Convolutional neural network",
                "Deep learning",
                "Image segmentation",
                "Recurrent neural network",
                "Pattern recognition (psychology)",
                "Computer vision",
                "Artificial neural network",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.10504",
            "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard."
        },
        "https://openalex.org/W4205991051": {
            "title": "The Power of Scale for Parameter-Efficient Prompt Tuning",
            "openalex_id": "https://openalex.org/W4205991051",
            "cited_by_count": 1855,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4362554255",
                "https://openalex.org/W4309877123",
                "https://openalex.org/W4287802662",
                "https://openalex.org/W3154646238",
                "https://openalex.org/W3125011624",
                "https://openalex.org/W3037551068",
                "https://openalex.org/W3023594376",
                "https://openalex.org/W3023285645",
                "https://openalex.org/W2899852118",
                "https://openalex.org/W1508631387"
            ],
            "references": [
                "https://openalex.org/W131533222",
                "https://openalex.org/W1599016936",
                "https://openalex.org/W1665214252",
                "https://openalex.org/W2130158090",
                "https://openalex.org/W2135293965",
                "https://openalex.org/W2145755360",
                "https://openalex.org/W2606964149",
                "https://openalex.org/W2746097825",
                "https://openalex.org/W2804897457",
                "https://openalex.org/W2898662126"
            ],
            "authors": [
                "Brian Lester",
                "Rami Al\u2010Rfou",
                "Noah Constant"
            ],
            "venue": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
            "doi": "https://doi.org/10.18653/v1/2021.emnlp-main.243",
            "concepts": [
                "Computer science",
                "Robustness (evolution)",
                "Reuse",
                "Margin (machine learning)",
                "Fine-tuning",
                "Code (set theory)",
                "Artificial intelligence",
                "Language model",
                "Machine learning",
                "Ecology",
                "Biochemistry",
                "Chemistry",
                "Physics",
                "Set (abstract data type)",
                "Quantum mechanics",
                "Gene",
                "Biology",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://aclanthology.org/2021.emnlp-main.243.pdf",
            "abstract": "In this work, we explore \u201cprompt tuning,\u201d a simple yet effective mechanism for learning \u201csoft prompts\u201d to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3\u2019s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \u201ccloses the gap\u201d and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \u201cprefix tuning\u201d of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \u201cprompt ensembling.\u201d We release code and model checkpoints to reproduce our experiments."
        },
        "https://openalex.org/W4365512576": {
            "title": "ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
            "openalex_id": "https://openalex.org/W4365512576",
            "cited_by_count": 1816,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4280543773",
                "https://openalex.org/W4200375594",
                "https://openalex.org/W2808360891",
                "https://openalex.org/W2387622493",
                "https://openalex.org/W2366083136",
                "https://openalex.org/W2362452928",
                "https://openalex.org/W2360028903",
                "https://openalex.org/W2357832196",
                "https://openalex.org/W1932132538",
                "https://openalex.org/W178231042"
            ],
            "references": [
                "https://openalex.org/W1976732164",
                "https://openalex.org/W2046430955",
                "https://openalex.org/W2048189380",
                "https://openalex.org/W2066545696",
                "https://openalex.org/W2346062110",
                "https://openalex.org/W2506282302",
                "https://openalex.org/W2530395818",
                "https://openalex.org/W2612890464",
                "https://openalex.org/W2725331431",
                "https://openalex.org/W2736601468"
            ],
            "authors": [
                "Partha Pratim Ray"
            ],
            "venue": "Internet of Things and Cyber-Physical Systems",
            "doi": "https://doi.org/10.1016/j.iotcps.2023.04.003",
            "concepts": [
                "Scope (computer science)",
                "Engineering ethics",
                "Data science",
                "Computer science",
                "Knowledge management",
                "Management science",
                "Engineering",
                "Programming language"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.iotcps.2023.04.003",
            "abstract": "In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time."
        },
        "https://openalex.org/W3121523901": {
            "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "openalex_id": "https://openalex.org/W3121523901",
            "cited_by_count": 1789,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4304700937",
                "https://openalex.org/W4298195702",
                "https://openalex.org/W3093768914",
                "https://openalex.org/W2945402993",
                "https://openalex.org/W2770018148",
                "https://openalex.org/W2475116013",
                "https://openalex.org/W2385135707",
                "https://openalex.org/W2358308169",
                "https://openalex.org/W2140315382",
                "https://openalex.org/W2059109728"
            ],
            "references": [
                "https://openalex.org/W1614298861",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1821462560",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2183341477",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2401231614",
                "https://openalex.org/W2518108298",
                "https://openalex.org/W2549139847",
                "https://openalex.org/W2612445135"
            ],
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zihang Jiang",
                "Francis E. H. Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00060",
            "concepts": [
                "Security token",
                "Computer science",
                "Transformer",
                "Artificial intelligence",
                "Pixel",
                "Scratch",
                "Pattern recognition (psychology)",
                "Lexical analysis",
                "Speech recognition",
                "Programming language",
                "Computer network",
                "Engineering",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2101.11986",
            "abstract": "Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>"
        },
        "https://openalex.org/W3039448353": {
            "title": "Deep Learning for 3D Point Clouds: A Survey",
            "openalex_id": "https://openalex.org/W3039448353",
            "cited_by_count": 1763,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4399442168",
                "https://openalex.org/W4389574804",
                "https://openalex.org/W4375867731",
                "https://openalex.org/W4320729701",
                "https://openalex.org/W4254103348",
                "https://openalex.org/W3210378990",
                "https://openalex.org/W3034745255",
                "https://openalex.org/W2970686063",
                "https://openalex.org/W2949096641",
                "https://openalex.org/W2114282491"
            ],
            "references": [
                "https://openalex.org/W1599454686",
                "https://openalex.org/W1644641054",
                "https://openalex.org/W1662382123",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1920022804",
                "https://openalex.org/W1923184257",
                "https://openalex.org/W2007200979",
                "https://openalex.org/W2027710719",
                "https://openalex.org/W2057175746"
            ],
            "authors": [
                "Yulan Guo",
                "Hanyun Wang",
                "Qingyong Hu",
                "Hao Liu",
                "Li Liu",
                "Mohammed Bennamoun"
            ],
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "doi": "https://doi.org/10.1109/tpami.2020.3005434",
            "concepts": [
                "Point cloud",
                "Deep learning",
                "Artificial intelligence",
                "Computer science",
                "Segmentation",
                "Point (geometry)",
                "Object detection",
                "Machine learning",
                "Data science",
                "Computer vision",
                "Geometry",
                "Mathematics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://oulurepo.oulu.fi/bitstream/10024/33082/1/nbnfi-fe2022030121340.pdf",
            "abstract": "Point cloud learning has lately attracted increasing attention due to its wide applications in many areas, such as computer vision, autonomous driving, and robotics. As a dominating technique in AI, deep learning has been successfully used to solve various 2D vision problems. However, deep learning on point clouds is still in its infancy due to the unique challenges faced by the processing of point clouds with deep neural networks. Recently, deep learning on point clouds has become even thriving, with numerous methods being proposed to address different problems in this area. To stimulate future research, this paper presents a comprehensive review of recent progress in deep learning methods for point clouds. It covers three major tasks, including 3D shape classification, 3D object detection and tracking, and 3D point cloud segmentation. It also presents comparative results on several publicly available datasets, together with insightful observations and inspiring future research directions."
        },
        "https://openalex.org/W4214493665": {
            "title": "CvT: Introducing Convolutions to Vision Transformers",
            "openalex_id": "https://openalex.org/W4214493665",
            "cited_by_count": 1739,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4382323155",
                "https://openalex.org/W4315697128",
                "https://openalex.org/W4292794827",
                "https://openalex.org/W4280599700",
                "https://openalex.org/W4224939635",
                "https://openalex.org/W3205506801",
                "https://openalex.org/W3183570023",
                "https://openalex.org/W3102845713",
                "https://openalex.org/W2971502891",
                "https://openalex.org/W2016508734"
            ],
            "references": [
                "https://openalex.org/W1928278792",
                "https://openalex.org/W1977295328",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2531409750",
                "https://openalex.org/W2533598788",
                "https://openalex.org/W2896457183",
                "https://openalex.org/W2908510526",
                "https://openalex.org/W2923014074",
                "https://openalex.org/W2952809536"
            ],
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00009",
            "concepts": [
                "Transformer",
                "Computer science",
                "Convolutional neural network",
                "FLOPS",
                "Embedding",
                "Artificial intelligence",
                "Convolutional code",
                "Computer engineering",
                "Pattern recognition (psychology)",
                "Algorithm",
                "Parallel computing",
                "Decoding methods",
                "Engineering",
                "Electrical engineering",
                "Voltage"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.15808",
            "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT."
        },
        "https://openalex.org/W4214612132": {
            "title": "ViViT: A Video Vision Transformer",
            "openalex_id": "https://openalex.org/W4214612132",
            "cited_by_count": 1696,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4383066092",
                "https://openalex.org/W4380075502",
                "https://openalex.org/W4375867731",
                "https://openalex.org/W4304166257",
                "https://openalex.org/W4294635752",
                "https://openalex.org/W4230611425",
                "https://openalex.org/W2787993192",
                "https://openalex.org/W2731899572",
                "https://openalex.org/W2611989081",
                "https://openalex.org/W2158269427"
            ],
            "references": [
                "https://openalex.org/W1522734439",
                "https://openalex.org/W1533861849",
                "https://openalex.org/W1923404803",
                "https://openalex.org/W2016053056",
                "https://openalex.org/W2020163092",
                "https://openalex.org/W2068611653",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2156303437",
                "https://openalex.org/W2163605009"
            ],
            "authors": [
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Georg Heigold",
                "Chen Sun",
                "Mario Lu\u010di\u0107",
                "Cordelia Schmid"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00676",
            "concepts": [
                "Computer science",
                "Leverage (statistics)",
                "Transformer",
                "Artificial intelligence",
                "Deep learning",
                "Pattern recognition (psychology)",
                "Machine learning",
                "Computer vision",
                "Engineering",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.15691",
            "abstract": "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks."
        },
        "https://openalex.org/W4312815172": {
            "title": "Masked-attention Mask Transformer for Universal Image Segmentation",
            "openalex_id": "https://openalex.org/W4312815172",
            "cited_by_count": 1666,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W80586315",
                "https://openalex.org/W4313052709",
                "https://openalex.org/W4205800335",
                "https://openalex.org/W3144569342",
                "https://openalex.org/W2945274617",
                "https://openalex.org/W2758994127",
                "https://openalex.org/W2185902295",
                "https://openalex.org/W2103507220",
                "https://openalex.org/W2055202857",
                "https://openalex.org/W2022929107"
            ],
            "references": [
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1991367009",
                "https://openalex.org/W2037227137",
                "https://openalex.org/W2088049833",
                "https://openalex.org/W2117539524",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2340897893",
                "https://openalex.org/W2412782625",
                "https://openalex.org/W2531409750"
            ],
            "authors": [
                "Bowen Cheng",
                "Ishan Misra",
                "Alexander G. Schwing",
                "Alexander Kirillov",
                "Rohit Girdhar"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.00135",
            "concepts": [
                "Segmentation",
                "Margin (machine learning)",
                "Computer science",
                "Semantics (computer science)",
                "Artificial intelligence",
                "Transformer",
                "Image segmentation",
                "Pixel",
                "Task (project management)",
                "Scale-space segmentation",
                "Computer vision",
                "Natural language processing",
                "Pattern recognition (psychology)",
                "Machine learning",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Programming language",
                "Systems engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/2112.01527",
            "abstract": "Image segmentation groups pixels with different semantics, e.g., category or instance membership. Each choice of semantics defines a task. While only the semantics of each task differ, current research focuses on designing spe-cialized architectures for each task. We present Masked- attention Mask Transformer (Mask2Former), a new archi-tecture capable of addressing any image segmentation task (panoptic, instance or semantic). Its key components in-clude masked attention, which extracts localized features by constraining cross-attention within predicted mask regions. In addition to reducing the research effort by at least three times, it outperforms the best specialized architectures by a significant margin on four popular datasets. Most no-tably, Mask2Former sets a new state-of-the-art for panoptic segmentation (57.8 PQ on COCO), instance segmentation (50.1 AP on COCO) and semantic segmentation (57.7 mIoU onADE20K)."
        }
    },
    "_metadata": {
        "query": "GPT: Generative Pre-trained Transformer",
        "total_citations": 50,
        "collection_time": 4.075343132019043,
        "requests_made": 3,
        "main_paper": {
            "title": "Opinion Paper: \u201cSo what if ChatGPT wrote it?\u201d Multidisciplinary perspectives on opportunities, challenges and implications of generative conversational AI for research, practice and policy",
            "id": "https://openalex.org/W4360620450",
            "cited_by_count": 2660,
            "publication_year": 2023
        }
    },
    "https://openalex.org/W2896457183": {
        "https://openalex.org/W3096609285": {
            "title": "End-to-End Object Detection with Transformers",
            "openalex_id": "https://openalex.org/W3096609285",
            "cited_by_count": 10315,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4376620596",
                "https://openalex.org/W4298525700",
                "https://openalex.org/W4293054914",
                "https://openalex.org/W3177249605",
                "https://openalex.org/W3138508047",
                "https://openalex.org/W2953362004",
                "https://openalex.org/W2534152068",
                "https://openalex.org/W2187606256",
                "https://openalex.org/W1972515067",
                "https://openalex.org/W1689909837"
            ],
            "references": [
                "https://openalex.org/W1533861849",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2068730032",
                "https://openalex.org/W2130942839",
                "https://openalex.org/W2177466532",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2222512263",
                "https://openalex.org/W2555182955",
                "https://openalex.org/W2565639579",
                "https://openalex.org/W2612624696"
            ],
            "authors": [
                "Nicolas Carion",
                "Francisco Massa",
                "Gabriel Synnaeve",
                "Nicolas Usunier",
                "Alexander Kirillov",
                "Sergey Zagoruyko"
            ],
            "venue": "Lecture notes in computer science",
            "doi": "https://doi.org/10.1007/978-3-030-58452-8_13",
            "concepts": [
                "Computer science",
                "Object detection",
                "Encoder",
                "Transformer",
                "Artificial intelligence",
                "Data mining",
                "Pascal (unit)",
                "ENCODE",
                "Segmentation",
                "Machine learning",
                "Computer vision",
                "Programming language",
                "Biochemistry",
                "Chemistry",
                "Physics",
                "Voltage",
                "Quantum mechanics",
                "Gene",
                "Operating system"
            ],
            "type": "book-chapter",
            "language": "en",
            "is_oa": false,
            "oa_url": "https://arxiv.org/pdf/2005.12872"
        },
        "https://openalex.org/W3035524453": {
            "title": "Momentum Contrast for Unsupervised Visual Representation Learning",
            "openalex_id": "https://openalex.org/W3035524453",
            "cited_by_count": 9972,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W60493759",
                "https://openalex.org/W4386437125",
                "https://openalex.org/W4308619659",
                "https://openalex.org/W4287121366",
                "https://openalex.org/W3174759195",
                "https://openalex.org/W3167013339",
                "https://openalex.org/W2962474440",
                "https://openalex.org/W2726367589",
                "https://openalex.org/W2440023763",
                "https://openalex.org/W2186489521"
            ],
            "references": [
                "https://openalex.org/W1536680647",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1976921161",
                "https://openalex.org/W2025768430",
                "https://openalex.org/W2031489346",
                "https://openalex.org/W2099471712",
                "https://openalex.org/W2102605133"
            ],
            "authors": [
                "Kaiming He",
                "Haoqi Fan",
                "Yuxin Wu",
                "Saining Xie",
                "Ross Girshick"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr42600.2020.00975",
            "concepts": [
                "Artificial intelligence",
                "Pascal (unit)",
                "Computer science",
                "Contrast (vision)",
                "Unsupervised learning",
                "Segmentation",
                "Feature learning",
                "Machine learning",
                "Representation (politics)",
                "Pattern recognition (psychology)",
                "Transfer of learning",
                "Encoder",
                "Natural language processing",
                "Politics",
                "Political science",
                "Law",
                "Programming language",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/1911.05722",
            "abstract": "We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks."
        },
        "https://openalex.org/W4312933868": {
            "title": "High-Resolution Image Synthesis with Latent Diffusion Models",
            "openalex_id": "https://openalex.org/W4312933868",
            "cited_by_count": 7793,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W49967185",
                "https://openalex.org/W425542480",
                "https://openalex.org/W3178025616",
                "https://openalex.org/W3035059915",
                "https://openalex.org/W2946160871",
                "https://openalex.org/W2907830442",
                "https://openalex.org/W2131831293",
                "https://openalex.org/W2060947339",
                "https://openalex.org/W2017457812",
                "https://openalex.org/W1995073329"
            ],
            "references": [
                "https://openalex.org/W1583912456",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1909320841",
                "https://openalex.org/W1959608418",
                "https://openalex.org/W2031342017",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2125389028",
                "https://openalex.org/W2129069237"
            ],
            "authors": [
                "Robin Rombach",
                "Andreas Blattmann",
                "Dominik Lorenz",
                "Patrick Esser",
                "Bj\u00f6rn Ommer"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01042",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Pixel",
                "Inference",
                "Inpainting",
                "Image translation",
                "Computer vision",
                "Image (mathematics)"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2112.10752",
            "abstract": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state of the art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including unconditional image generation, text-to-image synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
        },
        "https://openalex.org/W2911489562": {
            "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
            "openalex_id": "https://openalex.org/W2911489562",
            "cited_by_count": 5660,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W4387356431",
                "https://openalex.org/W4308171175",
                "https://openalex.org/W3185751515",
                "https://openalex.org/W3111301126",
                "https://openalex.org/W3045642779",
                "https://openalex.org/W2963862093",
                "https://openalex.org/W2572241437",
                "https://openalex.org/W2527712604",
                "https://openalex.org/W2489933339",
                "https://openalex.org/W2057069926"
            ],
            "references": [
                "https://openalex.org/W1981208470",
                "https://openalex.org/W2047782770",
                "https://openalex.org/W2052217781",
                "https://openalex.org/W2071879021",
                "https://openalex.org/W2100627415",
                "https://openalex.org/W2136437513",
                "https://openalex.org/W2149369282",
                "https://openalex.org/W2153579005",
                "https://openalex.org/W2154142897",
                "https://openalex.org/W2168041406"
            ],
            "authors": [
                "Jinhyuk Lee",
                "Wonjin Yoon",
                "Sungdong Kim",
                "Donghyeon Kim",
                "Sunkyu Kim",
                "Chan Ho So",
                "Jaewoo Kang"
            ],
            "venue": "Bioinformatics",
            "doi": "https://doi.org/10.1093/bioinformatics/btz682",
            "concepts": [
                "Biomedical text mining",
                "Computer science",
                "Artificial intelligence",
                "Natural language processing",
                "Language model",
                "Named-entity recognition",
                "Relationship extraction",
                "Text mining",
                "Text corpus",
                "Representation (politics)",
                "F1 score",
                "Domain (mathematical analysis)",
                "Source code",
                "Information extraction",
                "Mathematical analysis",
                "Mathematics",
                "Management",
                "Politics",
                "Political science",
                "Law",
                "Economics",
                "Task (project management)",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/1901.08746",
            "abstract": "Biomedical text mining is becoming increasingly important as the number of biomedical documents rapidly grows. With the progress in natural language processing (NLP), extracting valuable information from biomedical literature has gained popularity among researchers, and deep learning has boosted the development of effective biomedical text mining models. However, directly applying the advancements in NLP to biomedical text mining often yields unsatisfactory results due to a word distribution shift from general domain corpora to biomedical corpora. In this article, we investigate how the recently introduced pre-trained language model BERT can be adapted for biomedical corpora. We introduce BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora. With almost the same architecture across tasks, BioBERT largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre-trained on biomedical corpora. While BERT obtains performance comparable to that of previous state-of-the-art models, BioBERT significantly outperforms them on the following three representative biomedical text mining tasks: biomedical named entity recognition (0.62% F1 score improvement), biomedical relation extraction (2.80% F1 score improvement) and biomedical question answering (12.24% MRR improvement). Our analysis results show that pre-training BERT on biomedical corpora helps it to understand complex biomedical texts. We make the pre-trained weights of BioBERT freely available at https://github.com/naver/biobert-pretrained, and the source code for fine-tuning BioBERT available at https://github.com/dmis-lab/biobert."
        },
        "https://openalex.org/W4313156423": {
            "title": "Masked Autoencoders Are Scalable Vision Learners",
            "openalex_id": "https://openalex.org/W4313156423",
            "cited_by_count": 4756,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4390516098",
                "https://openalex.org/W4285411112",
                "https://openalex.org/W4235240664",
                "https://openalex.org/W3081694532",
                "https://openalex.org/W2965083567",
                "https://openalex.org/W2757182831",
                "https://openalex.org/W2389214306",
                "https://openalex.org/W2171299904",
                "https://openalex.org/W2085033728",
                "https://openalex.org/W1838576100"
            ],
            "references": [
                "https://openalex.org/W1533861849",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2025768430",
                "https://openalex.org/W2063971957",
                "https://openalex.org/W2102409316",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2134670479",
                "https://openalex.org/W2138621090"
            ],
            "authors": [
                "Kaiming He",
                "Xinlei Chen",
                "Saining Xie",
                "Yanghao Li",
                "Piotr Doll\u00e1r",
                "Ross Girshick"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01553",
            "concepts": [
                "Computer science",
                "Scalability",
                "Artificial intelligence",
                "Encoder",
                "Masking (illustration)",
                "Representation (politics)",
                "Image (mathematics)",
                "Feature learning",
                "Pixel",
                "Pattern recognition (psychology)",
                "Computer vision",
                "Art",
                "Database",
                "Politics",
                "Political science",
                "Law",
                "Visual arts",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/2111.06377",
            "abstract": "This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3\u00d7 or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pretraining and shows promising scaling behavior."
        },
        "https://openalex.org/W4312443924": {
            "title": "A ConvNet for the 2020s",
            "openalex_id": "https://openalex.org/W4312443924",
            "cited_by_count": 4351,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W97075385",
                "https://openalex.org/W4235240664",
                "https://openalex.org/W2965083567",
                "https://openalex.org/W2389214306",
                "https://openalex.org/W2357523926",
                "https://openalex.org/W2095886385",
                "https://openalex.org/W2089704382",
                "https://openalex.org/W1983399550",
                "https://openalex.org/W1838576100",
                "https://openalex.org/W1522196789"
            ],
            "references": [
                "https://openalex.org/W1536680647",
                "https://openalex.org/W1665214252",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2056695679",
                "https://openalex.org/W2077513643",
                "https://openalex.org/W2086161653",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2102605133",
                "https://openalex.org/W2108598243"
            ],
            "authors": [
                "Zhuang Liu",
                "Hanzi Mao",
                "Chao-Yuan Wu",
                "Christoph Feichtenhofer",
                "Trevor Darrell",
                "Saining Xie"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01167",
            "concepts": [
                "Transformer",
                "Computer science",
                "Artificial intelligence",
                "Segmentation",
                "Scalability",
                "Object detection",
                "Machine learning",
                "Image segmentation",
                "Computer vision",
                "Pattern recognition (psychology)",
                "Engineering",
                "Electrical engineering",
                "Voltage",
                "Database"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "The \"Roaring 20s\" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually \"modernize\" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets."
        },
        "https://openalex.org/W2996428491": {
            "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
            "openalex_id": "https://openalex.org/W2996428491",
            "cited_by_count": 4238,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W3211744874",
                "https://openalex.org/W2982905616",
                "https://openalex.org/W2393172683",
                "https://openalex.org/W2392760275",
                "https://openalex.org/W2392060890",
                "https://openalex.org/W2375873920",
                "https://openalex.org/W2146114872",
                "https://openalex.org/W2083530853",
                "https://openalex.org/W2009831055",
                "https://openalex.org/W1994626569"
            ],
            "references": [
                "https://openalex.org/W131533222",
                "https://openalex.org/W1486649854",
                "https://openalex.org/W1599016936",
                "https://openalex.org/W1991145433",
                "https://openalex.org/W2124741472",
                "https://openalex.org/W2130158090",
                "https://openalex.org/W2153579005",
                "https://openalex.org/W2170973209",
                "https://openalex.org/W2250539671",
                "https://openalex.org/W2251939518"
            ],
            "authors": [
                "Zhenzhong Lan",
                "Mingda Chen",
                "Sebastian Goodman",
                "Kevin Gimpel",
                "Piyush Sharma",
                "Radu Soricut"
            ],
            "venue": "arXiv (Cornell University)",
            "doi": "https://doi.org/10.48550/arxiv.1909.11942",
            "concepts": [
                "Computer science",
                "Sentence",
                "Language model",
                "Artificial intelligence",
                "Code (set theory)",
                "Natural language processing",
                "Point (geometry)",
                "Coherence (philosophical gambling strategy)",
                "Machine learning",
                "Programming language",
                "Set (abstract data type)",
                "Physics",
                "Geometry",
                "Mathematics",
                "Quantum mechanics"
            ],
            "type": "preprint",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/abs/1909.11942",
            "abstract": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT."
        },
        "https://openalex.org/W3152893301": {
            "title": "Graph neural networks: A review of methods and applications",
            "openalex_id": "https://openalex.org/W3152893301",
            "cited_by_count": 4235,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4312417841",
                "https://openalex.org/W4226493464",
                "https://openalex.org/W3133861977",
                "https://openalex.org/W3115442681",
                "https://openalex.org/W3103566983",
                "https://openalex.org/W3029198973",
                "https://openalex.org/W2951211570",
                "https://openalex.org/W2735662278",
                "https://openalex.org/W2391000461",
                "https://openalex.org/W2165912799"
            ],
            "references": [
                "https://openalex.org/W103340358",
                "https://openalex.org/W1501856433",
                "https://openalex.org/W2000375074",
                "https://openalex.org/W2020890899",
                "https://openalex.org/W2022322548",
                "https://openalex.org/W2034618876",
                "https://openalex.org/W2056562706",
                "https://openalex.org/W2064675550",
                "https://openalex.org/W2092750499",
                "https://openalex.org/W2097073572"
            ],
            "authors": [
                "Jie Zhou",
                "Ganqu Cui",
                "Shengding Hu",
                "Zhengyan Zhang",
                "Cheng Yang",
                "Zhiyuan Liu",
                "Lifeng Wang",
                "Changcheng Li",
                "Maosong Sun"
            ],
            "venue": "AI Open",
            "doi": "https://doi.org/10.1016/j.aiopen.2021.01.001",
            "concepts": [
                "Computer science",
                "Graph",
                "Artificial intelligence",
                "Categorization",
                "Deep learning",
                "Graph database",
                "Theoretical computer science",
                "Convolutional neural network",
                "Machine learning"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.aiopen.2021.01.001",
            "abstract": "Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research."
        },
        "https://openalex.org/W3177318507": {
            "title": "Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting",
            "openalex_id": "https://openalex.org/W3177318507",
            "cited_by_count": 3690,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W65617392",
                "https://openalex.org/W63071447",
                "https://openalex.org/W4231964008",
                "https://openalex.org/W2888625260",
                "https://openalex.org/W2589098947",
                "https://openalex.org/W2385621972",
                "https://openalex.org/W2351267244",
                "https://openalex.org/W2275988210",
                "https://openalex.org/W2169928498",
                "https://openalex.org/W1529400504"
            ],
            "references": [
                "https://openalex.org/W1969852690",
                "https://openalex.org/W2054685200",
                "https://openalex.org/W2130942839",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2319170717",
                "https://openalex.org/W2549483845",
                "https://openalex.org/W2604847698",
                "https://openalex.org/W2613328025",
                "https://openalex.org/W2757354914",
                "https://openalex.org/W2762309767"
            ],
            "authors": [
                "Haoyi Zhou",
                "Shanghang Zhang",
                "Jieqi Peng",
                "Shuai Zhang",
                "Jianxin Li",
                "Hui Xiong",
                "Wancai Zhang"
            ],
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "doi": "https://doi.org/10.1609/aaai.v35i12.17325",
            "concepts": [
                "Transformer",
                "Computer science",
                "Encoder",
                "Sequence (biology)",
                "Dependency (UML)",
                "Algorithm",
                "Artificial intelligence",
                "Engineering",
                "Voltage",
                "Biology",
                "Electrical engineering",
                "Genetics",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/17325/17132",
            "abstract": "Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L log L) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem."
        },
        "https://openalex.org/W3159481202": {
            "title": "Emerging Properties in Self-Supervised Vision Transformers",
            "openalex_id": "https://openalex.org/W3159481202",
            "cited_by_count": 3288,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4390516098",
                "https://openalex.org/W4390062853",
                "https://openalex.org/W4389256085",
                "https://openalex.org/W4313644201",
                "https://openalex.org/W4285328440",
                "https://openalex.org/W4205302943",
                "https://openalex.org/W2561132942",
                "https://openalex.org/W2384362569",
                "https://openalex.org/W2181948922",
                "https://openalex.org/W2142795561"
            ],
            "references": [
                "https://openalex.org/W1821462560",
                "https://openalex.org/W2020308406",
                "https://openalex.org/W2031489346",
                "https://openalex.org/W2086161653",
                "https://openalex.org/W2100664567",
                "https://openalex.org/W2117539524",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2134670479",
                "https://openalex.org/W2144796873",
                "https://openalex.org/W2148809531"
            ],
            "authors": [
                "Mathilde Caron",
                "Hugo Touvron",
                "Ishan Misra",
                "Herv\u00e9 Je\u01f5ou",
                "Julien Mairal",
                "Piotr Bojanowski",
                "Armand Joulin"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00951",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Machine learning",
                "Transformer",
                "Segmentation",
                "Supervised learning",
                "Encoder",
                "Pattern recognition (psychology)",
                "Artificial neural network",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://hal.science/hal-03323359/document",
            "abstract": "In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) [16] that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder [26], multi-crop training [9], and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base."
        },
        "https://openalex.org/W2980282514": {
            "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
            "openalex_id": "https://openalex.org/W2980282514",
            "cited_by_count": 3263,
            "publication_year": 2019,
            "related_works": [
                "https://openalex.org/W4391913857",
                "https://openalex.org/W4391375266",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2530322880",
                "https://openalex.org/W2390279801",
                "https://openalex.org/W2382290278",
                "https://openalex.org/W2376932109",
                "https://openalex.org/W2358668433",
                "https://openalex.org/W2350741829",
                "https://openalex.org/W2001405890"
            ],
            "references": [
                "https://openalex.org/W2123442489",
                "https://openalex.org/W2752194699",
                "https://openalex.org/W2787560479",
                "https://openalex.org/W2793978524",
                "https://openalex.org/W2896457183",
                "https://openalex.org/W2908510526",
                "https://openalex.org/W2911109671",
                "https://openalex.org/W2946119234",
                "https://openalex.org/W2946417913",
                "https://openalex.org/W2949251082"
            ],
            "authors": [
                "Thomas Wolf",
                "Lysandre Debut",
                "Victor Sanh",
                "Julien Chaumond",
                "Cl\u00e9ment Delangue",
                "Anthony Moi",
                "Pierric Cistac",
                "Tim Rault",
                "R\u00e9mi Louf",
                "Morgan Funtowicz",
                "Jamie Brew"
            ],
            "venue": "arXiv (Cornell University)",
            "doi": "https://doi.org/10.48550/arxiv.1910.03771",
            "concepts": [
                "Transformer",
                "Architecture",
                "Computer science",
                "Software engineering",
                "Engineering",
                "Electrical engineering",
                "Voltage",
                "Art",
                "Visual arts"
            ],
            "type": "preprint",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/abs/1910.03771",
            "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{https://github.com/huggingface/transformers}."
        },
        "https://openalex.org/W4323655724": {
            "title": "ChatGPT for good? On opportunities and challenges of large language models for education",
            "openalex_id": "https://openalex.org/W4323655724",
            "cited_by_count": 3094,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4362576712",
                "https://openalex.org/W4312355418",
                "https://openalex.org/W2387560707",
                "https://openalex.org/W2384329035",
                "https://openalex.org/W2373380871",
                "https://openalex.org/W2370820329",
                "https://openalex.org/W2370554813",
                "https://openalex.org/W2363525455",
                "https://openalex.org/W2348562106",
                "https://openalex.org/W2314810092"
            ],
            "references": [
                "https://openalex.org/W1583837637",
                "https://openalex.org/W2783539613",
                "https://openalex.org/W2896457183",
                "https://openalex.org/W2937444804",
                "https://openalex.org/W2955088691",
                "https://openalex.org/W2958068682",
                "https://openalex.org/W2965373594",
                "https://openalex.org/W2969717756",
                "https://openalex.org/W2970597249",
                "https://openalex.org/W3085139254"
            ],
            "authors": [
                "Enkelejda Kasneci",
                "Kathrin Se\u00dfler",
                "Stefan K\u00fcchemann",
                "Maria Bannert",
                "Daryna Dementieva",
                "Frank Fischer",
                "Urs Gasser",
                "Georg Groh",
                "Stephan G\u00fcnnemann",
                "Eyke H\u00fcllermeier",
                "Stephan Krusche",
                "Gitta Kutyniok",
                "Tilman Michaeli",
                "Claudia Nerdel",
                "J\u00fcrgen Pfeffer",
                "Oleksandra Poquet",
                "Michael Sailer",
                "Albrecht Schmidt",
                "Tina Seidel",
                "Matthias Stadler",
                "J. Weller",
                "Jochen K\u00fchn",
                "Gjergji Kasneci"
            ],
            "venue": "Learning and Individual Differences",
            "doi": "https://doi.org/10.1016/j.lindif.2023.102274",
            "concepts": [
                "Curriculum",
                "Computer science",
                "Field (mathematics)",
                "Engineering ethics",
                "Knowledge management",
                "Management science",
                "Psychology",
                "Pedagogy",
                "Engineering",
                "Mathematics",
                "Pure mathematics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.lindif.2023.102274"
        },
        "https://openalex.org/W3170841864": {
            "title": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers",
            "openalex_id": "https://openalex.org/W3170841864",
            "cited_by_count": 2910,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4286681602",
                "https://openalex.org/W4242339654",
                "https://openalex.org/W3209312100",
                "https://openalex.org/W2349160795",
                "https://openalex.org/W2159686533",
                "https://openalex.org/W2100576949",
                "https://openalex.org/W2069133146",
                "https://openalex.org/W2017545316",
                "https://openalex.org/W1663079876",
                "https://openalex.org/W1522196789"
            ],
            "references": [
                "https://openalex.org/W1610060839",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1745334888",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1923697677",
                "https://openalex.org/W2124592697",
                "https://openalex.org/W2125215748",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2340897893"
            ],
            "authors": [
                "Sixiao Zheng",
                "Jiachen Lu",
                "Hengshuang Zhao",
                "Xiatian Zhu",
                "Zekun Luo",
                "Yabiao Wang",
                "Yanwei Fu",
                "Jianfeng Feng",
                "Tao Xiang",
                "Philip H. S. Torr",
                "Li Zhang"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr46437.2021.00681",
            "concepts": [
                "Computer science",
                "Segmentation",
                "Encoder",
                "Transformer",
                "Artificial intelligence",
                "Pascal (unit)",
                "Image segmentation",
                "Computer vision",
                "Pattern recognition (psychology)",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Programming language",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2012.15840",
            "abstract": "Most recent semantic segmentation methods adopt a fully-convolutional network (FCN) with an encoder-decoder architecture. The encoder progressively reduces the spatial resolution and learns more abstract/semantic visual concepts with larger receptive fields. Since context modeling is critical for segmentation, the latest efforts have been focused on increasing the receptive field, through either dilated/atrous convolutions or inserting attention modules. However, the encoder-decoder based FCN architecture remains unchanged. In this paper, we aim to provide an alternative perspective by treating semantic segmentation as a sequence-to-sequence prediction task. Specifically, we deploy a pure transformer (i.e., without convolution and resolution reduction) to encode an image as a sequence of patches. With the global context modeled in every layer of the transformer, this encoder can be combined with a simple decoder to provide a powerful segmentation model, termed SEgmentation TRansformer (SETR). Extensive experiments show that SETR achieves new state of the art on ADE20K (50.28% mIoU), Pascal Context (55.83% mIoU) and competitive results on Cityscapes. Particularly, we achieve the first position in the highly competitive ADE20K test server leaderboard on the day of submission."
        },
        "https://openalex.org/W4213019189": {
            "title": "A Survey on Vision Transformer",
            "openalex_id": "https://openalex.org/W4213019189",
            "cited_by_count": 2527,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4391266461",
                "https://openalex.org/W4321487865",
                "https://openalex.org/W4313906399",
                "https://openalex.org/W4312273141",
                "https://openalex.org/W4310274968",
                "https://openalex.org/W4293226380",
                "https://openalex.org/W2590798552",
                "https://openalex.org/W1810370127",
                "https://openalex.org/W1594946127",
                "https://openalex.org/W1587378402"
            ],
            "references": [
                "https://openalex.org/W1598730426",
                "https://openalex.org/W1783366411",
                "https://openalex.org/W1821462560",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W2011815965",
                "https://openalex.org/W2025768430",
                "https://openalex.org/W2027377866",
                "https://openalex.org/W2053619738",
                "https://openalex.org/W2064296540"
            ],
            "authors": [
                "Kai Han",
                "Yunhe Wang",
                "Hanting Chen",
                "Xinghao Chen",
                "Jianyuan Guo",
                "Zhenhua Liu",
                "Yehui Tang",
                "An Xiao",
                "Chunjing Xu",
                "Yixing Xu",
                "Zhaohui Yang",
                "Yiman Zhang",
                "Dacheng Tao"
            ],
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "doi": "https://doi.org/10.1109/tpami.2022.3152247",
            "concepts": [
                "Transformer",
                "Computer science",
                "Artificial intelligence",
                "Convolutional neural network",
                "Inductive bias",
                "Machine vision",
                "Artificial neural network",
                "Machine learning",
                "Computer vision",
                "Engineering",
                "Multi-task learning",
                "Electrical engineering",
                "Systems engineering",
                "Voltage",
                "Task (project management)"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2012.12556",
            "abstract": "Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers."
        },
        "https://openalex.org/W3036601975": {
            "title": "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations",
            "openalex_id": "https://openalex.org/W3036601975",
            "cited_by_count": 2355,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4391913857",
                "https://openalex.org/W4391375266",
                "https://openalex.org/W3204019825",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2478288626",
                "https://openalex.org/W2390279801",
                "https://openalex.org/W2382290278",
                "https://openalex.org/W2376932109",
                "https://openalex.org/W2358668433",
                "https://openalex.org/W2001405890"
            ],
            "references": [
                "https://openalex.org/W10548402",
                "https://openalex.org/W1494198834",
                "https://openalex.org/W2121879602",
                "https://openalex.org/W2124509324",
                "https://openalex.org/W2127141656",
                "https://openalex.org/W2152790380",
                "https://openalex.org/W2296701362",
                "https://openalex.org/W2547875792",
                "https://openalex.org/W273093436",
                "https://openalex.org/W2794209590"
            ],
            "authors": [
                "Alexei Baevski",
                "Henry Zhou",
                "Abdelrahman Mohamed",
                "Michael Auli"
            ],
            "venue": "arXiv (Cornell University)",
            "doi": "https://doi.org/10.48550/arxiv.2006.11477",
            "concepts": [
                "Computer science",
                "Natural language processing",
                "Self representation",
                "Artificial intelligence",
                "Speech recognition",
                "Art",
                "Humanities"
            ],
            "type": "preprint",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/abs/2006.11477",
            "abstract": "We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data."
        },
        "https://openalex.org/W4206706211": {
            "title": "Transformers in Vision: A Survey",
            "openalex_id": "https://openalex.org/W4206706211",
            "cited_by_count": 2278,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W97075385",
                "https://openalex.org/W4235240664",
                "https://openalex.org/W2965083567",
                "https://openalex.org/W2889616422",
                "https://openalex.org/W2389214306",
                "https://openalex.org/W2357523926",
                "https://openalex.org/W2095886385",
                "https://openalex.org/W2089704382",
                "https://openalex.org/W1983399550",
                "https://openalex.org/W1838576100"
            ],
            "references": [
                "https://openalex.org/W1673923490",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1895577753",
                "https://openalex.org/W1920022804",
                "https://openalex.org/W1933349210",
                "https://openalex.org/W1959608418",
                "https://openalex.org/W2064675550",
                "https://openalex.org/W2097073572",
                "https://openalex.org/W2101032778",
                "https://openalex.org/W2108598243"
            ],
            "authors": [
                "Salman Khan",
                "Muzammal Naseer",
                "Munawar Hayat",
                "Syed Waqas Zamir",
                "Fahad Shahbaz Khan",
                "Mubarak Shah"
            ],
            "venue": "ACM Computing Surveys",
            "doi": "https://doi.org/10.1145/3505244",
            "concepts": [
                "Computer science",
                "Transformer",
                "Segmentation",
                "Artificial intelligence",
                "Image processing",
                "Scalability",
                "Computer vision",
                "Database",
                "Physics",
                "Quantum mechanics",
                "Voltage",
                "Image (mathematics)"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "http://arxiv.org/pdf/2101.01169",
            "abstract": "Astounding results from Transformer models on natural language tasks have intrigued the vision community to study their application to computer vision problems. Among their salient benefits, Transformers enable modeling long dependencies between input sequence elements and support parallel processing of sequence as compared to recurrent networks e.g., Long short-term memory (LSTM). Different from convolutional networks, Transformers require minimal inductive biases for their design and are naturally suited as set-functions. Furthermore, the straightforward design of Transformers allows processing multiple modalities (e.g., images, videos, text and speech) using similar processing blocks and demonstrates excellent scalability to very large capacity networks and huge datasets. These strengths have led to exciting progress on a number of vision tasks using Transformer networks. This survey aims to provide a comprehensive overview of the Transformer models in the computer vision discipline. We start with an introduction to fundamental concepts behind the success of Transformers i.e., self-attention, large-scale pre-training, and bidirectional encoding. We then cover extensive applications of transformers in vision including popular recognition tasks (e.g., image classification, object detection, action recognition, and segmentation), generative modeling, multi-modal tasks (e.g., visual-question answering, visual reasoning, and visual grounding), video processing (e.g., activity recognition, video forecasting), low-level vision (e.g., image super-resolution, image enhancement, and colorization) and 3D analysis (e.g., point cloud classification and segmentation). We compare the respective advantages and limitations of popular techniques both in terms of architectural design and their experimental value. Finally, we provide an analysis on open research directions and possible future works."
        },
        "https://openalex.org/W4212875960": {
            "title": "UNETR: Transformers for 3D Medical Image Segmentation",
            "openalex_id": "https://openalex.org/W4212875960",
            "cited_by_count": 1889,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4298287631",
                "https://openalex.org/W4225394202",
                "https://openalex.org/W3036642985",
                "https://openalex.org/W3032952384",
                "https://openalex.org/W3017902212",
                "https://openalex.org/W2982145560",
                "https://openalex.org/W2969450769",
                "https://openalex.org/W2964335273",
                "https://openalex.org/W2953061907",
                "https://openalex.org/W1847088711"
            ],
            "references": [
                "https://openalex.org/W1901129140",
                "https://openalex.org/W2285838348",
                "https://openalex.org/W2301358467",
                "https://openalex.org/W2412782625",
                "https://openalex.org/W2463818697",
                "https://openalex.org/W2515788890",
                "https://openalex.org/W2604785265",
                "https://openalex.org/W2604790786",
                "https://openalex.org/W2608631154",
                "https://openalex.org/W2618677231"
            ],
            "authors": [
                "Ali Hatamizadeh",
                "Yucheng Tang",
                "Vishwesh Nath",
                "Dong Yang",
                "Andriy Myronenko",
                "Bennett A. Landman",
                "Holger R. Roth",
                "Daguang Xu"
            ],
            "venue": "2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)",
            "doi": "https://doi.org/10.1109/wacv51458.2022.00181",
            "concepts": [
                "Computer science",
                "Segmentation",
                "Encoder",
                "Transformer",
                "Artificial intelligence",
                "Convolutional neural network",
                "Deep learning",
                "Image segmentation",
                "Recurrent neural network",
                "Pattern recognition (psychology)",
                "Computer vision",
                "Artificial neural network",
                "Engineering",
                "Voltage",
                "Electrical engineering",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.10504",
            "abstract": "Fully Convolutional Neural Networks (FCNNs) with contracting and expanding paths have shown prominence for the majority of medical image segmentation applications since the past decade. In FCNNs, the encoder plays an integral role by learning both global and local features and contextual representations which can be utilized for semantic output prediction by the decoder. Despite their success, the locality of convolutional layers in FCNNs, limits the capability of learning long-range spatial dependencies. Inspired by the recent success of transformers for Natural Language Processing (NLP) in long-range sequence learning, we reformulate the task of volumetric (3D) medical image segmentation as a sequence-to-sequence prediction problem. We introduce a novel architecture, dubbed as UNEt TRansformers (UNETR), that utilizes a transformer as the encoder to learn sequence representations of the input volume and effectively capture the global multi-scale information, while also following the successful \"U-shaped\" network design for the encoder and decoder. The transformer encoder is directly connected to a decoder via skip connections at different resolutions to compute the final semantic segmentation output. We have validated the performance of our method on the Multi Atlas Labeling Beyond The Cranial Vault (BTCV) dataset for multi-organ segmentation and the Medical Segmentation Decathlon (MSD) dataset for brain tumor and spleen segmentation tasks. Our benchmarks demonstrate new state-of-the-art performance on the BTCV leaderboard."
        },
        "https://openalex.org/W4365512576": {
            "title": "ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
            "openalex_id": "https://openalex.org/W4365512576",
            "cited_by_count": 1816,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4280543773",
                "https://openalex.org/W4200375594",
                "https://openalex.org/W2808360891",
                "https://openalex.org/W2387622493",
                "https://openalex.org/W2366083136",
                "https://openalex.org/W2362452928",
                "https://openalex.org/W2360028903",
                "https://openalex.org/W2357832196",
                "https://openalex.org/W1932132538",
                "https://openalex.org/W178231042"
            ],
            "references": [
                "https://openalex.org/W1976732164",
                "https://openalex.org/W2046430955",
                "https://openalex.org/W2048189380",
                "https://openalex.org/W2066545696",
                "https://openalex.org/W2346062110",
                "https://openalex.org/W2506282302",
                "https://openalex.org/W2530395818",
                "https://openalex.org/W2612890464",
                "https://openalex.org/W2725331431",
                "https://openalex.org/W2736601468"
            ],
            "authors": [
                "Partha Pratim Ray"
            ],
            "venue": "Internet of Things and Cyber-Physical Systems",
            "doi": "https://doi.org/10.1016/j.iotcps.2023.04.003",
            "concepts": [
                "Scope (computer science)",
                "Engineering ethics",
                "Data science",
                "Computer science",
                "Knowledge management",
                "Management science",
                "Engineering",
                "Programming language"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.iotcps.2023.04.003",
            "abstract": "In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time."
        },
        "https://openalex.org/W3121523901": {
            "title": "Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet",
            "openalex_id": "https://openalex.org/W3121523901",
            "cited_by_count": 1789,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4304700937",
                "https://openalex.org/W4298195702",
                "https://openalex.org/W3093768914",
                "https://openalex.org/W2945402993",
                "https://openalex.org/W2770018148",
                "https://openalex.org/W2475116013",
                "https://openalex.org/W2385135707",
                "https://openalex.org/W2358308169",
                "https://openalex.org/W2140315382",
                "https://openalex.org/W2059109728"
            ],
            "references": [
                "https://openalex.org/W1614298861",
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1821462560",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2183341477",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2401231614",
                "https://openalex.org/W2518108298",
                "https://openalex.org/W2549139847",
                "https://openalex.org/W2612445135"
            ],
            "authors": [
                "Li Yuan",
                "Yunpeng Chen",
                "Tao Wang",
                "Weihao Yu",
                "Yujun Shi",
                "Zihang Jiang",
                "Francis E. H. Tay",
                "Jiashi Feng",
                "Shuicheng Yan"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00060",
            "concepts": [
                "Security token",
                "Computer science",
                "Transformer",
                "Artificial intelligence",
                "Pixel",
                "Scratch",
                "Pattern recognition (psychology)",
                "Lexical analysis",
                "Speech recognition",
                "Programming language",
                "Computer network",
                "Engineering",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2101.11986",
            "abstract": "Transformers, which are popular for language modeling, have been explored for solving vision tasks recently, e.g., the Vision Transformer (ViT) for image classification. The ViT model splits each image into a sequence of tokens with fixed length and then applies multiple Transformer layers to model their global relation for classification. However, ViT achieves inferior performance to CNNs when trained from scratch on a midsize dataset like ImageNet. We find it is because: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines among neighboring pixels, leading to low training sample efficiency; 2) the redundant attention backbone design of ViT leads to limited feature richness for fixed computation budgets and limited training samples. To overcome such limitations, we propose a new Tokens-To-Token Vision Transformer (T2T-VTT), which incorporates 1) a layer-wise Tokens-to-Token (T2T) transformation to progressively structurize the image to tokens by recursively aggregating neighboring Tokens into one Token (Tokens-to-Token), such that local structure represented by surrounding tokens can be modeled and tokens length can be reduced; 2) an efficient backbone with a deep-narrow structure for vision transformer motivated by CNN architecture design after empirical study. Notably, T2T-ViT reduces the parameter count and MACs of vanilla ViT by half, while achieving more than 3.0% improvement when trained from scratch on ImageNet. It also outperforms ResNets and achieves comparable performance with MobileNets by directly training on ImageNet. For example, T2T-ViT with comparable size to ResNet50 (21.5M parameters) can achieve 83.3% top1 accuracy in image resolution 384x384 on ImageNet. <sup xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">1</sup>"
        },
        "https://openalex.org/W4214493665": {
            "title": "CvT: Introducing Convolutions to Vision Transformers",
            "openalex_id": "https://openalex.org/W4214493665",
            "cited_by_count": 1739,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4382323155",
                "https://openalex.org/W4315697128",
                "https://openalex.org/W4292794827",
                "https://openalex.org/W4280599700",
                "https://openalex.org/W4224939635",
                "https://openalex.org/W3205506801",
                "https://openalex.org/W3183570023",
                "https://openalex.org/W3102845713",
                "https://openalex.org/W2971502891",
                "https://openalex.org/W2016508734"
            ],
            "references": [
                "https://openalex.org/W1928278792",
                "https://openalex.org/W1977295328",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2531409750",
                "https://openalex.org/W2533598788",
                "https://openalex.org/W2896457183",
                "https://openalex.org/W2908510526",
                "https://openalex.org/W2923014074",
                "https://openalex.org/W2952809536"
            ],
            "authors": [
                "Haiping Wu",
                "Bin Xiao",
                "Noel Codella",
                "Mengchen Liu",
                "Xiyang Dai",
                "Lu Yuan",
                "Lei Zhang"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00009",
            "concepts": [
                "Transformer",
                "Computer science",
                "Convolutional neural network",
                "FLOPS",
                "Embedding",
                "Artificial intelligence",
                "Convolutional code",
                "Computer engineering",
                "Pattern recognition (psychology)",
                "Algorithm",
                "Parallel computing",
                "Decoding methods",
                "Engineering",
                "Electrical engineering",
                "Voltage"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.15808",
            "abstract": "We present in this paper a new architecture, named Convolutional vision Transformer (CvT), that improves Vision Transformer (ViT) in performance and efficiency by introducing convolutions into ViT to yield the best of both de-signs. This is accomplished through two primary modifications: a hierarchy of Transformers containing a new convolutional token embedding, and a convolutional Transformer block leveraging a convolutional projection. These changes introduce desirable properties of convolutional neural networks (CNNs) to the ViT architecture (i.e. shift, scale, and distortion invariance) while maintaining the merits of Transformers (i.e. dynamic attention, global context, and better generalization). We validate CvT by conducting extensive experiments, showing that this approach achieves state-of-the-art performance over other Vision Transformers and ResNets on ImageNet-1k, with fewer parameters and lower FLOPs. In addition, performance gains are maintained when pretrained on larger datasets (e.g. ImageNet-22k) and fine-tuned to downstream tasks. Pretrained on ImageNet-22k, our CvT-W24 obtains a top-1 accuracy of 87.7% on the ImageNet-1k val set. Finally, our results show that the positional encoding, a crucial component in existing Vision Transformers, can be safely re-moved in our model, simplifying the design for higher resolution vision tasks. Code will be released at https://github.com/microsoft/CvT."
        },
        "https://openalex.org/W4214612132": {
            "title": "ViViT: A Video Vision Transformer",
            "openalex_id": "https://openalex.org/W4214612132",
            "cited_by_count": 1696,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4383066092",
                "https://openalex.org/W4380075502",
                "https://openalex.org/W4375867731",
                "https://openalex.org/W4304166257",
                "https://openalex.org/W4294635752",
                "https://openalex.org/W4230611425",
                "https://openalex.org/W2787993192",
                "https://openalex.org/W2731899572",
                "https://openalex.org/W2611989081",
                "https://openalex.org/W2158269427"
            ],
            "references": [
                "https://openalex.org/W1522734439",
                "https://openalex.org/W1533861849",
                "https://openalex.org/W1923404803",
                "https://openalex.org/W2016053056",
                "https://openalex.org/W2020163092",
                "https://openalex.org/W2068611653",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2156303437",
                "https://openalex.org/W2163605009"
            ],
            "authors": [
                "Anurag Arnab",
                "Mostafa Dehghani",
                "Georg Heigold",
                "Chen Sun",
                "Mario Lu\u010di\u0107",
                "Cordelia Schmid"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00676",
            "concepts": [
                "Computer science",
                "Leverage (statistics)",
                "Transformer",
                "Artificial intelligence",
                "Deep learning",
                "Pattern recognition (psychology)",
                "Machine learning",
                "Computer vision",
                "Engineering",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.15691",
            "abstract": "We present pure-transformer based models for video classification, drawing upon the recent success of such models in image classification. Our model extracts spatiotemporal tokens from the input video, which are then encoded by a series of transformer layers. In order to handle the long sequences of tokens encountered in video, we propose several, efficient variants of our model which factorise the spatial- and temporal-dimensions of the input. Although transformer-based models are known to only be effective when large training datasets are available, we show how we can effectively regularise the model during training and leverage pretrained image models to be able to train on comparatively small datasets. We conduct thorough ablation studies, and achieve state-of-the-art results on multiple video classification benchmarks including Kinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time, outperforming prior methods based on deep 3D convolutional networks."
        },
        "https://openalex.org/W3171125843": {
            "title": "Pre-Trained Image Processing Transformer",
            "openalex_id": "https://openalex.org/W3171125843",
            "cited_by_count": 1602,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W915438175",
                "https://openalex.org/W4380075502",
                "https://openalex.org/W4321353415",
                "https://openalex.org/W4246352526",
                "https://openalex.org/W2745001401",
                "https://openalex.org/W2378211422",
                "https://openalex.org/W2130974462",
                "https://openalex.org/W2121910908",
                "https://openalex.org/W2086519370",
                "https://openalex.org/W2028665553"
            ],
            "references": [
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1885185971",
                "https://openalex.org/W1906770428",
                "https://openalex.org/W1988952689",
                "https://openalex.org/W2047710600",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2170590026",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2242218935",
                "https://openalex.org/W2256362396"
            ],
            "authors": [
                "Hanting Chen",
                "Yunhe Wang",
                "Tianyu Guo",
                "Chang Xu",
                "Yiping Deng",
                "Zhenhua Liu",
                "Siwei Ma",
                "Chunjing Xu",
                "Chao Xu",
                "Wen Gao"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr46437.2021.01212",
            "concepts": [
                "Computer science",
                "Transformer",
                "Artificial intelligence",
                "Benchmark (surveying)",
                "Deep learning",
                "Image processing",
                "Machine learning",
                "Pattern recognition (psychology)",
                "Image (mathematics)",
                "Voltage",
                "Engineering",
                "Geodesy",
                "Geography",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2012.00364",
            "abstract": "As the computing power of modern hardware is increasing strongly, pre-trained deep learning models (e.g., BERT, GPT-3) learned on large-scale datasets have shown their effectiveness over conventional methods. The big progress is mainly contributed to the representation ability of transformer and its variant architectures. In this paper, we study the low-level computer vision task (e.g., denoising, super-resolution and deraining) and develop a new pre-trained model, namely, image processing transformer (IPT). To maximally excavate the capability of transformer, we present to utilize the well-known ImageNet benchmark for generating a large amount of corrupted image pairs. The IPT model is trained on these images with multi-heads and multi-tails. In addition, the contrastive learning is introduced for well adapting to different image processing tasks. The pre-trained model can therefore efficiently employed on desired task after fine-tuning. With only one pre-trained model, IPT outperforms the current state-of-the-art methods on various low-level benchmarks. Code is available at https://github.com/huawei-noah/Pretrained-IPT and https://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/IPT"
        },
        "https://openalex.org/W3090449556": {
            "title": "UNITER: UNiversal Image-TExt Representation Learning",
            "openalex_id": "https://openalex.org/W3090449556",
            "cited_by_count": 1578,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4380190185",
                "https://openalex.org/W4298897568",
                "https://openalex.org/W4290852288",
                "https://openalex.org/W3217388757",
                "https://openalex.org/W3215212336",
                "https://openalex.org/W3204607391",
                "https://openalex.org/W3164229987",
                "https://openalex.org/W3122720459",
                "https://openalex.org/W2964413124",
                "https://openalex.org/W1938708284"
            ],
            "references": [
                "https://openalex.org/W1773149199",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1933349210",
                "https://openalex.org/W2109586012",
                "https://openalex.org/W2251512949",
                "https://openalex.org/W2277195237",
                "https://openalex.org/W2321533354",
                "https://openalex.org/W2326925005",
                "https://openalex.org/W2489434015",
                "https://openalex.org/W2525778437"
            ],
            "authors": [
                "Yen-Chun Chen",
                "Linjie Li",
                "Licheng Yu",
                "Ahmed El Kholy",
                "Faisal Ahmed",
                "Zhe Gan",
                "Yu Cheng",
                "Jingjing Liu"
            ],
            "venue": "Lecture notes in computer science",
            "doi": "https://doi.org/10.1007/978-3-030-58577-8_7",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Masking (illustration)",
                "Natural language processing",
                "Language model",
                "Closed captioning",
                "Image (mathematics)",
                "Question answering",
                "Pattern recognition (psychology)",
                "Art",
                "Visual arts"
            ],
            "type": "book-chapter",
            "language": "en",
            "is_oa": false,
            "oa_url": "https://arxiv.org/pdf/1909.11740"
        },
        "https://openalex.org/W3019166713": {
            "title": "A Survey of the Usages of Deep Learning for Natural Language Processing",
            "openalex_id": "https://openalex.org/W3019166713",
            "cited_by_count": 1547,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4383066092",
                "https://openalex.org/W4375867731",
                "https://openalex.org/W4304166257",
                "https://openalex.org/W4294635752",
                "https://openalex.org/W4283262748",
                "https://openalex.org/W4230611425",
                "https://openalex.org/W3215138031",
                "https://openalex.org/W2804383999",
                "https://openalex.org/W2731899572",
                "https://openalex.org/W2611989081"
            ],
            "references": [
                "https://openalex.org/W102708294",
                "https://openalex.org/W1494910745",
                "https://openalex.org/W1504913471",
                "https://openalex.org/W1505640990",
                "https://openalex.org/W1505680913",
                "https://openalex.org/W1517853909",
                "https://openalex.org/W1525482321",
                "https://openalex.org/W1538131130",
                "https://openalex.org/W1540449438",
                "https://openalex.org/W1541301615"
            ],
            "authors": [
                "Daniel W. Otter",
                "Julian Richard Medina",
                "Jugal Kalita"
            ],
            "venue": "IEEE Transactions on Neural Networks and Learning Systems",
            "doi": "https://doi.org/10.1109/tnnls.2020.2979670",
            "concepts": [
                "Computer science",
                "Field (mathematics)",
                "Deep learning",
                "Artificial intelligence",
                "Natural (archaeology)",
                "Data science",
                "Computational linguistics",
                "Natural language processing",
                "Cognitive science",
                "History",
                "Psychology",
                "Archaeology",
                "Mathematics",
                "Pure mathematics"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1109/tnnls.2020.2979670",
            "abstract": "Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This article provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to many applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field."
        },
        "https://openalex.org/W3153465022": {
            "title": "PCT: Point cloud transformer",
            "openalex_id": "https://openalex.org/W3153465022",
            "cited_by_count": 1458,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4389574804",
                "https://openalex.org/W4375867731",
                "https://openalex.org/W3150655618",
                "https://openalex.org/W3108295644",
                "https://openalex.org/W3016928466",
                "https://openalex.org/W2936725271",
                "https://openalex.org/W2626737336",
                "https://openalex.org/W2114282491",
                "https://openalex.org/W2081900870",
                "https://openalex.org/W1578717197"
            ],
            "references": [
                "https://openalex.org/W1662382123",
                "https://openalex.org/W1920022804",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2553307952",
                "https://openalex.org/W2560609797",
                "https://openalex.org/W2597655663",
                "https://openalex.org/W2626778328",
                "https://openalex.org/W2750779823",
                "https://openalex.org/W2752782242",
                "https://openalex.org/W2765754958"
            ],
            "authors": [
                "Meng-Hao Guo",
                "Jun-Xiong Cai",
                "Zheng-Ning Liu",
                "Tai\u2010Jiang Mu",
                "Ralph R. Martin",
                "Shi\u2010Min Hu"
            ],
            "venue": "Computational Visual Media",
            "doi": "https://doi.org/10.1007/s41095-021-0229-5",
            "concepts": [
                "Point cloud",
                "Computer science",
                "Segmentation",
                "Embedding",
                "Transformer",
                "Artificial intelligence",
                "Deep learning",
                "Engineering",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://link.springer.com/content/pdf/10.1007/s41095-021-0229-5.pdf",
            "abstract": "The irregular domain and lack of ordering make it challenging to design deep neural networks for point cloud processing. This paper presents a novel framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is based on Transformer, which achieves huge success in natural language processing and displays great potential in image processing. It is inherently permutation invariant for processing a sequence of points, making it well-suited for point cloud learning. To better capture local context within the point cloud, we enhance input embedding with the support of farthest point sampling and nearest neighbor search. Extensive experiments demonstrate that the PCT achieves the state-of-the-art performance on shape classification, part segmentation and normal estimation tasks."
        },
        "https://openalex.org/W4312349930": {
            "title": "Swin Transformer V2: Scaling Up Capacity and Resolution",
            "openalex_id": "https://openalex.org/W4312349930",
            "cited_by_count": 1451,
            "publication_year": 2022,
            "related_works": [
                "https://openalex.org/W4321441197",
                "https://openalex.org/W4321276295",
                "https://openalex.org/W4294432981",
                "https://openalex.org/W3011538607",
                "https://openalex.org/W2953716828",
                "https://openalex.org/W2944728705",
                "https://openalex.org/W2904022177",
                "https://openalex.org/W2591697403",
                "https://openalex.org/W2469820710",
                "https://openalex.org/W2359348847"
            ],
            "references": [
                "https://openalex.org/W1686810756",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1861492603",
                "https://openalex.org/W2097117768",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2112796928",
                "https://openalex.org/W2163605009",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2338908902",
                "https://openalex.org/W2502312327"
            ],
            "authors": [
                "Ze Liu",
                "Han Hu",
                "Yutong Lin",
                "Zhuliang Yao",
                "Zhenda Xie",
                "Yixuan Wei",
                "Ning Jia",
                "Yue Cao",
                "Zheng Zhang",
                "Li Dong",
                "Furu Wei",
                "Baining Guo"
            ],
            "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
            "doi": "https://doi.org/10.1109/cvpr52688.2022.01170",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Transformer",
                "Normalization (sociology)",
                "Scaling",
                "Segmentation",
                "Computer vision",
                "Pattern recognition (psychology)",
                "Voltage",
                "Engineering",
                "Geometry",
                "Mathematics",
                "Sociology",
                "Anthropology",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2111.09883",
            "abstract": "We present techniques for scaling Swin Transformer [35] up to 3 billion parameters and making it capable of training with images of up to 1,536x1,536 resolution. By scaling up capacity and resolution, Swin Transformer sets new records on four representative vision benchmarks: 84.0% top-1 accuracy on ImageNet- V2 image classification, 63.1 / 54.4 box / mask mAP on COCO object detection, 59.9 mIoU on ADE20K semantic segmentation, and 86.8% top-1 accuracy on Kinetics-400 video action classification. We tackle issues of training instability, and study how to effectively transfer models pre-trained at low resolutions to higher resolution ones. To this aim, several novel technologies are proposed: 1) a residual post normalization technique and a scaled cosine attention approach to improve the stability of large vision models; 2) a log-spaced continuous position bias technique to effectively transfer models pre-trained at low-resolution images and windows to their higher-resolution counterparts. In addition, we share our crucial implementation details that lead to significant savings of GPU memory consumption and thus make it feasi-ble to train large vision models with regular GPUs. Using these techniques and self-supervised pre-training, we suc-cessfully train a strong 3 billion Swin Transformer model and effectively transfer it to various vision tasks involving high-resolution images or windows, achieving the state-of-the-art accuracy on a variety of benchmarks. Code is avail-able at https://github.com/microsoft/Swin-Transformer."
        },
        "https://openalex.org/W3046375318": {
            "title": "Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing",
            "openalex_id": "https://openalex.org/W3046375318",
            "cited_by_count": 1441,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W69308499",
                "https://openalex.org/W4387929264",
                "https://openalex.org/W4387517132",
                "https://openalex.org/W4288365749",
                "https://openalex.org/W4287903637",
                "https://openalex.org/W3105220303",
                "https://openalex.org/W2999168658",
                "https://openalex.org/W2936497627",
                "https://openalex.org/W2918609062",
                "https://openalex.org/W2901701848"
            ],
            "references": [
                "https://openalex.org/W1522301498",
                "https://openalex.org/W154351976",
                "https://openalex.org/W1566289585",
                "https://openalex.org/W1614298861",
                "https://openalex.org/W1905522558",
                "https://openalex.org/W2034269086",
                "https://openalex.org/W2047782770",
                "https://openalex.org/W2064675550",
                "https://openalex.org/W2075201173",
                "https://openalex.org/W2112227057"
            ],
            "authors": [
                "\u88d5\u4e8c \u6c60\u8c37",
                "Robert Tinn",
                "Hao Cheng",
                "Michael Lucas",
                "Naoto Usuyama",
                "Xiaodong Liu",
                "Tristan Naumann",
                "Jianfeng Gao",
                "Hoifung Poon"
            ],
            "venue": "ACM Transactions on Computing for Healthcare",
            "doi": "https://doi.org/10.1145/3458754",
            "concepts": [
                "Computer science",
                "Benchmark (surveying)",
                "Artificial intelligence",
                "Domain (mathematical analysis)",
                "Natural language processing",
                "Language model",
                "Task (project management)",
                "Biomedicine",
                "AKA",
                "Language understanding",
                "Variety (cybernetics)",
                "Named-entity recognition",
                "Bioinformatics",
                "Mathematical analysis",
                "Mathematics",
                "Management",
                "Geodesy",
                "Library science",
                "Economics",
                "Biology",
                "Geography"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2007.15779",
            "abstract": "Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning Benchmark) at https://aka.ms/BLURB ."
        },
        "https://openalex.org/W4214893857": {
            "title": "Segmenter: Transformer for Semantic Segmentation",
            "openalex_id": "https://openalex.org/W4214893857",
            "cited_by_count": 1431,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W3116076068",
                "https://openalex.org/W2951359407",
                "https://openalex.org/W2775347418",
                "https://openalex.org/W2772917594",
                "https://openalex.org/W2755342338",
                "https://openalex.org/W2229312674",
                "https://openalex.org/W2166024367",
                "https://openalex.org/W2079911747",
                "https://openalex.org/W2058170566",
                "https://openalex.org/W1969923398"
            ],
            "references": [
                "https://openalex.org/W1546771929",
                "https://openalex.org/W1836465849",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W1923697677",
                "https://openalex.org/W1994616650",
                "https://openalex.org/W2022508996",
                "https://openalex.org/W2095705004",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2125215748"
            ],
            "authors": [
                "Robin Strudel",
                "Ricardo Garc\u00eda",
                "Ivan Laptev",
                "Cordelia Schmid"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00717",
            "concepts": [
                "Computer science",
                "Segmentation",
                "Transformer",
                "Artificial intelligence",
                "Natural language processing",
                "Computer vision",
                "Engineering",
                "Electrical engineering",
                "Voltage"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://hal.science/hal-03481207/document",
            "abstract": "Image segmentation is often ambiguous at the level of individual image patches and requires contextual information to reach label consensus. In this paper we introduce Segmenter, a transformer model for semantic segmentation. In contrast to convolution-based methods, our approach allows to model global context already at the first layer and throughout the network. We build on the recent Vision Transformer (ViT) and extend it to semantic segmentation. To do so, we rely on the output embeddings corresponding to image patches and obtain class labels from these embed-dings with a point-wise linear decoder or a mask trans-former decoder. We leverage models pre-trained for image classification and show that we can fine-tune them on moderate sized datasets available for semantic segmentation. The linear decoder allows to obtain excellent results already, but the performance can be further improved by a mask transformer generating class masks. We conduct an extensive ablation study to show the impact of the different parameters, in particular the performance is better for large models and small patch sizes. Segmenter attains excellent results for semantic segmentation. It outperforms the state of the art on both ADE20K and Pascal Context datasets and is competitive on Cityscapes."
        },
        "https://openalex.org/W4214755140": {
            "title": "Point Transformer",
            "openalex_id": "https://openalex.org/W4214755140",
            "cited_by_count": 1423,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W3150655618",
                "https://openalex.org/W3016928466",
                "https://openalex.org/W2980582925",
                "https://openalex.org/W2979603868",
                "https://openalex.org/W2936725271",
                "https://openalex.org/W2862230042",
                "https://openalex.org/W2626737336",
                "https://openalex.org/W2562256921",
                "https://openalex.org/W2005998065",
                "https://openalex.org/W1522196789"
            ],
            "references": [
                "https://openalex.org/W1644641054",
                "https://openalex.org/W1920022804",
                "https://openalex.org/W2211722331",
                "https://openalex.org/W2460657278",
                "https://openalex.org/W2553307952",
                "https://openalex.org/W2555618208",
                "https://openalex.org/W2556802233",
                "https://openalex.org/W2557465155",
                "https://openalex.org/W2560609797",
                "https://openalex.org/W2606202972"
            ],
            "authors": [
                "Hengshuang Zhao",
                "Li Jiang",
                "Jiaya Jia",
                "Philip H. S. Torr",
                "Vladlen Koltun"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.01595",
            "concepts": [
                "Computer science",
                "Segmentation",
                "Artificial intelligence",
                "Point cloud",
                "Transformer",
                "Image segmentation",
                "Computer vision",
                "Object detection",
                "Point (geometry)",
                "Pattern recognition (psychology)",
                "Engineering",
                "Mathematics",
                "Geometry",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "Self-attention networks have revolutionized natural language processing and are making impressive strides in image analysis tasks such as image classification and object detection. Inspired by this success, we investigate the application of self-attention networks to 3D point cloud processing. We design self-attention layers for point clouds and use these to construct self-attention networks for tasks such as semantic scene segmentation, object part segmentation, and object classification. Our Point Transformer design improves upon prior work across domains and tasks. For example, on the challenging S3DIS dataset for large-scale semantic scene segmentation, the Point Transformer attains an mIoU of 70.4% on Area 5, outperforming the strongest prior model by 3.3 absolute percentage points and crossing the 70% mIoU threshold for the first time."
        },
        "https://openalex.org/W3177500196": {
            "title": "ProtTrans: Toward Understanding the Language of Life Through Self-Supervised Learning",
            "openalex_id": "https://openalex.org/W3177500196",
            "cited_by_count": 1384,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4394896187",
                "https://openalex.org/W4386462264",
                "https://openalex.org/W4364306694",
                "https://openalex.org/W4312192474",
                "https://openalex.org/W4306674287",
                "https://openalex.org/W4283697347",
                "https://openalex.org/W3170094116",
                "https://openalex.org/W3107602296",
                "https://openalex.org/W3046775127",
                "https://openalex.org/W2961085424"
            ],
            "references": [
                "https://openalex.org/W1499450468",
                "https://openalex.org/W1501531009",
                "https://openalex.org/W1982825626",
                "https://openalex.org/W1985818354",
                "https://openalex.org/W1996073320",
                "https://openalex.org/W2013136212",
                "https://openalex.org/W2029476353",
                "https://openalex.org/W2030395559",
                "https://openalex.org/W2051545676",
                "https://openalex.org/W2057820165"
            ],
            "authors": [
                "Ahmed Elnaggar",
                "Michael Heinzinger",
                "Christian Dallago",
                "Ghalia Rehawi",
                "Yu Wang",
                "Llion Jones",
                "Tom Gibbs",
                "T. Feh\u00e9r",
                "Christoph Angerer",
                "Martin Steinegger",
                "Debsindhu Bhowmik",
                "Burkhard Rost"
            ],
            "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence",
            "doi": "https://doi.org/10.1109/tpami.2021.3095381",
            "concepts": [
                "Computer science",
                "Artificial intelligence",
                "Machine learning",
                "Language acquisition",
                "Natural language processing",
                "Psychology",
                "Mathematics education"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://ieeexplore.ieee.org/ielx7/34/9893033/09477085.pdf",
            "abstract": "Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models (LMs) taken from Natural Language Processing (NLP). These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The protein LMs (pLMs) were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw pLM- <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">embeddings</i> from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">embeddings</i> as exclusive input for several subsequent tasks: (1) a per-residue (per-token) prediction of protein secondary structure (3-state accuracy Q3=81%-87%); (2) per-protein (pooling) predictions of protein sub-cellular location (ten-state accuracy: Q10=81%) and membrane versus water-soluble (2-state accuracy Q2=91%). For secondary structure, the most informative embeddings (ProtT5) for the first time outperformed the state-of-the-art without multiple sequence alignments (MSAs) or evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that pLMs learned some of the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">grammar</i> of the <italic xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">language of life</i> . All our models are available through <uri xmlns:mml=\"http://www.w3.org/1998/Math/MathML\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">https://github.com/agemagician/ProtTrans</uri> ."
        },
        "https://openalex.org/W3151130473": {
            "title": "CrossViT: Cross-Attention Multi-Scale Vision Transformer for Image Classification",
            "openalex_id": "https://openalex.org/W3151130473",
            "cited_by_count": 1376,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4388335561",
                "https://openalex.org/W4385572700",
                "https://openalex.org/W4385009901",
                "https://openalex.org/W4307309205",
                "https://openalex.org/W4304700937",
                "https://openalex.org/W4288261899",
                "https://openalex.org/W4285141722",
                "https://openalex.org/W2997152889",
                "https://openalex.org/W2970530566",
                "https://openalex.org/W2967478618"
            ],
            "references": [
                "https://openalex.org/W1977295328",
                "https://openalex.org/W1978491093",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2150134853",
                "https://openalex.org/W2194775991",
                "https://openalex.org/W2307770531",
                "https://openalex.org/W2473156356",
                "https://openalex.org/W2490270993",
                "https://openalex.org/W2549139847",
                "https://openalex.org/W2560533888"
            ],
            "authors": [
                "Chun-Fu Richard Chen",
                "Quanfu Fan",
                "Rameswar Panda"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.00041",
            "concepts": [
                "Computer science",
                "Transformer",
                "Artificial intelligence",
                "Security token",
                "Computational complexity theory",
                "Convolutional neural network",
                "Computation",
                "Pattern recognition (psychology)",
                "Algorithm",
                "Physics",
                "Computer security",
                "Quantum mechanics",
                "Voltage"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.14899",
            "abstract": "The recently developed vision transformer (ViT) has achieved promising results on image classification compared to convolutional neural networks. Inspired by this, in this paper, we study how to learn multi-scale feature representations in transformer models for image classification. To this end, we propose a dual-branch transformer to com-bine image patches (i.e., tokens in a transformer) of different sizes to produce stronger image features. Our approach processes small-patch and large-patch tokens with two separate branches of different computational complexity and these tokens are then fused purely by attention multiple times to complement each other. Furthermore, to reduce computation, we develop a simple yet effective token fusion module based on cross attention, which uses a single token for each branch as a query to exchange information with other branches. Our proposed cross-attention only requires linear time for both computational and memory complexity instead of quadratic time otherwise. Extensive experiments demonstrate that our approach performs better than or on par with several concurrent works on vision transformer, in addition to efficient CNN models. For example, on the ImageNet1K dataset, with some architectural changes, our approach outperforms the recent DeiT by a large margin of 2% with a small to moderate increase in FLOPs and model parameters. Our source codes and models are available at https://github.com/IBM/CrossViT."
        },
        "https://openalex.org/W3209059054": {
            "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units",
            "openalex_id": "https://openalex.org/W3209059054",
            "cited_by_count": 1348,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W3177678247",
                "https://openalex.org/W2944572343",
                "https://openalex.org/W2562096895",
                "https://openalex.org/W2529301793",
                "https://openalex.org/W2391730868",
                "https://openalex.org/W2384121599",
                "https://openalex.org/W2333799855",
                "https://openalex.org/W2140536630",
                "https://openalex.org/W2038083449",
                "https://openalex.org/W1999617572"
            ],
            "references": [
                "https://openalex.org/W1494198834",
                "https://openalex.org/W1522301498",
                "https://openalex.org/W1553004968",
                "https://openalex.org/W2032210463",
                "https://openalex.org/W2100768664",
                "https://openalex.org/W2101234009",
                "https://openalex.org/W2110073835",
                "https://openalex.org/W2127141656",
                "https://openalex.org/W2150593711",
                "https://openalex.org/W2155273149"
            ],
            "authors": [
                "Wei-Ning Hsu",
                "Benjamin Bolte",
                "Yao-Hung Hubert Tsai",
                "Kushal Lakhotia",
                "Ruslan Salakhutdinov",
                "Abdelrahman Mohamed"
            ],
            "venue": "IEEE/ACM Transactions on Audio Speech and Language Processing",
            "doi": "https://doi.org/10.1109/taslp.2021.3122291",
            "concepts": [
                "Computer science",
                "Cluster analysis",
                "Representation (politics)",
                "Consistency (knowledge bases)",
                "Artificial intelligence",
                "Lexicon",
                "Speech recognition",
                "Utterance",
                "Segmentation",
                "Language model",
                "Machine learning",
                "Pattern recognition (psychology)",
                "Politics",
                "Political science",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2106.07447",
            "abstract": "Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960h) and Libri-light (60,000h) benchmarks with 10min, 1h, 10h, 100h, and 960h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets."
        },
        "https://openalex.org/W4214520160": {
            "title": "Vision Transformers for Dense Prediction",
            "openalex_id": "https://openalex.org/W4214520160",
            "cited_by_count": 1320,
            "publication_year": 2021,
            "related_works": [
                "https://openalex.org/W4297686120",
                "https://openalex.org/W4286681602",
                "https://openalex.org/W3209312100",
                "https://openalex.org/W2493838176",
                "https://openalex.org/W2349160795",
                "https://openalex.org/W2100576949",
                "https://openalex.org/W2100057527",
                "https://openalex.org/W2069133146",
                "https://openalex.org/W2017545316",
                "https://openalex.org/W1663079876"
            ],
            "references": [
                "https://openalex.org/W125693051",
                "https://openalex.org/W1522301498",
                "https://openalex.org/W1745334888",
                "https://openalex.org/W1901129140",
                "https://openalex.org/W1903029394",
                "https://openalex.org/W2108598243",
                "https://openalex.org/W2125215748",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2149259880",
                "https://openalex.org/W2150066425"
            ],
            "authors": [
                "Ren\u00e9 Ranftl",
                "Alexey Bochkovskiy",
                "Vladlen Koltun"
            ],
            "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)",
            "doi": "https://doi.org/10.1109/iccv48922.2021.01196",
            "concepts": [
                "Computer science",
                "Transformer",
                "Pascal (unit)",
                "Artificial intelligence",
                "Architecture",
                "Convolutional neural network",
                "Segmentation",
                "Pattern recognition (psychology)",
                "Computer engineering",
                "Computer vision",
                "Voltage",
                "Engineering",
                "Art",
                "Electrical engineering",
                "Visual arts",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://arxiv.org/pdf/2103.13413",
            "abstract": "We introduce dense prediction transformers, an architecture that leverages vision transformers in place of convolutional networks as a backbone for dense prediction tasks. We assemble tokens from various stages of the vision transformer into image-like representations at various resolutions and progressively combine them into full-resolution predictions using a convolutional decoder. The transformer backbone processes representations at a constant and relatively high resolution and has a global receptive field at every stage. These properties allow the dense prediction transformer to provide finer-grained and more globally coherent predictions when compared to fully-convolutional networks. Our experiments show that this architecture yields substantial improvements on dense prediction tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense prediction transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art. Our models are available at https://github.com/intel-isl/DPT."
        },
        "https://openalex.org/W3118485687": {
            "title": "A Primer in BERTology: What We Know About How BERT Works",
            "openalex_id": "https://openalex.org/W3118485687",
            "cited_by_count": 1317,
            "publication_year": 2020,
            "related_works": [
                "https://openalex.org/W4312713546",
                "https://openalex.org/W4205824991",
                "https://openalex.org/W3200723557",
                "https://openalex.org/W2802298219",
                "https://openalex.org/W2588198209",
                "https://openalex.org/W2567983276",
                "https://openalex.org/W2393996461",
                "https://openalex.org/W2362195430",
                "https://openalex.org/W2347494122",
                "https://openalex.org/W1909006023"
            ],
            "references": [
                "https://openalex.org/W1550933260",
                "https://openalex.org/W1821462560",
                "https://openalex.org/W2525778437",
                "https://openalex.org/W2599674900",
                "https://openalex.org/W2607074821",
                "https://openalex.org/W2612690371",
                "https://openalex.org/W2626778328",
                "https://openalex.org/W2799424953",
                "https://openalex.org/W2896457183",
                "https://openalex.org/W2898700502"
            ],
            "authors": [
                "Anna Rogers",
                "Olga Kovaleva",
                "Anna Rumshisky"
            ],
            "venue": "Transactions of the Association for Computational Linguistics",
            "doi": "https://doi.org/10.1162/tacl_a_00349",
            "concepts": [
                "Computer science",
                "Transformer",
                "Architecture",
                "State (computer science)",
                "Data science",
                "Artificial intelligence",
                "History",
                "Programming language",
                "Archaeology",
                "Physics",
                "Quantum mechanics",
                "Voltage"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00349/1923281/tacl_a_00349.pdf",
            "abstract": "Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research."
        },
        "https://openalex.org/W4382203079": {
            "title": "Are Transformers Effective for Time Series Forecasting?",
            "openalex_id": "https://openalex.org/W4382203079",
            "cited_by_count": 1303,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4386462264",
                "https://openalex.org/W4306674287",
                "https://openalex.org/W4306321456",
                "https://openalex.org/W4286629047",
                "https://openalex.org/W4285260836",
                "https://openalex.org/W4224009465",
                "https://openalex.org/W4205958290",
                "https://openalex.org/W3170094116",
                "https://openalex.org/W3046775127",
                "https://openalex.org/W2961085424"
            ],
            "references": [
                "https://openalex.org/W1536447791",
                "https://openalex.org/W1678356000",
                "https://openalex.org/W1969852690",
                "https://openalex.org/W2127334389",
                "https://openalex.org/W2133564696",
                "https://openalex.org/W2230528047",
                "https://openalex.org/W2604847698",
                "https://openalex.org/W2792764867",
                "https://openalex.org/W2892009249",
                "https://openalex.org/W2896457183"
            ],
            "authors": [
                "Ailing Zeng",
                "Muxi Chen",
                "Lei Zhang",
                "Qiang Xu"
            ],
            "venue": "Proceedings of the AAAI Conference on Artificial Intelligence",
            "doi": "https://doi.org/10.1609/aaai.v37i9.26317",
            "concepts": [
                "Transformer",
                "Computer science",
                "Data mining",
                "Time series",
                "Artificial intelligence",
                "Machine learning",
                "Engineering",
                "Voltage",
                "Electrical engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://ojs.aaai.org/index.php/AAAI/article/download/26317/26089",
            "abstract": "Recently, there has been a surge of Transformer-based solutions for the long-term time series forecasting (LTSF) task. Despite the growing performance over the past few years, we question the validity of this line of research in this work. Specifically, Transformers is arguably the most successful solution to extract the semantic correlations among the elements in a long sequence. However, in time series modeling, we are to extract the temporal relations in an ordered set of continuous points. While employing positional encoding and using tokens to embed sub-series in Transformers facilitate preserving some ordering information, the nature of the permutation-invariant self-attention mechanism inevitably results in temporal information loss. To validate our claim, we introduce a set of embarrassingly simple one-layer linear models named LTSF-Linear for comparison. Experimental results on nine real-life datasets show that LTSF-Linear surprisingly outperforms existing sophisticated Transformer-based LTSF models in all cases, and often by a large margin. Moreover, we conduct comprehensive empirical studies to explore the impacts of various design elements of LTSF models on their temporal relation extraction capability. We hope this surprising finding opens up new research directions for the LTSF task. We also advocate revisiting the validity of Transformer-based solutions for other time series analysis tasks (e.g., anomaly detection) in the future."
        }
    },
    "https://openalex.org/W4360620450": {
        "https://openalex.org/W4365512576": {
            "title": "ChatGPT: A comprehensive review on background, applications, key challenges, bias, ethics, limitations and future scope",
            "openalex_id": "https://openalex.org/W4365512576",
            "cited_by_count": 1816,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4280543773",
                "https://openalex.org/W4200375594",
                "https://openalex.org/W2808360891",
                "https://openalex.org/W2387622493",
                "https://openalex.org/W2366083136",
                "https://openalex.org/W2362452928",
                "https://openalex.org/W2360028903",
                "https://openalex.org/W2357832196",
                "https://openalex.org/W1932132538",
                "https://openalex.org/W178231042"
            ],
            "references": [
                "https://openalex.org/W1976732164",
                "https://openalex.org/W2046430955",
                "https://openalex.org/W2048189380",
                "https://openalex.org/W2066545696",
                "https://openalex.org/W2346062110",
                "https://openalex.org/W2506282302",
                "https://openalex.org/W2530395818",
                "https://openalex.org/W2612890464",
                "https://openalex.org/W2725331431",
                "https://openalex.org/W2736601468"
            ],
            "authors": [
                "Partha Pratim Ray"
            ],
            "venue": "Internet of Things and Cyber-Physical Systems",
            "doi": "https://doi.org/10.1016/j.iotcps.2023.04.003",
            "concepts": [
                "Scope (computer science)",
                "Engineering ethics",
                "Data science",
                "Computer science",
                "Knowledge management",
                "Management science",
                "Engineering",
                "Programming language"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.iotcps.2023.04.003",
            "abstract": "In recent years, artificial intelligence (AI) and machine learning have been transforming the landscape of scientific research. Out of which, the chatbot technology has experienced tremendous advancements in recent years, especially with ChatGPT emerging as a notable AI language model. This comprehensive review delves into the background, applications, key challenges, and future directions of ChatGPT. We begin by exploring its origins, development, and underlying technology, before examining its wide-ranging applications across industries such as customer service, healthcare, and education. We also highlight the critical challenges that ChatGPT faces, including ethical concerns, data biases, and safety issues, while discussing potential mitigation strategies. Finally, we envision the future of ChatGPT by exploring areas of further research and development, focusing on its integration with other technologies, improved human-AI interaction, and addressing the digital divide. This review offers valuable insights for researchers, developers, and stakeholders interested in the ever-evolving landscape of AI-driven conversational agents. This study explores the various ways ChatGPT has been revolutionizing scientific research, spanning from data processing and hypothesis generation to collaboration and public outreach. Furthermore, the paper examines the potential challenges and ethical concerns surrounding the use of ChatGPT in research, while highlighting the importance of striking a balance between AI-assisted innovation and human expertise. The paper presents several ethical issues in existing computing domain and how ChatGPT can invoke challenges to such notion. This work also includes some biases and limitations of ChatGPT. It is worth to note that despite of several controversies and ethical concerns, ChatGPT has attracted remarkable attentions from academia, research, and industries in a very short span of time."
        },
        "https://openalex.org/W4385071300": {
            "title": "Generative AI and ChatGPT: Applications, challenges, and AI-human collaboration",
            "openalex_id": "https://openalex.org/W4385071300",
            "cited_by_count": 807,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2530322880",
                "https://openalex.org/W2390279801",
                "https://openalex.org/W2382290278",
                "https://openalex.org/W2380075625",
                "https://openalex.org/W2376932109",
                "https://openalex.org/W2358668433",
                "https://openalex.org/W2350741829",
                "https://openalex.org/W2001405890",
                "https://openalex.org/W1596801655"
            ],
            "references": [
                "https://openalex.org/W2040484355",
                "https://openalex.org/W2136922672",
                "https://openalex.org/W2512367456",
                "https://openalex.org/W2522133792",
                "https://openalex.org/W2563424741",
                "https://openalex.org/W2594557664",
                "https://openalex.org/W2612690371",
                "https://openalex.org/W2735791018",
                "https://openalex.org/W2796133875",
                "https://openalex.org/W2899856450"
            ],
            "authors": [
                "Fiona Fui\u2010Hoon Nah",
                "Ruilin Zheng",
                "Jingyuan Cai",
                "Keng Siau",
                "Langtao Chen"
            ],
            "venue": "Journal of Information Technology Case and Application Research",
            "doi": "https://doi.org/10.1080/15228053.2023.2233814",
            "concepts": [
                "Generative grammar",
                "Computer science",
                "Knowledge management",
                "Data science",
                "Artificial intelligence"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.tandfonline.com/doi/pdf/10.1080/15228053.2023.2233814?needAccess=true"
        },
        "https://openalex.org/W4385878593": {
            "title": "New Era of Artificial Intelligence in Education: Towards a Sustainable Multifaceted Revolution",
            "openalex_id": "https://openalex.org/W4385878593",
            "cited_by_count": 592,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4323893170",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2380850119",
                "https://openalex.org/W2377237701",
                "https://openalex.org/W2360099860",
                "https://openalex.org/W2352463596",
                "https://openalex.org/W2101450440",
                "https://openalex.org/W1968552888",
                "https://openalex.org/W1583826057"
            ],
            "references": [
                "https://openalex.org/W2075950485",
                "https://openalex.org/W2092909411",
                "https://openalex.org/W2670772560",
                "https://openalex.org/W2810576341",
                "https://openalex.org/W2891378911",
                "https://openalex.org/W2901669506",
                "https://openalex.org/W2904197112",
                "https://openalex.org/W2917352236",
                "https://openalex.org/W2971598856",
                "https://openalex.org/W2974642226"
            ],
            "authors": [
                "Firuz Kamalov",
                "David Santandreu Calonge",
                "Ikhlaas Gurrib"
            ],
            "venue": "Sustainability",
            "doi": "https://doi.org/10.3390/su151612451",
            "concepts": [
                "Mainstream",
                "Software deployment",
                "Applications of artificial intelligence",
                "Computer science",
                "Conversation",
                "Artificial intelligence",
                "Engineering ethics",
                "Engineering management",
                "Engineering",
                "Knowledge management",
                "Psychology",
                "Political science",
                "Software engineering",
                "Communication",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/2071-1050/15/16/12451/pdf?version=1692181759",
            "abstract": "The recent high performance of ChatGPT on several standardized academic tests has thrust the topic of artificial intelligence (AI) into the mainstream conversation about the future of education. As deep learning is poised to shift the teaching paradigm, it is essential to have a clear understanding of its effects on the current education system to ensure sustainable development and deployment of AI-driven technologies at schools and universities. This research aims to investigate the potential impact of AI on education through review and analysis of the existing literature across three major axes: applications, advantages, and challenges. Our review focuses on the use of artificial intelligence in collaborative teacher\u2013student learning, intelligent tutoring systems, automated assessment, and personalized learning. We also report on the potential negative aspects, ethical issues, and possible future routes for AI implementation in education. Ultimately, we find that the only way forward is to embrace the new technology, while implementing guardrails to prevent its abuse."
        },
        "https://openalex.org/W4383913712": {
            "title": "Human resource management in the age of generative artificial intelligence: Perspectives and research directions on ChatGPT",
            "openalex_id": "https://openalex.org/W4383913712",
            "cited_by_count": 530,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W2392455911",
                "https://openalex.org/W2389147080",
                "https://openalex.org/W2387777532",
                "https://openalex.org/W2382709029",
                "https://openalex.org/W2377883125",
                "https://openalex.org/W2375492428",
                "https://openalex.org/W2374248756",
                "https://openalex.org/W2362479786",
                "https://openalex.org/W2359053655",
                "https://openalex.org/W2350419982"
            ],
            "references": [
                "https://openalex.org/W106141397",
                "https://openalex.org/W1501886583",
                "https://openalex.org/W1597835403",
                "https://openalex.org/W1791587663",
                "https://openalex.org/W1971765404",
                "https://openalex.org/W1973294727",
                "https://openalex.org/W1978484347",
                "https://openalex.org/W2036262779",
                "https://openalex.org/W2050422934",
                "https://openalex.org/W2072780901"
            ],
            "authors": [
                "Pawan Budhwar",
                "Soumyadeb Chowdhury",
                "Geoffrey Wood",
                "Herman Aguinis",
                "Greg J. Bamber",
                "Jose R. Beltran",
                "Paul Boselie",
                "Fang Lee Cooke",
                "Stephanie Decker",
                "Angelo S. DeNisi",
                "Prasanta Kumar Dey",
                "David Guest",
                "Andrew J. Knoblich",
                "Ashish Malik",
                "Jaap Paauwe",
                "Savvas Papagiannidis",
                "Charmi Patel",
                "Vijay Pereira",
                "Shuang Ren",
                "Steven G. Rogelberg",
                "Mark N. K. Saunders",
                "Rosalie L. Tung",
                "Arup Varma"
            ],
            "venue": "Human Resource Management Journal",
            "doi": "https://doi.org/10.1111/1748-8583.12524",
            "concepts": [
                "Generative grammar",
                "Context (archaeology)",
                "Stakeholder",
                "Scholarship",
                "Realm",
                "Knowledge management",
                "Sociology",
                "Engineering ethics",
                "Political science",
                "Artificial intelligence",
                "Public relations",
                "Computer science",
                "Engineering",
                "Law",
                "Paleontology",
                "Biology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/1748-8583.12524",
            "abstract": "Abstract ChatGPT and its variants that use generative artificial intelligence (AI) models have rapidly become a focal point in academic and media discussions about their potential benefits and drawbacks across various sectors of the economy, democracy, society, and environment. It remains unclear whether these technologies result in job displacement or creation, or if they merely shift human labour by generating new, potentially trivial or practically irrelevant, information and decisions. According to the CEO of ChatGPT, the potential impact of this new family of AI technology could be as big as \u201cthe printing press\u201d, with significant implications for employment, stakeholder relationships, business models, and academic research, and its full consequences are largely undiscovered and uncertain. The introduction of more advanced and potent generative AI tools in the AI market, following the launch of ChatGPT, has ramped up the \u201cAI arms race\u201d, creating continuing uncertainty for workers, expanding their business applications, while heightening risks related to well\u2010being, bias, misinformation, context insensitivity, privacy issues, ethical dilemmas, and security. Given these developments, this perspectives editorial offers a collection of perspectives and research pathways to extend HRM scholarship in the realm of generative AI. In doing so, the discussion synthesizes the literature on AI and generative AI, connecting it to various aspects of HRM processes, practices, relationships, and outcomes, thereby contributing to shaping the future of HRM research."
        },
        "https://openalex.org/W4386098991": {
            "title": "Challenges and Opportunities of Generative AI for Higher Education as Explained by ChatGPT",
            "openalex_id": "https://openalex.org/W4386098991",
            "cited_by_count": 484,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W60509564",
                "https://openalex.org/W4319841090",
                "https://openalex.org/W3156291593",
                "https://openalex.org/W2795995417",
                "https://openalex.org/W2749946359",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2372644570",
                "https://openalex.org/W2078423454",
                "https://openalex.org/W2030179342",
                "https://openalex.org/W1418625534"
            ],
            "references": [
                "https://openalex.org/W1977644847",
                "https://openalex.org/W1979290264",
                "https://openalex.org/W1993854686",
                "https://openalex.org/W2071478337",
                "https://openalex.org/W2267728153",
                "https://openalex.org/W2418575326",
                "https://openalex.org/W2624306658",
                "https://openalex.org/W2784291000",
                "https://openalex.org/W2936922813",
                "https://openalex.org/W2937671419"
            ],
            "authors": [
                "Rosario Michel\u2010Villarreal",
                "Eliseo Luis Vilalta-perdomo",
                "David Ernesto Salinas-Navarro",
                "Ricardo Thierry-Aguilera",
                "Flor Silvestre Gerardou"
            ],
            "venue": "Education Sciences",
            "doi": "https://doi.org/10.3390/educsci13090856",
            "concepts": [
                "Generative grammar",
                "Engineering ethics",
                "Perspective (graphical)",
                "Higher education",
                "Field (mathematics)",
                "Sociology",
                "Empirical research",
                "Knowledge management",
                "Ethnography",
                "Computer science",
                "Political science",
                "Epistemology",
                "Engineering",
                "Artificial intelligence",
                "Philosophy",
                "Mathematics",
                "Law",
                "Pure mathematics",
                "Anthropology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/2227-7102/13/9/856/pdf?version=1692771591",
            "abstract": "ChatGPT is revolutionizing the field of higher education by leveraging deep learning models to generate human-like content. However, its integration into academic settings raises concerns regarding academic integrity, plagiarism detection, and the potential impact on critical thinking skills. This article presents a study that adopts a thing ethnography approach to understand ChatGPT\u2019s perspective on the challenges and opportunities it represents for higher education. The research explores the potential benefits and limitations of ChatGPT, as well as mitigation strategies for addressing the identified challenges. Findings emphasize the urgent need for clear policies, guidelines, and frameworks to responsibly integrate ChatGPT in higher education. It also highlights the need for empirical research to understand user experiences and perceptions. The findings provide insights that can guide future research efforts in understanding the implications of ChatGPT and similar Artificial Intelligence (AI) systems in higher education. The study concludes by highlighting the importance of thing ethnography as an innovative approach for engaging with intelligent AI systems and calls for further research to explore best practices and strategies in utilizing Generative AI for educational purposes."
        },
        "https://openalex.org/W4360615722": {
            "title": "Chatbots in Education and Research: A Critical Examination of Ethical Implications and Solutions",
            "openalex_id": "https://openalex.org/W4360615722",
            "cited_by_count": 449,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4388930439",
                "https://openalex.org/W4386931570",
                "https://openalex.org/W4382808543",
                "https://openalex.org/W4300450609",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2391010541",
                "https://openalex.org/W2387276901",
                "https://openalex.org/W2385953334",
                "https://openalex.org/W2357367123",
                "https://openalex.org/W2351303360"
            ],
            "references": [
                "https://openalex.org/W1894471607",
                "https://openalex.org/W2047997444",
                "https://openalex.org/W2495735354",
                "https://openalex.org/W2790902688",
                "https://openalex.org/W2969625533",
                "https://openalex.org/W2980349812",
                "https://openalex.org/W3006273443",
                "https://openalex.org/W3017131514",
                "https://openalex.org/W3084223432",
                "https://openalex.org/W3093566773"
            ],
            "authors": [
                "Chokri Kooli"
            ],
            "venue": "Sustainability",
            "doi": "https://doi.org/10.3390/su15075614",
            "concepts": [
                "Engineering ethics",
                "Perspective (graphical)",
                "Field (mathematics)",
                "Exploratory research",
                "Interpretation (philosophy)",
                "Phenomenon",
                "Management science",
                "Knowledge management",
                "Sociology",
                "Computer science",
                "Data science",
                "Epistemology",
                "Engineering",
                "Artificial intelligence",
                "Social science",
                "Philosophy",
                "Mathematics",
                "Pure mathematics",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/2071-1050/15/7/5614/pdf?version=1679544559",
            "abstract": "A new era of education and research based on chatbots and artificial intelligence is quickly growing. However, the application of these new systems is associated with several challenges and limitations, mainly related to ethics. This paper explores the potential use of AI systems and chatbots in the academic field and their impact on research and education from an ethical perspective. Through a qualitative methodology, the researcher perform exploratory research and data collection based on expert analysis and interpretation. The researcher conducted a comprehensive review of the main potential challenges associated with the use of chatbots in education and research to identify current practices, challenges, and opportunities. This explorative work provides a foundational understanding of the studied topic. It also helps us to better understand the subjective experiences and perspectives of the observed phenomenon, and uncovers their meanings and proposes potential solutions to the observed issues. This study examines the advantages and limitations of AI systems and chatbots, as well as their role in supporting human expertise and judgment. The paper also discusses the ethical challenges related to the use of AI systems and chatbots in research, as well as the potential for misuse and exploitation. It also proposes effective solutions to the observed ethical dilemmas. The research admits that we live in a new era of AI-based education and research. The observed technological advancements will definitely shift research processes and transform educative systems, especially in term of assessments. Digital assessments are going to disappear and assessment methods need to be more creative and innovative. The paper highlights the necessity of adaptation to the new reality of AI systems and chatbots. Co-living, sustainability and continuous adaptation to the development of these systems will become a matter of emergency. Raising awareness, adopting appropriate legislations and solidifying ethical values will strengthen research and protect educational systems. The presence of AI systems and chatbots in education needs to be considered as an opportunity for development rather than a threat."
        },
        "https://openalex.org/W4386251129": {
            "title": "Transforming Education: A Comprehensive Review of Generative Artificial Intelligence in Educational Settings through Bibliometric and Content Analysis",
            "openalex_id": "https://openalex.org/W4386251129",
            "cited_by_count": 444,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4385368139",
                "https://openalex.org/W4322745238",
                "https://openalex.org/W4241245680",
                "https://openalex.org/W4237580245",
                "https://openalex.org/W3200688510",
                "https://openalex.org/W3124327509",
                "https://openalex.org/W3113185420",
                "https://openalex.org/W2906134827",
                "https://openalex.org/W2326080043",
                "https://openalex.org/W2169196470"
            ],
            "references": [
                "https://openalex.org/W2952164904",
                "https://openalex.org/W3084223432",
                "https://openalex.org/W3088980625",
                "https://openalex.org/W3134822068",
                "https://openalex.org/W3161501253",
                "https://openalex.org/W3175587171",
                "https://openalex.org/W3188288927",
                "https://openalex.org/W4211263275",
                "https://openalex.org/W4213337800",
                "https://openalex.org/W4225289236"
            ],
            "authors": [
                "Zied Bahroun",
                "Chiraz Anane",
                "Vian Ahmed",
                "Andrew Zacca"
            ],
            "venue": "Sustainability",
            "doi": "https://doi.org/10.3390/su151712983",
            "concepts": [
                "Transformative learning",
                "Curriculum",
                "Field (mathematics)",
                "Content analysis",
                "Engineering ethics",
                "Generative grammar",
                "Educational research",
                "Data science",
                "Sociology",
                "Knowledge management",
                "Computer science",
                "Engineering",
                "Pedagogy",
                "Artificial intelligence",
                "Social science",
                "Mathematics",
                "Pure mathematics"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/2071-1050/15/17/12983/pdf?version=1693276970",
            "abstract": "In the ever-evolving era of technological advancements, generative artificial intelligence (GAI) emerges as a transformative force, revolutionizing education. This review paper, guided by the PRISMA framework, presents a comprehensive analysis of GAI in education, synthesizing key insights from a selection of 207 research papers to identify research gaps and future directions in the field. This study begins with a content analysis that explores GAI\u2019s transformative impact in specific educational domains, including medical education and engineering education. The versatile applications of GAI encompass assessment, personalized learning support, and intelligent tutoring systems. Ethical considerations, interdisciplinary collaboration, and responsible technology use are highlighted, emphasizing the need for transparent GAI models and addressing biases. Subsequently, a bibliometric analysis of GAI in education is conducted, examining prominent AI tools, research focus, geographic distribution, and interdisciplinary collaboration. ChatGPT emerges as a dominant GAI tool, and the analysis reveals significant and exponential growth in GAI research in 2023. Moreover, this paper identifies promising future research directions, such as GAI-enhanced curriculum design and longitudinal studies tracking its long-term impact on learning outcomes. These findings provide a comprehensive understanding of GAI\u2019s potential in reshaping education and offer valuable insights to researchers, educators, and policymakers interested in the intersection of GAI and education."
        },
        "https://openalex.org/W4387379065": {
            "title": "The Potential of Generative Artificial Intelligence Across Disciplines: Perspectives and Future Directions",
            "openalex_id": "https://openalex.org/W4387379065",
            "cited_by_count": 431,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4380551139",
                "https://openalex.org/W4365211920",
                "https://openalex.org/W4317695495",
                "https://openalex.org/W4316116392",
                "https://openalex.org/W4283803360",
                "https://openalex.org/W4238433571",
                "https://openalex.org/W3174044702",
                "https://openalex.org/W3014948380",
                "https://openalex.org/W2967848559",
                "https://openalex.org/W2280377497"
            ],
            "references": [
                "https://openalex.org/W2772633599",
                "https://openalex.org/W2783479218",
                "https://openalex.org/W2905604475",
                "https://openalex.org/W2914431450",
                "https://openalex.org/W2945904062",
                "https://openalex.org/W2947308709",
                "https://openalex.org/W2963437685",
                "https://openalex.org/W2969625533",
                "https://openalex.org/W2982529807",
                "https://openalex.org/W2994986458"
            ],
            "authors": [
                "Keng\u2010Boon Ooi",
                "Garry Wei\u2010Han Tan",
                "Mostafa Al\u2010Emran",
                "Mohammed A. Al\u2010Sharafi",
                "Alexandru C\u0103p\u0103\u021b\u00een\u0103",
                "Amrita Chakraborty",
                "Yogesh K. Dwivedi",
                "Tzu-Ling Huang",
                "Arpan Kumar Kar",
                "Voon\u2010Hsien Lee",
                "Xiu-Ming Loh",
                "Adrian Micu",
                "Patrick Mikalef",
                "Emmanuel Mogaji",
                "Neeraj Pandey",
                "Ramakrishnan Raman",
                "Nripendra P. Rana",
                "Prianka Sarker",
                "Anshuman Sharma",
                "Ching\u2010I Teng",
                "Samuel Fosso Wamba",
                "Lai\u2010Wan Wong"
            ],
            "venue": "Journal of Computer Information Systems",
            "doi": "https://doi.org/10.1080/08874417.2023.2261010",
            "concepts": [
                "Generative grammar",
                "Variety (cybernetics)",
                "Computer science",
                "Generative model",
                "Knowledge management",
                "Discipline",
                "Artificial intelligence",
                "Data science",
                "Sociology",
                "Social science"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "ABSTRACTIn a short span of time since its introduction, generative artificial intelligence (AI) has garnered much interest at both personal and organizational levels. This is because of its potential to cause drastic and widespread shifts in many aspects of life that are comparable to those of the Internet and smartphones. More specifically, generative AI utilizes machine learning, neural networks, and other techniques to generate new content (e.g. text, images, music) by analyzing patterns and information from the training data. This has enabled generative AI to have a wide range of applications, from creating personalized content to improving business operations. Despite its many benefits, there are also significant concerns about the negative implications of generative AI. In view of this, the current article brings together experts in a variety of fields to expound and provide multi-disciplinary insights on the opportunities, challenges, and research agendas of generative AI in specific industries (i.e. marketing, healthcare, human resource, education, banking, retailing, the workplace, manufacturing, and sustainable IT management).KEYWORDS: Generative artificial intelligencemachine learninglarge language modelChatGPTBard Disclosure statementNo potential conflict of interest was reported by the author(s)."
        },
        "https://openalex.org/W4386417345": {
            "title": "The impact of Generative AI (GenAI) on practices, policies and research direction in education: a case of ChatGPT and Midjourney",
            "openalex_id": "https://openalex.org/W4386417345",
            "cited_by_count": 402,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W99102149",
                "https://openalex.org/W3170131249",
                "https://openalex.org/W3166510378",
                "https://openalex.org/W3147440745",
                "https://openalex.org/W3094365592",
                "https://openalex.org/W2997034443",
                "https://openalex.org/W2896391874",
                "https://openalex.org/W2259665829",
                "https://openalex.org/W1994256903",
                "https://openalex.org/W1774155268"
            ],
            "references": [
                "https://openalex.org/W1797231943",
                "https://openalex.org/W1820505434",
                "https://openalex.org/W1979290264",
                "https://openalex.org/W2001829221",
                "https://openalex.org/W2063626674",
                "https://openalex.org/W2069897807",
                "https://openalex.org/W2082267121",
                "https://openalex.org/W2114903686",
                "https://openalex.org/W2122513519",
                "https://openalex.org/W2137655683"
            ],
            "authors": [
                "Thomas K. F. Chiu"
            ],
            "venue": "Interactive Learning Environments",
            "doi": "https://doi.org/10.1080/10494820.2023.2253861",
            "concepts": [
                "Thematic analysis",
                "Focus group",
                "Pedagogy",
                "Psychology",
                "Qualitative research",
                "Mathematics education",
                "Sociology",
                "Social science",
                "Anthropology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.tandfonline.com/doi/pdf/10.1080/10494820.2023.2253861?needAccess=true&role=button",
            "abstract": "Generative artificial intelligence (GenAI) tools have become increasingly accessible and have impacted school education in numerous ways. However, most of the discussions occur in higher education. In schools, teachers\u2019 perspectives are crucial for making sense of innovative technologies. Accordingly, this qualitative study aims to investigate how GenAI changes our school education from the perspectives of teachers and leaders. It used four domains \u2013 learning, teaching, assessment, and administration \u2013 as the initial framework suggested in a systematic literature review study on AI in education. The participants were 88 school teachers and leaders of different backgrounds. They completed a survey and joined a focus group to share how ChatGPT and Midjounery had a GenAI effect on school education. Thematic analysis identified four main themes and 12 subthemes. The findings provide three suggestions for practices: know-it-all attitude, new prerequisite knowledge, interdisciplinary teaching, and three implications for policy: new assessment, AI education, and professional standards. They also further suggest six future research directions for GenAI in education."
        },
        "https://openalex.org/W4378473491": {
            "title": "Unlocking the opportunities through ChatGPT Tool towards ameliorating the education system",
            "openalex_id": "https://openalex.org/W4378473491",
            "cited_by_count": 387,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4391375266",
                "https://openalex.org/W4231340554",
                "https://openalex.org/W3213789065",
                "https://openalex.org/W3168296622",
                "https://openalex.org/W3080576469",
                "https://openalex.org/W2931838652",
                "https://openalex.org/W2765153054",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2596173151",
                "https://openalex.org/W1491839574"
            ],
            "references": [
                "https://openalex.org/W4311430511",
                "https://openalex.org/W4313564799",
                "https://openalex.org/W4313678819",
                "https://openalex.org/W4317910584",
                "https://openalex.org/W4318014888",
                "https://openalex.org/W4318240052",
                "https://openalex.org/W4318474896",
                "https://openalex.org/W4318612697",
                "https://openalex.org/W4318618627",
                "https://openalex.org/W4318925155"
            ],
            "authors": [
                "Mohd Javaid",
                "Abid Haleem",
                "Ravi Pratap Singh",
                "Shahbaz Khan",
                "Ibrahim Haleem Khan"
            ],
            "venue": "BenchCouncil Transactions on Benchmarks Standards and Evaluations",
            "doi": "https://doi.org/10.1016/j.tbench.2023.100115",
            "concepts": [
                "Engineering management",
                "Business",
                "Process management",
                "Engineering"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.tbench.2023.100115",
            "abstract": "Artificial Intelligence (AI)-based ChatGPT developed by OpenAI is now widely accepted in several fields, including education. Students can learn about ideas and theories by using this technology while generating content with it. ChatGPT is built on State of the Art (SOA), like Deep Learning (DL), Natural Language Processing (NLP), and Machine Learning (ML), an extrapolation of a class of ML-NLP models known as Large Language Model (LLMs). It may be used to automate test and assignment grading, giving instructors more time to concentrate on instruction. This technology can be utilised to customise learning for kids, enabling them to focus more intently on the subject matter and critical thinking ChatGPT is an excellent tool for language lessons since it can translate text from one language to another. It may provide lists of vocabulary terms and meanings, assisting students in developing their language proficiency with resources. Personalised learning opportunities are one of ChatGPT's significant applications in the classroom. This might include creating educational resources and content tailored to a student's unique interests, skills, and learning goals. This paper discusses the need for ChatGPT and the significant features of ChatGPT in the education system. Further, it identifies and discusses the significant applications of ChatGPT in education. Using ChatGPT, educators may design lessons and instructional materials specific to each student's requirements and skills based on current trends. Students may work at their speed and concentrate on the areas where they need the most support, resulting in a more effective and efficient learning environment. Both instructors and students may profit significantly from using ChatGPT in the classroom. Instructors may save time on numerous duties by using this technology. In future, ChatGPT will become a powerful tool for enhancing students' and teachers' experience."
        },
        "https://openalex.org/W4376139682": {
            "title": "The role of ChatGPT in higher education: Benefits, challenges, and future research directions",
            "openalex_id": "https://openalex.org/W4376139682",
            "cited_by_count": 365,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4250032403",
                "https://openalex.org/W3039266826",
                "https://openalex.org/W2954091239",
                "https://openalex.org/W2902169127",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2734499810",
                "https://openalex.org/W2605852498",
                "https://openalex.org/W2508845121",
                "https://openalex.org/W2095322754"
            ],
            "references": [
                "https://openalex.org/W1484391064",
                "https://openalex.org/W1505263122",
                "https://openalex.org/W1507769851",
                "https://openalex.org/W1531273296",
                "https://openalex.org/W1568871075",
                "https://openalex.org/W162353500",
                "https://openalex.org/W1840336382",
                "https://openalex.org/W19453093",
                "https://openalex.org/W1969735952",
                "https://openalex.org/W1974141383"
            ],
            "authors": [
                "Tareq Rasul",
                "Sumesh Nair",
                "Diane Robyn Kalendra",
                "Mulyadi Robin",
                "Fernando de Oliveira Santini",
                "Wagner J\u00fanior Ladeira",
                "Mingwei Sun",
                "Ingrid Day",
                "Raouf Ahmad Rather",
                "Liz Heathcote"
            ],
            "venue": "Journal of Applied Learning & Teaching",
            "doi": "https://doi.org/10.37074/jalt.2023.6.1.29",
            "concepts": [
                "Perspective (graphical)",
                "Higher education",
                "Engineering ethics",
                "Reliability (semiconductor)",
                "Computer science",
                "Knowledge management",
                "Psychology",
                "Political science",
                "Engineering",
                "Artificial intelligence",
                "Power (physics)",
                "Physics",
                "Quantum mechanics",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://journals.sfu.ca/jalt/index.php/jalt/article/download/787/583",
            "abstract": "This paper examines the potential benefits and challenges of using the generative AI model, ChatGPT, in higher education, in the backdrop of the constructivist theory of learning. This perspective-type study presents five benefits of ChatGPT: the potential to facilitate adaptive learning, provide personalised feedback, support research and data analysis, offer automated administrative services, and aid in developing innovative assessments. Additionally, the paper identifies five challenges: academic integrity concerns, reliability issues, inability to evaluate and reinforce graduate skill sets, limitations in assessing learning outcomes, and potential biases and falsified information in information processing. The paper argues that tertiary educators and students must exercise caution when using ChatGPT for academic purposes to ensure its ethical, reliable, and effective use. To achieve this, the paper proposes various propositions, such as prioritising education on the responsible and ethical use of ChatGPT, devising new assessment strategies, addressing bias and falsified information, and including AI literacy as part of graduate skills. By balancing the potential benefits and challenges, ChatGPT can enhance students\u2019 learning experiences in higher education."
        },
        "https://openalex.org/W4385416665": {
            "title": "The Power of Generative AI: A Review of Requirements, Models, Input\u2013Output Formats, Evaluation Metrics, and Challenges",
            "openalex_id": "https://openalex.org/W4385416665",
            "cited_by_count": 351,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W775311126",
                "https://openalex.org/W4391334978",
                "https://openalex.org/W4388137171",
                "https://openalex.org/W4380551139",
                "https://openalex.org/W4365211920",
                "https://openalex.org/W4317695495",
                "https://openalex.org/W4301024388",
                "https://openalex.org/W4300030714",
                "https://openalex.org/W1967909251",
                "https://openalex.org/W1517876498"
            ],
            "references": [
                "https://openalex.org/W1861492603",
                "https://openalex.org/W1895577753",
                "https://openalex.org/W2185175083",
                "https://openalex.org/W2339754110",
                "https://openalex.org/W2587706859",
                "https://openalex.org/W2592232824",
                "https://openalex.org/W2605045867",
                "https://openalex.org/W2746457594",
                "https://openalex.org/W2755577605",
                "https://openalex.org/W2796929742"
            ],
            "authors": [
                "Ajay Bandi",
                "Pydi Venkata Satya Ramesh Adapa",
                "Yudu Eswar Vinay Pratap Kumar Kuchi"
            ],
            "venue": "Future Internet",
            "doi": "https://doi.org/10.3390/fi15080260",
            "concepts": [
                "Computer science",
                "Generative grammar",
                "Field (mathematics)",
                "Taxonomy (biology)",
                "Artificial intelligence",
                "Generative Design",
                "Software",
                "Transformer",
                "Machine learning",
                "Generative model",
                "Software engineering",
                "Programming language",
                "Metric (unit)",
                "Operations management",
                "Botany",
                "Physics",
                "Mathematics",
                "Quantum mechanics",
                "Voltage",
                "Pure mathematics",
                "Economics",
                "Biology"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/1999-5903/15/8/260/pdf?version=1690812126",
            "abstract": "Generative artificial intelligence (AI) has emerged as a powerful technology with numerous applications in various domains. There is a need to identify the requirements and evaluation metrics for generative AI models designed for specific tasks. The purpose of the research aims to investigate the fundamental aspects of generative AI systems, including their requirements, models, input\u2013output formats, and evaluation metrics. The study addresses key research questions and presents comprehensive insights to guide researchers, developers, and practitioners in the field. Firstly, the requirements necessary for implementing generative AI systems are examined and categorized into three distinct categories: hardware, software, and user experience. Furthermore, the study explores the different types of generative AI models described in the literature by presenting a taxonomy based on architectural characteristics, such as variational autoencoders (VAEs), generative adversarial networks (GANs), diffusion models, transformers, language models, normalizing flow models, and hybrid models. A comprehensive classification of input and output formats used in generative AI systems is also provided. Moreover, the research proposes a classification system based on output types and discusses commonly used evaluation metrics in generative AI. The findings contribute to advancements in the field, enabling researchers, developers, and practitioners to effectively implement and evaluate generative AI models for various applications. The significance of the research lies in understanding that generative AI system requirements are crucial for effective planning, design, and optimal performance. A taxonomy of models aids in selecting suitable options and driving advancements. Classifying input\u2013output formats enables leveraging diverse formats for customized systems, while evaluation metrics establish standardized methods to assess model quality and performance."
        },
        "https://openalex.org/W4379416244": {
            "title": "Leveraging ChatGPT and other generative artificial intelligence (AI)-based applications in the hospitality and tourism industry: practices, challenges and research agenda",
            "openalex_id": "https://openalex.org/W4379416244",
            "cited_by_count": 318,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W40154911",
                "https://openalex.org/W3195436559",
                "https://openalex.org/W3171522609",
                "https://openalex.org/W3167248615",
                "https://openalex.org/W250174581",
                "https://openalex.org/W2373766366",
                "https://openalex.org/W2095919374",
                "https://openalex.org/W2057290649",
                "https://openalex.org/W2029400148",
                "https://openalex.org/W1983398261"
            ],
            "references": [
                "https://openalex.org/W1963725693",
                "https://openalex.org/W2007504794",
                "https://openalex.org/W2042230785",
                "https://openalex.org/W2064427787",
                "https://openalex.org/W2142221817",
                "https://openalex.org/W2253407806",
                "https://openalex.org/W2809971311",
                "https://openalex.org/W2981380027",
                "https://openalex.org/W3155545287",
                "https://openalex.org/W3194522055"
            ],
            "authors": [
                "Yogesh K. Dwivedi",
                "Neeraj Pandey",
                "Wendy L. Currie",
                "Adrian Micu"
            ],
            "venue": "International Journal of Contemporary Hospitality Management",
            "doi": "https://doi.org/10.1108/ijchm-05-2023-0686",
            "concepts": [
                "Hospitality",
                "Tourism",
                "Hospitality industry",
                "Marketing",
                "Generative grammar",
                "Originality",
                "Business",
                "Knowledge management",
                "Transformational leadership",
                "Hospitality management studies",
                "Sociology",
                "Public relations",
                "Qualitative research",
                "Computer science",
                "Artificial intelligence",
                "Political science",
                "Social science",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://cronfa.swan.ac.uk/Record/cronfa63514/Download/63514__27547__6fa77a87e5c04123ac7c751d44f6dec1.pdf",
            "abstract": "Purpose The hospitality and tourism sector has witnessed phenomenal growth in customer numbers during the postpandemic times. This growth has been accompanied by the use of technologies in customer interface and backend activities, including the adoption of self-serving technologies. This study aims to analyze the existing practices and challenges and establish a research agenda for the implementation of generative artificial intelligence (AI) (such as ChatGPT) and similar tools in the hospitality and tourism industry. Design/methodology/approach This study analyzes the existing literature and practices. This study draws upon these practices to outline a novel research agenda for scholars and practitioners working in this domain. Findings The integration of generative AI technologies, such as ChatGPT, will have a transformational impact on the hospitality and tourism industry. This study highlights the potential challenges of implementing such technologies from the perspectives of companies, customers and regulators. Research limitations/implications This study serves as a reference material for those who are planning to use generative AI tools like ChatGPT in their hospitality and tourism businesses. This study also highlights potential pitfalls that ChatGPT-enabled systems may encounter during service delivery processes. Originality/value This study is a pioneering work that assesses the applications of ChatGPT in the hospitality and tourism industry. This study highlights the potential and challenges in implementing ChatGPT within the hospitality and tourism industry."
        },
        "https://openalex.org/W4386580390": {
            "title": "The ethics of ChatGPT \u2013 Exploring the ethical issues of an emerging technology",
            "openalex_id": "https://openalex.org/W4386580390",
            "cited_by_count": 304,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W74688916",
                "https://openalex.org/W4229781014",
                "https://openalex.org/W3152561232",
                "https://openalex.org/W3008947587",
                "https://openalex.org/W2911852084",
                "https://openalex.org/W2386390545",
                "https://openalex.org/W2357986026",
                "https://openalex.org/W2351051585",
                "https://openalex.org/W1987721955",
                "https://openalex.org/W153116910"
            ],
            "references": [
                "https://openalex.org/W1010192393",
                "https://openalex.org/W1532794173",
                "https://openalex.org/W1599725196",
                "https://openalex.org/W1970923285",
                "https://openalex.org/W2023102928",
                "https://openalex.org/W2025404306",
                "https://openalex.org/W2037181080",
                "https://openalex.org/W2046413193",
                "https://openalex.org/W2049667815",
                "https://openalex.org/W2065588045"
            ],
            "authors": [
                "Bernd Carsten Stahl",
                "Damian Eke"
            ],
            "venue": "International Journal of Information Management",
            "doi": "https://doi.org/10.1016/j.ijinfomgt.2023.102700",
            "concepts": [
                "Engineering ethics",
                "Ethics of technology",
                "Ethical issues",
                "Sociology",
                "Information ethics",
                "Political science",
                "Environmental ethics",
                "Engineering",
                "Philosophy",
                "Meta-ethics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.ijinfomgt.2023.102700",
            "abstract": "This article explores ethical issues raised by generative conversational AI systems like ChatGPT. It applies established approaches for analysing ethics of emerging technologies to undertake a systematic review of possible benefits and concerns. The methodology combines ethical issues identified by Anticipatory Technology Ethics, Ethical Impact Assessment, and Ethical Issues of Emerging ICT Applications with AI-specific issues from the literature. These are applied to analyse ChatGPT's capabilities to produce humanlike text and interact seamlessly. The analysis finds ChatGPT could provide high-level societal and ethical benefits. However, it also raises significant ethical concerns across social justice, individual autonomy, cultural identity, and environmental issues. Key high-impact concerns include responsibility, inclusion, social cohesion, autonomy, safety, bias, accountability, and environmental impacts. While the current discourse focuses narrowly on specific issues such as authorship, this analysis systematically uncovers a broader, more balanced range of ethical issues worthy of attention. Findings are consistent with emerging research and industry priorities on ethics of generative AI. Implications include the need for diverse stakeholder engagement, considering benefits and risks holistically when developing applications, and multi-level policy interventions to promote positive outcomes. Overall, the analysis demonstrates that applying established ethics of technology methodologies can produce a rigorous, comprehensive foundation to guide discourse and action around impactful emerging technologies like ChatGPT. The paper advocates sustaining this broad, balanced ethics perspective as use cases unfold to realize benefits while addressing ethical downsides."
        },
        "https://openalex.org/W4384934594": {
            "title": "The impact of AI writing tools on the content and organization of students\u2019 writing: EFL teachers\u2019 perspective",
            "openalex_id": "https://openalex.org/W4384934594",
            "cited_by_count": 282,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4327510170",
                "https://openalex.org/W4241027915",
                "https://openalex.org/W2618086975",
                "https://openalex.org/W2565485611",
                "https://openalex.org/W2360356595",
                "https://openalex.org/W2292778187",
                "https://openalex.org/W2151083776",
                "https://openalex.org/W2071712343",
                "https://openalex.org/W2005366727",
                "https://openalex.org/W1525621371"
            ],
            "references": [
                "https://openalex.org/W1979290264",
                "https://openalex.org/W2006479099",
                "https://openalex.org/W2906463257",
                "https://openalex.org/W2912127582",
                "https://openalex.org/W2942832874",
                "https://openalex.org/W2981194182",
                "https://openalex.org/W3048924892",
                "https://openalex.org/W3089424874",
                "https://openalex.org/W3093253033",
                "https://openalex.org/W3159593734"
            ],
            "authors": [
                "Marzuki Marzuki",
                "Utami Widiati",
                "Diyenti Rusdin",
                "Darwin Darwin",
                "Inda Indrawati"
            ],
            "venue": "Cogent Education",
            "doi": "https://doi.org/10.1080/2331186x.2023.2236469",
            "concepts": [
                "Perspective (graphical)",
                "Quality (philosophy)",
                "Psychology",
                "Mathematics education",
                "Professional writing",
                "Variety (cybernetics)",
                "Second language writing",
                "Qualitative research",
                "Diversity (politics)",
                "Pedagogy",
                "Computer science",
                "Linguistics",
                "Sociology",
                "Second language",
                "Artificial intelligence",
                "Social science",
                "Philosophy",
                "Epistemology",
                "Anthropology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.tandfonline.com/doi/pdf/10.1080/2331186X.2023.2236469?needAccess=true&role=button",
            "abstract": "The primary objective of this study was to examine the range of available Artificial Intelligence (AI) writing tools and assess their influence on student writing, particularly in terms of content and organization, as perceived by English as a Foreign Language (EFL) teachers. Utilizing a qualitative approach, the research was constructed within a case study design. The data was collected via semi-structured interviews, targeting information about the diversity of AI writing tools and their impact on students\" writing quality. The study gathered data from four EFL teachers across three distinct universities in Indonesia, shedding light on the variety of AI writing tools used in their classrooms. These included applications like Quillbot, WordTune, Jenni, Chat-GPT, Paperpal, Copy.ai, and Essay Writer. Furthermore, these teachers unanimously agreed that the AI writing tools positively improved their students' writing quality, particularly enhancing the quality of their content and organization. The findings of this study imply that integrating AI writing tools can prove beneficial in elevating the quality of EFL student writing. In response to this study's limitations, recommendations for future research were also addressed."
        },
        "https://openalex.org/W4391855109": {
            "title": "A Review on Large Language Models: Architectures, Applications, Taxonomies, Open Issues and Challenges",
            "openalex_id": "https://openalex.org/W4391855109",
            "cited_by_count": 272,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W4391375266",
                "https://openalex.org/W4313893854",
                "https://openalex.org/W4313246833",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2390279801",
                "https://openalex.org/W2290480557",
                "https://openalex.org/W2242910500",
                "https://openalex.org/W2135054431",
                "https://openalex.org/W2081307663"
            ],
            "references": [
                "https://openalex.org/W179875071",
                "https://openalex.org/W187290754",
                "https://openalex.org/W2001771035",
                "https://openalex.org/W2005874308",
                "https://openalex.org/W2037450062",
                "https://openalex.org/W2070398553",
                "https://openalex.org/W2075354211",
                "https://openalex.org/W2121879602",
                "https://openalex.org/W2137983211",
                "https://openalex.org/W2165545766"
            ],
            "authors": [
                "Mohaimenul Azam Khan Raiaan",
                "Md. Saddam Hossain Mukta",
                "Kaniz Fatema",
                "Nur Mohammad Fahad",
                "Sadman Sakib",
                "Most. Marufatul Jannat Mim",
                "Jubaer Ahmad",
                "Mohammed Eunus Ali",
                "Sami Azam"
            ],
            "venue": "IEEE Access",
            "doi": "https://doi.org/10.1109/access.2024.3365742",
            "concepts": [
                "Computer science",
                "Open research",
                "Data science",
                "Natural language processing",
                "World Wide Web"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10433480.pdf",
            "abstract": "Large Language Models (LLMs) recently demonstrated extraordinary capability, including natural language processing (NLP), language translation, text generation, question answering, etc. Moreover, LLMs are a new and essential part of computerized language processing, having the ability to understand complex verbal patterns and generate coherent and appropriate replies for the situation. Though this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it difficult to understand the overall impact of these improvements. Since a lot of new research on LLMs is coming out quickly, it is getting tough to get an overview of all of them in a short note. Consequently, the research community would benefit from a short but thorough review of the recent changes in this area. This article thoroughly overviews LLMs, including their history, architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training phase. It then provides an overview of the existing works, the history of LLMs, their evolution over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different training methods that have been used to train them. It also demonstrated the datasets utilized in the studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and healthcare, education, social, business, and agriculture. It also illustrates how LLMs create an impact on society and shape the future of AI and how they can be used to solve real-world problems. Then it also explores open issues and challenges to deploying LLMs in real-world scenario. Our review paper aims to help practitioners, researchers, and experts thoroughly understand the evolution of LLMs, pre-trained architectures, applications, challenges, and future goals."
        },
        "https://openalex.org/W4386544117": {
            "title": "The impact of ChatGPT on higher education",
            "openalex_id": "https://openalex.org/W4386544117",
            "cited_by_count": 272,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4381331847",
                "https://openalex.org/W4254361043",
                "https://openalex.org/W3204793433",
                "https://openalex.org/W2978601735",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2598535773",
                "https://openalex.org/W2127840439",
                "https://openalex.org/W2116355764",
                "https://openalex.org/W2090497605"
            ],
            "references": [
                "https://openalex.org/W1418763296",
                "https://openalex.org/W1980446154",
                "https://openalex.org/W2105321265",
                "https://openalex.org/W2121902912",
                "https://openalex.org/W2138967573",
                "https://openalex.org/W2188972293",
                "https://openalex.org/W2212962555",
                "https://openalex.org/W2595273079",
                "https://openalex.org/W2611072219",
                "https://openalex.org/W2617381106"
            ],
            "authors": [
                "Juan Dempere",
                "Kennedy Prince Modugu",
                "Allam Hesham",
                "Lakshmana Kumar Ramasamy"
            ],
            "venue": "Frontiers in Education",
            "doi": "https://doi.org/10.3389/feduc.2023.1206936",
            "concepts": [
                "Higher education",
                "Chatbot",
                "Thematic analysis",
                "Transformative learning",
                "Computer science",
                "Psychology",
                "Political science",
                "World Wide Web",
                "Sociology",
                "Pedagogy",
                "Qualitative research",
                "Social science",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.frontiersin.org/articles/10.3389/feduc.2023.1206936/pdf?isPublishedV2=False",
            "abstract": "Introduction This study explores the effects of Artificial Intelligence (AI) chatbots, with a particular focus on OpenAI\u2019s ChatGPT, on Higher Education Institutions (HEIs). With the rapid advancement of AI, understanding its implications in the educational sector becomes paramount. Methods Utilizing databases like PubMed, IEEE Xplore, and Google Scholar, we systematically searched for literature on AI chatbots\u2019 impact on HEIs. Our criteria prioritized peer-reviewed articles, prominent media outlets, and English publications, excluding tangential AI chatbot mentions. After selection, data extraction focused on authors, study design, and primary findings. The analysis combined descriptive and thematic approaches, emphasizing patterns and applications of AI chatbots in HEIs. Results The literature review revealed diverse perspectives on ChatGPT\u2019s potential in education. Notable benefits include research support, automated grading, and enhanced human-computer interaction. However, concerns such as online testing security, plagiarism, and broader societal and economic impacts like job displacement, the digital literacy gap, and AI-induced anxiety were identified. The study also underscored the transformative architecture of ChatGPT and its versatile applications in the educational sector. Furthermore, potential advantages like streamlined enrollment, improved student services, teaching enhancements, research aid, and increased student retention were highlighted. Conversely, risks such as privacy breaches, misuse, bias, misinformation, decreased human interaction, and accessibility issues were identified. Discussion While AI\u2019s global expansion is undeniable, there is a pressing need for balanced regulation in its application within HEIs. Faculty members are encouraged to utilize AI tools like ChatGPT proactively and ethically to mitigate risks, especially academic fraud. Despite the study\u2019s limitations, including an incomplete representation of AI\u2019s overall effect on education and the absence of concrete integration guidelines, it is evident that AI technologies like ChatGPT present both significant benefits and risks. The study advocates for a thoughtful and responsible integration of such technologies within HEIs."
        },
        "https://openalex.org/W4376255328": {
            "title": "ChatGPT and the hospitality and tourism industry: an overview of current trends and future research directions",
            "openalex_id": "https://openalex.org/W4376255328",
            "cited_by_count": 260,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4301934261",
                "https://openalex.org/W40154911",
                "https://openalex.org/W3195436559",
                "https://openalex.org/W3171522609",
                "https://openalex.org/W3167248615",
                "https://openalex.org/W2951269499",
                "https://openalex.org/W250174581",
                "https://openalex.org/W2373766366",
                "https://openalex.org/W2057290649",
                "https://openalex.org/W2029400148"
            ],
            "references": [
                "https://openalex.org/W2487188274",
                "https://openalex.org/W2602140409",
                "https://openalex.org/W2766563046",
                "https://openalex.org/W2923469844",
                "https://openalex.org/W2975322939",
                "https://openalex.org/W2995563835",
                "https://openalex.org/W3006615433",
                "https://openalex.org/W3119589883",
                "https://openalex.org/W3207025357",
                "https://openalex.org/W3214445496"
            ],
            "authors": [
                "Do\u011fan G\u00fcrsoy",
                "Yu Li",
                "Hak Jun Song"
            ],
            "venue": "Journal of Hospitality Marketing & Management",
            "doi": "https://doi.org/10.1080/19368623.2023.2211993",
            "concepts": [
                "Tourism",
                "Hospitality",
                "Hospitality industry",
                "Marketing",
                "Popularity",
                "Business",
                "Knowledge management",
                "Computer science",
                "Political science",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "Since its launch, ChatGPT, an artificial intelligence chatbot developed by Open AI based on the premises of generative pre-trained transformer autoregressive language models, has gained widespread popularity and is making significant impact on society with its unique features, such as natural language processing and contextual awareness. ChatGPT is viewed as a major disruptive innovation that is likely to revolutionize the operations in many industries including the hospitality and tourism industry. The adoption of ChatGPT will result in substantial changes throughout the hospitality and tourism industry by disrupting how customer search for information, make decisions, and how businesses produce, create, and deliver customized services and experiences. This conceptual paper provides a comprehensive discussion on generative pre-trained transformers' (GPTs) benefits, and potential challenges and threats they pose to the hospitality and tourism industry. The feasibility of integrating GPT into different travel stages and decision-making processes is also discussed. The article concludes by proposing a potential future research agenda on using GPT in creating and delivering hospitality and tourism experiences, which can guide further advancements in the field."
        },
        "https://openalex.org/W4394828356": {
            "title": "GPT (Generative Pre-Trained Transformer)\u2014 A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
            "openalex_id": "https://openalex.org/W4394828356",
            "cited_by_count": 259,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W4288263119",
                "https://openalex.org/W4287631637",
                "https://openalex.org/W4286930972",
                "https://openalex.org/W4285240985",
                "https://openalex.org/W3202115945",
                "https://openalex.org/W3099576124",
                "https://openalex.org/W3094085917",
                "https://openalex.org/W3015724364",
                "https://openalex.org/W2967994095",
                "https://openalex.org/W2900126711"
            ],
            "references": [
                "https://openalex.org/W1598517837",
                "https://openalex.org/W1967537161",
                "https://openalex.org/W2050556626",
                "https://openalex.org/W2053973149",
                "https://openalex.org/W2213443318",
                "https://openalex.org/W2473418344",
                "https://openalex.org/W2578240541",
                "https://openalex.org/W2611369375",
                "https://openalex.org/W2783378158",
                "https://openalex.org/W2790808809"
            ],
            "authors": [
                "Gokul Yenduri",
                "M. Ramalingam",
                "G. Chemmalar Selvi",
                "Y. Supriya",
                "Gautam Srivastava",
                "Praveen Kumar Reddy Maddikunta",
                "G. Deepti Raj",
                "Rutvij H. Jhaveri",
                "B. Prabadevi",
                "Weizheng Wang",
                "Athanasios V. Vasilakos",
                "Thippa Reddy Gadekallu"
            ],
            "venue": "IEEE Access",
            "doi": "https://doi.org/10.1109/access.2024.3389497",
            "concepts": [
                "Computer science",
                "Transformer",
                "Architecture",
                "Natural language understanding",
                "Emerging technologies",
                "Natural language",
                "Generative grammar",
                "Artificial intelligence",
                "Risk analysis (engineering)",
                "Engineering",
                "Electrical engineering",
                "Medicine",
                "Art",
                "Voltage",
                "Visual arts"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10500411.pdf",
            "abstract": "The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, its enabling technologies, their impact on various applications, emerging challenges, and potential solutions."
        },
        "https://openalex.org/W4386072877": {
            "title": "Analyzing the role of ChatGPT as a writing assistant at higher education level: A systematic review of the literature",
            "openalex_id": "https://openalex.org/W4386072877",
            "cited_by_count": 246,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4386072877",
                "https://openalex.org/W4252930150",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2515724224",
                "https://openalex.org/W2490542553",
                "https://openalex.org/W2481970174",
                "https://openalex.org/W2366718778",
                "https://openalex.org/W2085908183",
                "https://openalex.org/W2051263629"
            ],
            "references": [
                "https://openalex.org/W2316441559",
                "https://openalex.org/W4210503805",
                "https://openalex.org/W4283804100",
                "https://openalex.org/W4310917376",
                "https://openalex.org/W4312083290",
                "https://openalex.org/W4312338678",
                "https://openalex.org/W4313422136",
                "https://openalex.org/W4313453502",
                "https://openalex.org/W4315498228",
                "https://openalex.org/W4318263917"
            ],
            "authors": [
                "Muhammad Imran",
                "Norah Almusharraf"
            ],
            "venue": "Contemporary Educational Technology",
            "doi": "https://doi.org/10.30935/cedtech/13605",
            "concepts": [
                "Facilitator",
                "Systematic review",
                "Originality",
                "Process (computing)",
                "Academic writing",
                "Writing process",
                "Psychology",
                "Academic integrity",
                "Higher education",
                "Computer science",
                "Medical education",
                "Engineering ethics",
                "Pedagogy",
                "Mathematics education",
                "Engineering",
                "Library science",
                "Medicine",
                "MEDLINE",
                "Political science",
                "Social psychology",
                "Creativity",
                "Law",
                "Operating system"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.cedtech.net/download/analyzing-the-role-of-chatgpt-as-a-writing-assistant-at-higher-education-level-a-systematic-review-13605.pdf",
            "abstract": "This study examines the role of ChatGPT as a writing assistant in academia through a systematic literature review of the 30 most relevant articles. Since its release in November 2022, ChatGPT has become the most debated topic among scholars and is also being used by many users from different fields. Many articles, reviews, blogs, and opinion essays have been published in which the potential role of ChatGPT as a writing assistant is discussed. For this systematic review, 550 articles published six months after ChatGPT\u2019s release (December 2022 to May 2023) were collected based on specific keywords, and the final 30 most relevant articles were finalized through PRISMA flowchart. The analyzed literature identifies different opinions and scenarios associated with using ChatGPT as a writing assistant and how to interact with it. Findings show that artificial intelligence (AI) in education is a part of the ongoing development process, and its latest chatbot, ChatGPT is a part of it. Therefore, the education process, particularly academic writing, has both opportunities and challenges in adopting ChatGPT as a writing assistant. The need is to understand its role as an aid and facilitator for both the learners and instructors, as chatbots are relatively beneficial devices to facilitate, create ease and support the academic process. However, academia should revisit and update students\u2019 and teachers\u2019 training, policies, and assessment ways in writing courses for academic integrity and originality, like plagiarism issues, AI-generated assignments, online/home-based exams, and auto-correction challenges."
        },
        "https://openalex.org/W4387163553": {
            "title": "Artificial intelligence in logistics and supply chain management: A primer and roadmap for research",
            "openalex_id": "https://openalex.org/W4387163553",
            "cited_by_count": 244,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W388184414",
                "https://openalex.org/W3093134843",
                "https://openalex.org/W3024999678",
                "https://openalex.org/W2983500849",
                "https://openalex.org/W2772323916",
                "https://openalex.org/W2374116601",
                "https://openalex.org/W2026811664",
                "https://openalex.org/W1968552888",
                "https://openalex.org/W1527532029",
                "https://openalex.org/W1511346092"
            ],
            "references": [
                "https://openalex.org/W2934302500",
                "https://openalex.org/W2963849010",
                "https://openalex.org/W2969625533",
                "https://openalex.org/W3081258743",
                "https://openalex.org/W3090893589",
                "https://openalex.org/W3185736845",
                "https://openalex.org/W3202077368",
                "https://openalex.org/W3204486714",
                "https://openalex.org/W4206249537",
                "https://openalex.org/W4210289118"
            ],
            "authors": [
                "R. Glenn Richey",
                "Soumyadeb Chowdhury",
                "Beth Davis\u2010Sramek",
                "Mihalis Giannakis",
                "Yogesh K. Dwivedi"
            ],
            "venue": "Journal of Business Logistics",
            "doi": "https://doi.org/10.1111/jbl.12364",
            "concepts": [
                "Supply chain management",
                "Supply chain",
                "Conversation",
                "Domain (mathematical analysis)",
                "Computer science",
                "Knowledge management",
                "Business",
                "Sociology",
                "Marketing",
                "Mathematical analysis",
                "Mathematics",
                "Communication"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1111/jbl.12364",
            "abstract": "Abstract The dawn of generative artificial intelligence (AI) has the potential to transform logistics and supply chain management radically. However, this promising innovation is met with a scholarly discourse grappling with an interplay between the promising capabilities and potential drawbacks. This conversation frequently includes dystopian forecasts of mass unemployment and detrimental repercussions concerning academic research integrity. Despite the current hype, existing research exploring the intersection between AI and the logistics and supply chain management (L&amp;SCM) sector remains limited. Therefore, this editorial seeks to fill this void, synthesizing the potential applications of AI within the L&amp;SCM domain alongside an analysis of the implementation challenges. In doing so, we propose a robust research framework as a primer and roadmap for future research. This will give researchers and organizations comprehensive insights and strategies to navigate the complex yet promising landscape of AI integration within the L&amp;SCM domain."
        },
        "https://openalex.org/W4387745359": {
            "title": "Generative artificial intelligence in marketing: Applications, opportunities, challenges, and research agenda",
            "openalex_id": "https://openalex.org/W4387745359",
            "cited_by_count": 241,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4388185474",
                "https://openalex.org/W4367154464",
                "https://openalex.org/W4242311056",
                "https://openalex.org/W372070118",
                "https://openalex.org/W3139064098",
                "https://openalex.org/W3138475190",
                "https://openalex.org/W3135048975",
                "https://openalex.org/W2607446648",
                "https://openalex.org/W1550694910",
                "https://openalex.org/W1499615636"
            ],
            "references": [
                "https://openalex.org/W1114177902",
                "https://openalex.org/W1583356901",
                "https://openalex.org/W1791587663",
                "https://openalex.org/W1966906413",
                "https://openalex.org/W1971987011",
                "https://openalex.org/W1973904190",
                "https://openalex.org/W2019591674",
                "https://openalex.org/W2022148052",
                "https://openalex.org/W2041527120",
                "https://openalex.org/W2059486617"
            ],
            "authors": [
                "Nir Kshetri",
                "Yogesh K. Dwivedi",
                "Thomas H. Davenport",
                "Niki Panteli"
            ],
            "venue": "International Journal of Information Management",
            "doi": "https://doi.org/10.1016/j.ijinfomgt.2023.102716",
            "concepts": [
                "Transformative learning",
                "Generative grammar",
                "Marketing",
                "Process (computing)",
                "Digital marketing",
                "Marketing management",
                "Marketing research",
                "Business",
                "Knowledge management",
                "Sociology",
                "Computer science",
                "Artificial intelligence",
                "Pedagogy",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://eprints.lancs.ac.uk/id/eprint/206556/1/IJIM_FinalAccepted_5_10_2023.pdf"
        },
        "https://openalex.org/W4386711999": {
            "title": "The GenAI is out of the bottle: generative artificial intelligence from a business model innovation perspective",
            "openalex_id": "https://openalex.org/W4386711999",
            "cited_by_count": 233,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W54497855",
                "https://openalex.org/W3125814499",
                "https://openalex.org/W3121970507",
                "https://openalex.org/W2899084033",
                "https://openalex.org/W2380075625",
                "https://openalex.org/W217960748",
                "https://openalex.org/W2148533276",
                "https://openalex.org/W2110028391",
                "https://openalex.org/W2090827041",
                "https://openalex.org/W2032233321"
            ],
            "references": [
                "https://openalex.org/W2012759258",
                "https://openalex.org/W203466237",
                "https://openalex.org/W2173098628",
                "https://openalex.org/W2201618547",
                "https://openalex.org/W2268817733",
                "https://openalex.org/W2293716611",
                "https://openalex.org/W2553003093",
                "https://openalex.org/W2553227351",
                "https://openalex.org/W2618530766",
                "https://openalex.org/W2891378911"
            ],
            "authors": [
                "Dominik K. Kanbach",
                "Louisa Heiduk",
                "Georg Blueher",
                "Maximilian Schreiter",
                "Alexander Lahmann"
            ],
            "venue": "Review of Managerial Science",
            "doi": "https://doi.org/10.1007/s11846-023-00696-z",
            "concepts": [
                "Variety (cybernetics)",
                "Perspective (graphical)",
                "Generative grammar",
                "Business model",
                "Computer science",
                "Business intelligence",
                "Knowledge management",
                "Data science",
                "Marketing",
                "Artificial intelligence",
                "Business"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://link.springer.com/content/pdf/10.1007/s11846-023-00696-z.pdf",
            "abstract": "Abstract The introduction of ChatGPT in November 2022 by OpenAI has stimulated substantial discourse on the implementation of artificial intelligence (AI) in various domains such as academia, business, and society at large. Although AI has been utilized in numerous areas for several years, the emergence of generative AI (GAI) applications such as ChatGPT, Jasper, or DALL-E are considered a breakthrough for the acceleration of AI technology due to their ease of use, intuitive interface, and performance. With GAI, it is possible to create a variety of content such as texts, images, audio, code, and even videos. This creates a variety of implications for businesses requiring a deeper examination, including an influence on business model innovation (BMI). Therefore, this study provides a BMI perspective on GAI with two primary contributions: (1) The development of six comprehensive propositions outlining the impact of GAI on businesses, and (2) the discussion of three industry examples, specifically software engineering, healthcare, and financial services. This study employs a qualitative content analysis using a scoping review methodology, drawing from a wide-ranging sample of 513 data points. These include academic publications, company reports, and public information such as press releases, news articles, interviews, and podcasts. The study thus contributes to the growing academic discourse in management research concerning AI's potential impact and offers practical insights into how to utilize this technology to develop new or improve existing business models."
        },
        "https://openalex.org/W4388817859": {
            "title": "ChatGPT in education: A blessing or a curse? A qualitative study exploring early adopters\u2019 utilization and perceptions",
            "openalex_id": "https://openalex.org/W4388817859",
            "cited_by_count": 231,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W3180421702",
                "https://openalex.org/W3165692504",
                "https://openalex.org/W3133282625",
                "https://openalex.org/W2413520594",
                "https://openalex.org/W2314038984",
                "https://openalex.org/W2313993255",
                "https://openalex.org/W2313412377",
                "https://openalex.org/W2005044196",
                "https://openalex.org/W1911503363",
                "https://openalex.org/W1485007142"
            ],
            "references": [
                "https://openalex.org/W1979290264",
                "https://openalex.org/W1995742667",
                "https://openalex.org/W2004468698",
                "https://openalex.org/W2044454790",
                "https://openalex.org/W2099697766",
                "https://openalex.org/W2134512579",
                "https://openalex.org/W2140279161",
                "https://openalex.org/W2769533150",
                "https://openalex.org/W2896282747",
                "https://openalex.org/W2896457183"
            ],
            "authors": [
                "Reza Hadi Mogavi",
                "Chao Deng",
                "Justin Juho Kim",
                "Pengyuan Zhou",
                "Young D. Kwon",
                "Ahmed Hosny Saleh Metwally",
                "Ahmed Tlili",
                "Simone Bassanelli",
                "Antonio Bucchiarone",
                "Sujit Gujar",
                "Lennart E. Nacke",
                "Pan Hui"
            ],
            "venue": "Computers in Human Behavior Artificial Humans",
            "doi": "https://doi.org/10.1016/j.chbah.2023.100027",
            "concepts": [
                "Blessing",
                "Curse",
                "Early adopter",
                "Perception",
                "Qualitative research",
                "Psychology",
                "Political science",
                "Business",
                "Sociology",
                "Marketing",
                "Geography",
                "Social science",
                "Anthropology",
                "Archaeology",
                "Neuroscience"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.chbah.2023.100027",
            "abstract": "To foster the development of pedagogically potent and ethically sound AI-integrated learning landscapes, it is pivotal to critically explore the perceptions and experiences of the users immersed in these contexts. In this study, we perform a thorough qualitative content analysis across four key social media platforms. Our goal is to understand the user experience (UX) and views of early adopters of ChatGPT across different education sectors. The results of our research show that ChatGPT is most commonly used in the domains of higher education, K-12 education, and practical skills training. In social media dialogues, the topics most frequently associated with ChatGPT are productivity, efficiency, and ethics. Early adopters' attitudes towards ChatGPT are multifaceted. On one hand, some view it as a transformative tool capable of amplifying student self-efficacy and learning motivation. On the other hand, there is a degree of apprehension among others. They worry about a potential overdependence on the AI system, which they fear might encourage superficial learning habits and erode students' social and critical thinking skills. This dichotomy of opinions underscores the complexity of Human-AI Interaction in educational contexts. Our investigation adds depth to this ongoing discourse, providing crowd-sourced insights for educators and learners who are considering incorporating ChatGPT or similar generative AI tools into their pedagogical strategies."
        },
        "https://openalex.org/W4386615720": {
            "title": "Use of ChatGPT in academia: Academic integrity hangs in the balance",
            "openalex_id": "https://openalex.org/W4386615720",
            "cited_by_count": 225,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4385729965",
                "https://openalex.org/W2768265748",
                "https://openalex.org/W2414344447",
                "https://openalex.org/W2183565050",
                "https://openalex.org/W2117415298",
                "https://openalex.org/W2115750419",
                "https://openalex.org/W2082389887",
                "https://openalex.org/W2042754183",
                "https://openalex.org/W1993891978",
                "https://openalex.org/W1782247654"
            ],
            "references": [
                "https://openalex.org/W1820801730",
                "https://openalex.org/W1970041439",
                "https://openalex.org/W1993879187",
                "https://openalex.org/W1996299251",
                "https://openalex.org/W2000380427",
                "https://openalex.org/W2014134373",
                "https://openalex.org/W2039810561",
                "https://openalex.org/W2057363637",
                "https://openalex.org/W2084957023",
                "https://openalex.org/W2088586850"
            ],
            "authors": [
                "Saeed Awadh Bin-Nashwan",
                "Mouad Sadallah",
                "Mohamed Bouteraa"
            ],
            "venue": "Technology in Society",
            "doi": "https://doi.org/10.1016/j.techsoc.2023.102370",
            "concepts": [
                "Academic integrity",
                "Cheating",
                "Psychology",
                "Work (physics)",
                "Variety (cybernetics)",
                "Academic dishonesty",
                "Academic writing",
                "Balance (ability)",
                "Social psychology",
                "Computer science",
                "Mathematics education",
                "Artificial intelligence",
                "Engineering",
                "Mechanical engineering",
                "Neuroscience"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.techsoc.2023.102370",
            "abstract": "In today's academic world, some academicians, researchers and students have begun employing Artificial Intelligence (AI) language models, e.g., ChatGPT, in completing a variety of academic tasks, including generating ideas, summarising literature, and essay writing. However, the use of ChatGPT in academic settings is a controversial issue, leading to a severe concern about academic integrity and AI-assisted cheating, while scholarly communities still lack clear principles on using such innovation in academia. Accordingly, this study aims to understand the motivations driving academics and researchers to use ChatGPT in their work, and specifically the role of academic integrity in making up adoption behavior. Based on 702 responses retrieved from users of ResearchGate and Academia.edu, we found that ChatGPT usage is positively shaped by time-saving feature, e-word of mouth, academic self-efficacy, academic self-esteem, and perceived stress. In contrast, peer influence and academic integrity had a negative effect on usage. Intriguingly, academic integrity-moderated interactions of time-saving, self-esteem and perceived stress on ChatGPT usage are found to be significantly positive. Therefore, we suggest that stakeholders, including academic institutions, publishers and AI language models' programmers, should work together to specify necessary guidelines for the ethical use of AI chatbots in academic work and research."
        },
        "https://openalex.org/W4387815597": {
            "title": "Exploring Artificial Intelligence in Academic Essay: Higher Education Student's Perspective",
            "openalex_id": "https://openalex.org/W4387815597",
            "cited_by_count": 225,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4295161771",
                "https://openalex.org/W4289532678",
                "https://openalex.org/W4238513149",
                "https://openalex.org/W2994258365",
                "https://openalex.org/W2901253749",
                "https://openalex.org/W2894069061",
                "https://openalex.org/W2616269406",
                "https://openalex.org/W2468223644",
                "https://openalex.org/W2364113653",
                "https://openalex.org/W1528089385"
            ],
            "references": [
                "https://openalex.org/W1560866327",
                "https://openalex.org/W2068015094",
                "https://openalex.org/W2149606817",
                "https://openalex.org/W2164339913",
                "https://openalex.org/W2337155942",
                "https://openalex.org/W2804896411",
                "https://openalex.org/W2909340128",
                "https://openalex.org/W2917352236",
                "https://openalex.org/W2917855746",
                "https://openalex.org/W2944719322"
            ],
            "authors": [
                "Agung Rinaldy Malik",
                "Yuni Pratiwi",
                "Kusubakti Andajani",
                "I Wayan Numertayasa",
                "Sri Suharti",
                "Arisa Darwis",
                "Marzuki Marzuki"
            ],
            "venue": "International Journal of Educational Research Open",
            "doi": "https://doi.org/10.1016/j.ijedro.2023.100296",
            "concepts": [
                "Creativity",
                "Transformative learning",
                "Ingenuity",
                "Academic writing",
                "Indonesian",
                "Critical thinking",
                "Higher education",
                "Mathematics education",
                "Psychology",
                "Sociology",
                "Pedagogy",
                "Social psychology",
                "Linguistics",
                "Epistemology",
                "Philosophy",
                "Political science",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.ijedro.2023.100296",
            "abstract": "Artificial Intelligence (AI) and academic essay writing merge to create a transformative intersection in education, each reciprocally refining and reforming the other. AI, through its innovative technologies and flexible learning strategies, elevates academic writing by offering dynamic, interactive learning settings, and personalized educational journeys. This study aimed to explore students' perceptions of AI usage in academic essay writing using a case study design. It involved 245 undergraduate students from 25 tertiary institutions in Eastern and Central Indonesian provinces, with data collected via Google form to gain insights into students' attitudes toward AI tools in writing academic essays. Findings indicated a positive reception of AI-powered writing tools, with students acknowledging their benefits in grammar checks, plagiarism detection, language translation, and essay outlines. AI was found to enhance students' writing abilities, self-efficacy, and understanding of academic integrity. However, some students expressed concerns about potential impacts on creativity, critical thinking, and ethical writing practices. The study emphasized a balanced approach to AI integration, where AI collaborates with human authors, and also identified popular AI tools used by Indonesian students. As such, the research highlighted AI's significant role in supporting academic writing while preserving human creativity and critical thinking, underscoring the importance of maintaining a balanced integration to sustain human ingenuity and critical thought in academic discourse."
        },
        "https://openalex.org/W4362667540": {
            "title": "On the use of AI-based tools like ChatGPT to support management research",
            "openalex_id": "https://openalex.org/W4362667540",
            "cited_by_count": 219,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4200071106",
                "https://openalex.org/W3126095231",
                "https://openalex.org/W2593155302",
                "https://openalex.org/W2384288445",
                "https://openalex.org/W2382526412",
                "https://openalex.org/W2378906650",
                "https://openalex.org/W2352855287",
                "https://openalex.org/W2347474189",
                "https://openalex.org/W2072812638",
                "https://openalex.org/W2041415459"
            ],
            "references": [
                "https://openalex.org/W2100495367",
                "https://openalex.org/W2953532875",
                "https://openalex.org/W3000235744",
                "https://openalex.org/W3094550259",
                "https://openalex.org/W3096831136",
                "https://openalex.org/W3121943121",
                "https://openalex.org/W3125505924",
                "https://openalex.org/W3195132146",
                "https://openalex.org/W4242937284",
                "https://openalex.org/W4306178549"
            ],
            "authors": [
                "Bastian Burger",
                "Dominik K. Kanbach",
                "Sascha Kraus",
                "Matthias Breier",
                "Vincenzo Corvello"
            ],
            "venue": "European Journal of Innovation Management",
            "doi": "https://doi.org/10.1108/ejim-02-2023-0156",
            "concepts": [
                "Computer science",
                "Objectivity (philosophy)",
                "Originality",
                "Data science",
                "Terminology",
                "Artificial intelligence",
                "Relevance (law)",
                "Management science",
                "Knowledge management",
                "Qualitative research",
                "Engineering",
                "Sociology",
                "Political science",
                "Social science",
                "Philosophy",
                "Linguistics",
                "Epistemology",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.emerald.com/insight/content/doi/10.1108/EJIM-02-2023-0156/full/pdf?title=on-the-use-of-ai-based-tools-like-chatgpt-to-support-management-research",
            "abstract": "Purpose The article discusses the current relevance of artificial intelligence (AI) in research and how AI improves various research methods. This article focuses on the practical case study of systematic literature reviews (SLRs) to provide a guideline for employing AI in the process. Design/methodology/approach Researchers no longer require technical skills to use AI in their research. The recent discussion about using Chat Generative Pre-trained Transformer (GPT), a chatbot by OpenAI, has reached the academic world and fueled heated debates about the future of academic research. Nevertheless, as the saying goes, AI will not replace our job; a human being using AI will. This editorial aims to provide an overview of the current state of using AI in research, highlighting recent trends and developments in the field. Findings The main result is guidelines for the use of AI in the scientific research process. The guidelines were developed for the literature review case but the authors believe the instructions provided can be adjusted to many fields of research, including but not limited to quantitative research, data qualification, research on unstructured data, qualitative data and even on many support functions and repetitive tasks. Originality/value AI already has the potential to make researchers\u2019 work faster, more reliable and more convenient. The authors highlight the advantages and limitations of AI in the current time, which should be present in any research utilizing AI. Advantages include objectivity and repeatability in research processes that currently are subject to human error. The most substantial disadvantages lie in the architecture of current general-purpose models, which understanding is essential for using them in research. The authors will describe the most critical shortcomings without going into technical detail and suggest how to work with the shortcomings daily."
        },
        "https://openalex.org/W4362721098": {
            "title": "Analysing the Role of ChatGPT in Improving Student Productivity in Higher Education",
            "openalex_id": "https://openalex.org/W4362721098",
            "cited_by_count": 217,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4237969969",
                "https://openalex.org/W419536403",
                "https://openalex.org/W2997121352",
                "https://openalex.org/W2773393136",
                "https://openalex.org/W2509366663",
                "https://openalex.org/W2506280730",
                "https://openalex.org/W2475724061",
                "https://openalex.org/W2366328218",
                "https://openalex.org/W2174706483",
                "https://openalex.org/W1594297642"
            ],
            "references": [
                "https://openalex.org/W4281398834",
                "https://openalex.org/W4287878175",
                "https://openalex.org/W4303968735",
                "https://openalex.org/W4308947461",
                "https://openalex.org/W4315498228",
                "https://openalex.org/W4321106177",
                "https://openalex.org/W4321153003",
                "https://openalex.org/W4321499901",
                "https://openalex.org/W4323313947",
                "https://openalex.org/W4323655724"
            ],
            "authors": [
                "Fauzi Fauzi",
                "Laros Tuhuteru",
                "Ferdinandus Sampe",
                "Abu Muna Almaududi Ausat",
                "Heliza Rahmania Hatta"
            ],
            "venue": "Journal on Education",
            "doi": "https://doi.org/10.31004/joe.v5i4.2563",
            "concepts": [
                "Productivity",
                "Active listening",
                "Quality (philosophy)",
                "Data collection",
                "Computer science",
                "Knowledge management",
                "Qualitative property",
                "Mathematics education",
                "Psychology",
                "Sociology",
                "Social science",
                "Philosophy",
                "Communication",
                "Epistemology",
                "Machine learning",
                "Economics",
                "Macroeconomics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://jonedu.org/index.php/joe/article/download/2563/2162",
            "abstract": "Student productivity in higher education is still a problem. In the digital era, technology is increasingly developing and provides convenience for doing various things, including in terms of learning. The purpose of this study will be an analysis of the role of ChatGPT in helping to improve the quality of student productivity. This research is qualitative in nature. Data collection techniques include listening and recording important information to conduct data analysis through data reduction, data display, and conclusion drawing. This study concludes that ChatGPT can make a significant contribution in improving the quality of student productivity. This language model can help students in various ways, such as providing useful information and resources, helping to improve language skills, facilitating collaboration, increasing time efficiency and effectiveness, and providing support and motivation."
        },
        "https://openalex.org/W4386696838": {
            "title": "Are users willing to embrace ChatGPT? Exploring the factors on the acceptance of chatbots from the perspective of AIDUA framework",
            "openalex_id": "https://openalex.org/W4386696838",
            "cited_by_count": 207,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4387007686",
                "https://openalex.org/W4383501580",
                "https://openalex.org/W4382052417",
                "https://openalex.org/W4313813117",
                "https://openalex.org/W4293646425",
                "https://openalex.org/W4214931137",
                "https://openalex.org/W3192088754",
                "https://openalex.org/W3176146353",
                "https://openalex.org/W3150216605",
                "https://openalex.org/W3084631705"
            ],
            "references": [
                "https://openalex.org/W1951721333",
                "https://openalex.org/W1956939146",
                "https://openalex.org/W2030304935",
                "https://openalex.org/W2066698317",
                "https://openalex.org/W2077267804",
                "https://openalex.org/W2085887978",
                "https://openalex.org/W2100379340",
                "https://openalex.org/W2105846236",
                "https://openalex.org/W2139087511",
                "https://openalex.org/W2212401464"
            ],
            "authors": [
                "Xiaoyue Ma",
                "Yudi Huo"
            ],
            "venue": "Technology in Society",
            "doi": "https://doi.org/10.1016/j.techsoc.2023.102362",
            "concepts": [
                "Chatbot",
                "Novelty",
                "Value (mathematics)",
                "Psychology",
                "Technology acceptance model",
                "Perspective (graphical)",
                "Cognition",
                "Social psychology",
                "Applied psychology",
                "Computer science",
                "Artificial intelligence",
                "Usability",
                "Human\u2013computer interaction",
                "Machine learning",
                "Neuroscience"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null
        },
        "https://openalex.org/W4382775625": {
            "title": "Artificial intelligence\u2019s impact on hospitality and tourism marketing: exploring key themes and addressing challenges",
            "openalex_id": "https://openalex.org/W4382775625",
            "cited_by_count": 204,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W2523188470",
                "https://openalex.org/W2521095233",
                "https://openalex.org/W2241904850",
                "https://openalex.org/W2116161828",
                "https://openalex.org/W2095919374",
                "https://openalex.org/W2084123167",
                "https://openalex.org/W1983398261",
                "https://openalex.org/W1886201166",
                "https://openalex.org/W1482830255",
                "https://openalex.org/W1418971400"
            ],
            "references": [
                "https://openalex.org/W1556258326",
                "https://openalex.org/W1836714491",
                "https://openalex.org/W1981425990",
                "https://openalex.org/W2023302438",
                "https://openalex.org/W2076198150",
                "https://openalex.org/W2150104072",
                "https://openalex.org/W2395490559",
                "https://openalex.org/W2403312433",
                "https://openalex.org/W2473314243",
                "https://openalex.org/W2487200295"
            ],
            "authors": [
                "Jacques Bulchand\u2010Gidumal",
                "Eduardo William Secin",
                "Peter O\u2019Connor",
                "Dimitrios Buhalis"
            ],
            "venue": "Current Issues in Tourism",
            "doi": "https://doi.org/10.1080/13683500.2023.2229480",
            "concepts": [
                "Hospitality",
                "Tourism",
                "Key (lock)",
                "Marketing",
                "Hospitality management studies",
                "Business",
                "Psychology",
                "Knowledge management",
                "Political science",
                "Computer science",
                "Law",
                "Computer security"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.tandfonline.com/doi/pdf/10.1080/13683500.2023.2229480?needAccess=true&role=button",
            "abstract": "Understanding how Artificial Intelligence (AI) impacts organizational functions supports stakeholders to prepare accordingly and profit from these developments. Adopting a grounded theory approach, this study uses three interlinked stages (in-depth interviews, focus groups and a questionnaire-based survey) to explore the impact of AI on the marketing function of hotels. The results identify ten trends related to AI's contribution to hotel marketing, clustered in four themes. AI reengineers internal processes and procedures by enabling data and content as catalysts of competitiveness; empowering the augmented worker and performing mass personalization and customization. AI also impacts relationships with stakeholders by determining return on investment; improving sustainability; and governing legal aspects and ethics regarding data use. AI supports networks to which the organizations belong by concentrating and integrating organizations and transforming distribution models. AI transforms customer processes and services by engaging smart and predictive customer care and by employing predictive and augmented product and service design. The study illustrates the changes that AI will likely bring to hospitality and tourism marketing, developing a research agenda and raising discussion points for academic and industry practitioners respectively."
        },
        "https://openalex.org/W4381332452": {
            "title": "Generative artificial intelligence in the metaverse era",
            "openalex_id": "https://openalex.org/W4381332452",
            "cited_by_count": 200,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4380551139",
                "https://openalex.org/W4365211920",
                "https://openalex.org/W4317425055",
                "https://openalex.org/W4312472357",
                "https://openalex.org/W4309465602",
                "https://openalex.org/W3014948380",
                "https://openalex.org/W2996036240",
                "https://openalex.org/W2248423326",
                "https://openalex.org/W2188884285",
                "https://openalex.org/W2166821106"
            ],
            "references": [
                "https://openalex.org/W2905776180",
                "https://openalex.org/W2935647139",
                "https://openalex.org/W2960610835",
                "https://openalex.org/W2972687021",
                "https://openalex.org/W2980703584",
                "https://openalex.org/W3001434439",
                "https://openalex.org/W3019917466",
                "https://openalex.org/W3047397334",
                "https://openalex.org/W3089502171",
                "https://openalex.org/W3146873565"
            ],
            "authors": [
                "Zhihan Lv"
            ],
            "venue": "Cognitive Robotics",
            "doi": "https://doi.org/10.1016/j.cogr.2023.06.001",
            "concepts": [
                "Generative grammar",
                "Metaverse",
                "Computer science",
                "Possible world",
                "Generative model",
                "Presentation (obstetrics)",
                "Artificial intelligence",
                "Epistemology",
                "Virtual reality",
                "Philosophy",
                "Medicine",
                "Radiology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.cogr.2023.06.001",
            "abstract": "Generative artificial intelligence (AI) is a form of AI that can autonomously generate new content, such as text, images, audio, and video. Generative AI provides innovative approaches for content production in the metaverse, filling gaps in the development of the metaverse. Products such as ChatGPT have the potential to enhance the search experience, reshape information generation and presentation methods, and become new entry points for online traffic. This is expected to significantly impact traditional search engine products, accelerating industry innovation and upgrading. This paper presents an overview of the technologies and prospective applications of generative AI in the breakthrough of metaverse technology and offers insights for increasing the effectiveness of generative AI in creating creative content."
        },
        "https://openalex.org/W4389359039": {
            "title": "Generative artificial intelligence",
            "openalex_id": "https://openalex.org/W4389359039",
            "cited_by_count": 197,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4396941953",
                "https://openalex.org/W4391334978",
                "https://openalex.org/W4388137171",
                "https://openalex.org/W4380551139",
                "https://openalex.org/W4317695495",
                "https://openalex.org/W4301024388",
                "https://openalex.org/W4241564561",
                "https://openalex.org/W2987280934",
                "https://openalex.org/W2093104230",
                "https://openalex.org/W1967909251"
            ],
            "references": [
                "https://openalex.org/W156476889",
                "https://openalex.org/W1593325875",
                "https://openalex.org/W1965473255",
                "https://openalex.org/W1988076559",
                "https://openalex.org/W2174890733",
                "https://openalex.org/W2528491735",
                "https://openalex.org/W2612690371",
                "https://openalex.org/W2742108348",
                "https://openalex.org/W2776393547",
                "https://openalex.org/W2804927761"
            ],
            "authors": [
                "Leonardo Banh",
                "Gero Strobel"
            ],
            "venue": "Electronic Markets",
            "doi": "https://doi.org/10.1007/s12525-023-00680-1",
            "concepts": [
                "Generative grammar",
                "Computer science",
                "Artificial intelligence",
                "Field (mathematics)",
                "Generative model",
                "Discriminative model",
                "Generative Design",
                "Engineering",
                "Mathematics",
                "Pure mathematics",
                "Metric (unit)",
                "Operations management"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://link.springer.com/content/pdf/10.1007/s12525-023-00680-1.pdf",
            "abstract": "Abstract Recent developments in the field of artificial intelligence (AI) have enabled new paradigms of machine processing, shifting from data-driven, discriminative AI tasks toward sophisticated, creative tasks through generative AI. Leveraging deep generative models, generative AI is capable of producing novel and realistic content across a broad spectrum (e.g., texts, images, or programming code) for various domains based on basic user prompts. In this article, we offer a comprehensive overview of the fundamentals of generative AI with its underpinning concepts and prospects. We provide a conceptual introduction to relevant terms and techniques, outline the inherent properties that constitute generative AI, and elaborate on the potentials and challenges. We underline the necessity for researchers and practitioners to comprehend the distinctive characteristics of generative artificial intelligence in order to harness its potential while mitigating its risks and to contribute to a principal understanding."
        },
        "https://openalex.org/W4392452942": {
            "title": "Using artificial intelligence in academic writing and research: An essential productivity tool",
            "openalex_id": "https://openalex.org/W4392452942",
            "cited_by_count": 196,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W756498608",
                "https://openalex.org/W4394443292",
                "https://openalex.org/W4285395220",
                "https://openalex.org/W4251394462",
                "https://openalex.org/W2493576743",
                "https://openalex.org/W2486167009",
                "https://openalex.org/W2135201366",
                "https://openalex.org/W1595575899",
                "https://openalex.org/W1580673008",
                "https://openalex.org/W1510936208"
            ],
            "references": [
                "https://openalex.org/W2783481537",
                "https://openalex.org/W2907510836",
                "https://openalex.org/W2972944069",
                "https://openalex.org/W2992586577",
                "https://openalex.org/W3019693485",
                "https://openalex.org/W3033472758",
                "https://openalex.org/W3112951420",
                "https://openalex.org/W3163841364",
                "https://openalex.org/W3204030525",
                "https://openalex.org/W3207456925"
            ],
            "authors": [
                "Mohamed Khalifa",
                "Mona Albadawy"
            ],
            "venue": "Computer Methods and Programs in Biomedicine Update",
            "doi": "https://doi.org/10.1016/j.cmpbup.2024.100145",
            "concepts": [
                "Computer science",
                "Relevance (law)",
                "Workflow",
                "Academic writing",
                "Engineering ethics",
                "Structuring",
                "Data science",
                "Knowledge management",
                "Psychology",
                "Mathematics education",
                "Engineering",
                "Political science",
                "Database",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.cmpbup.2024.100145",
            "abstract": "Academic writing is an essential component of research, characterized by structured expression of ideas, data-driven arguments, and logical reasoning. However, it poses challenges such as handling vast amounts of information and complex ideas. The integration of Artificial Intelligence (AI) into academic writing has become increasingly important, offering solutions to these challenges. This review aims to explore specific domains where AI significantly supports academic writing. A systematic review of literature from databases like PubMed, Embase, and Google Scholar, published since 2019, was conducted. Studies were included based on relevance to AI's application in academic writing and research, focusing on writing assistance, grammar improvement, structure optimization, and other related aspects. The search identified 24 studies through which six core domains were identified where AI helps academic writing and research: 1) facilitating idea generation and research design, 2) improving content and structuring, 3) supporting literature review and synthesis, 4) enhancing data management and analysis, 5) supporting editing, review, and publishing, and 6) assisting in communication, outreach, and ethical compliance. ChatGPT has shown substantial potential in these areas, though challenges like maintaining academic integrity and balancing AI use with human insight remain. AI significantly revolutionises academic writing and research across various domains. Recommendations include broader integration of AI tools in research workflows, emphasizing ethical and transparent use, providing adequate training for researchers, and maintaining a balance between AI utility and human insight. Ongoing research and development are essential to address emerging challenges and ethical considerations in AI's application in academia."
        },
        "https://openalex.org/W4386178880": {
            "title": "What drives students toward ChatGPT? An investigation of the factors influencing adoption and usage of ChatGPT",
            "openalex_id": "https://openalex.org/W4386178880",
            "cited_by_count": 192,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4200071106",
                "https://openalex.org/W3126095231",
                "https://openalex.org/W2593155302",
                "https://openalex.org/W2384288445",
                "https://openalex.org/W2382526412",
                "https://openalex.org/W2378906650",
                "https://openalex.org/W2352855287",
                "https://openalex.org/W2347474189",
                "https://openalex.org/W2072812638",
                "https://openalex.org/W2041415459"
            ],
            "references": [
                "https://openalex.org/W1608315453",
                "https://openalex.org/W1791587663",
                "https://openalex.org/W1969941463",
                "https://openalex.org/W1977971945",
                "https://openalex.org/W1982320306",
                "https://openalex.org/W1992085202",
                "https://openalex.org/W1994213885",
                "https://openalex.org/W2001429584",
                "https://openalex.org/W2033943395",
                "https://openalex.org/W2040467573"
            ],
            "authors": [
                "Chandan Kumar Tiwari",
                "Mohd Abass Bhat",
                "Shagufta Tariq Khan",
                "R. Subramaniam",
                "M. A. Khan"
            ],
            "venue": "Interactive Technology and Smart Education",
            "doi": "https://doi.org/10.1108/itse-04-2023-0061",
            "concepts": [
                "Novelty",
                "Originality",
                "Technology acceptance model",
                "Structural equation modeling",
                "Knowledge management",
                "Usability",
                "Product (mathematics)",
                "Psychology",
                "Instructional design",
                "Computer science",
                "Mathematics education",
                "Social psychology",
                "Creativity",
                "Geometry",
                "Mathematics",
                "Human\u2013computer interaction",
                "Machine learning"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null,
            "abstract": "Purpose The purpose of this paper is to identify the factors determining students\u2019 attitude toward using newly emerged artificial intelligence (AI) tool, Chat Generative Pre-Trained Transformer (ChatGPT), for educational and learning purpose based on technology acceptance model. Design/methodology/approach The recommended model was empirically tested with partial least squares structural equation modeling using 375 student survey responses. Findings The study revealed that students have a favorable view of the instructional use of ChatGPT. Usefulness, social presence and legitimacy of the tool, as well as enjoyment and motivation, contribute to a favorable attitude toward using this tool in a learning environment. However, perceived ease of use was not found to be a significant determinant in the adoption and utilization of ChatGPT by the students. Practical implications This research is intended to benefit enterprises, academic institutions and the global community by offering light on how students perceive the ChatGPT service in an educational setting. Furthermore, the application enhances confidence and interest among learners, leading to improved literacy and general awareness. Eventually, the outcome of this research will help AI developers to improve their product and service delivery, as well as benefit regulators in regulating the usage of AI-based bots. Originality/value Due to its novelty, the current research on AI-based ChatGPT usage in the education sector is rather restricted. This study provides the adoption aspects of ChatGPT, a new AI-based technology for students, thereby contributing significantly to the existing research on the adoption of advanced education technologies. In addition, the literature lacks research on the adoption of ChatGPT by students for educational purposes; this study addresses this gap by identifying adoption determinants of ChatGPT in education."
        },
        "https://openalex.org/W4386221177": {
            "title": "Educational Design Principles of Using AI Chatbot That Supports Self-Regulated Learning in Education: Goal Setting, Feedback, and Personalization",
            "openalex_id": "https://openalex.org/W4386221177",
            "cited_by_count": 188,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4387007686",
                "https://openalex.org/W4383501580",
                "https://openalex.org/W4382052417",
                "https://openalex.org/W4362575170",
                "https://openalex.org/W4313813117",
                "https://openalex.org/W4214931137",
                "https://openalex.org/W3192088754",
                "https://openalex.org/W3084631705",
                "https://openalex.org/W1819252502",
                "https://openalex.org/W1532772865"
            ],
            "references": [
                "https://openalex.org/W1526122805",
                "https://openalex.org/W1583411474",
                "https://openalex.org/W1748197048",
                "https://openalex.org/W1971218727",
                "https://openalex.org/W2001771035",
                "https://openalex.org/W2001829221",
                "https://openalex.org/W2011636729",
                "https://openalex.org/W2015490535",
                "https://openalex.org/W2015584585",
                "https://openalex.org/W2019760471"
            ],
            "authors": [
                "Daniel Chang",
                "Michael Pin-Chuan Lin",
                "Shiva Hajian",
                "Quincy Q. Wang"
            ],
            "venue": "Sustainability",
            "doi": "https://doi.org/10.3390/su151712921",
            "concepts": [
                "Chatbot",
                "Personalization",
                "Conceptualization",
                "Self-regulated learning",
                "Computer science",
                "Knowledge management",
                "Psychology",
                "World Wide Web",
                "Mathematics education",
                "Artificial intelligence"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/2071-1050/15/17/12921/pdf?version=1693121769",
            "abstract": "The invention of ChatGPT and generative AI technologies presents educators with significant challenges, as concerns arise regarding students potentially exploiting these tools unethically, misrepresenting their work, or gaining academic merits without active participation in the learning process. To effectively navigate this shift, it is crucial to embrace AI as a contemporary educational trend and establish pedagogical principles for properly utilizing emerging technologies like ChatGPT to promote self-regulation. Rather than suppressing AI-driven tools, educators should foster collaborations among stakeholders, including educators, instructional designers, AI researchers, and developers. This paper proposes three key pedagogical principles for integrating AI chatbots in classrooms, informed by Zimmerman\u2019s Self-Regulated Learning (SRL) framework and Judgment of Learning (JOL). We argue that the current conceptualization of AI chatbots in education is inadequate, so we advocate for the incorporation of goal setting (prompting), self-assessment and feedback, and personalization as three essential educational principles. First, we propose that teaching prompting is important for developing students\u2019 SRL. Second, configuring reverse prompting in the AI chatbot\u2019s capability will help to guide students\u2019 SRL and monitoring for understanding. Third, developing a data-driven mechanism that enables an AI chatbot to provide learning analytics helps learners to reflect on learning and develop SRL strategies. By bringing in Zimmerman\u2019s SRL framework with JOL, we aim to provide educators with guidelines for implementing AI in teaching and learning contexts, with a focus on promoting students\u2019 self-regulation in higher education through AI-assisted pedagogy and instructional design."
        },
        "https://openalex.org/W4378474327": {
            "title": "ChatGPT: Vision and challenges",
            "openalex_id": "https://openalex.org/W4378474327",
            "cited_by_count": 187,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4388870064",
                "https://openalex.org/W4248308508",
                "https://openalex.org/W4235186151",
                "https://openalex.org/W2667588871",
                "https://openalex.org/W2272354214",
                "https://openalex.org/W2210139803",
                "https://openalex.org/W2084768720",
                "https://openalex.org/W2056057048",
                "https://openalex.org/W2054685365",
                "https://openalex.org/W2043010663"
            ],
            "references": [
                "https://openalex.org/W2057515938",
                "https://openalex.org/W2784039661",
                "https://openalex.org/W2954296452",
                "https://openalex.org/W3125358881",
                "https://openalex.org/W3129706735",
                "https://openalex.org/W3174097952",
                "https://openalex.org/W3187018546",
                "https://openalex.org/W3217422184",
                "https://openalex.org/W4220966162",
                "https://openalex.org/W4312362554"
            ],
            "authors": [
                "Sukhpal Singh Gill",
                "Rupinder Kaur"
            ],
            "venue": "Internet of Things and Cyber-Physical Systems",
            "doi": "https://doi.org/10.1016/j.iotcps.2023.05.004",
            "concepts": [
                "Bridging (networking)",
                "Computer science",
                "Internet of Things",
                "Artificial intelligence",
                "The Internet",
                "Robotics",
                "Data science",
                "Engineering ethics",
                "Engineering",
                "Computer security",
                "Robot",
                "World Wide Web"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.iotcps.2023.05.004",
            "abstract": "Artificial intelligence (AI) and machine learning have changed the nature of scientific inquiry in recent years. Of these, the development of virtual assistants has accelerated greatly in the past few years, with ChatGPT becoming a prominent AI language model. In this study, we examine the foundations, vision, research challenges of ChatGPT. This article investigates into the background and development of the technology behind it, as well as its popular applications. Moreover, we discuss the advantages of bringing everything together through ChatGPT and Internet of Things (IoT). Further, we speculate on the future of ChatGPT by considering various possibilities for study and development, such as energy-efficiency, cybersecurity, enhancing its applicability to additional technologies (Robotics and Computer Vision), strengthening human-AI communications, and bridging the technological gap. Finally, we discuss the important ethics and current trends of ChatGPT."
        },
        "https://openalex.org/W4392783116": {
            "title": "Is it harmful or helpful? Examining the causes and consequences of generative AI usage among university students",
            "openalex_id": "https://openalex.org/W4392783116",
            "cited_by_count": 186,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W4390718435",
                "https://openalex.org/W4390549206",
                "https://openalex.org/W4237784285",
                "https://openalex.org/W3137171911",
                "https://openalex.org/W2931662336",
                "https://openalex.org/W2765597752",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2380075625",
                "https://openalex.org/W2134894512",
                "https://openalex.org/W2077865380"
            ],
            "references": [
                "https://openalex.org/W1924077383",
                "https://openalex.org/W2000380427",
                "https://openalex.org/W2023479110",
                "https://openalex.org/W2051980553",
                "https://openalex.org/W2054105061",
                "https://openalex.org/W2069622657",
                "https://openalex.org/W2076459868",
                "https://openalex.org/W2083309415",
                "https://openalex.org/W2083820822",
                "https://openalex.org/W2087558969"
            ],
            "authors": [
                "Muhammad Abbas",
                "Farooq Ahmed Jam",
                "Tariq Iqbal Khan"
            ],
            "venue": "International Journal of Educational Technology in Higher Education",
            "doi": "https://doi.org/10.1186/s41239-024-00444-7",
            "concepts": [
                "Generative grammar",
                "Higher education",
                "Psychology",
                "Mathematics education",
                "Political science",
                "Computer science",
                "Artificial intelligence",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://educationaltechnologyjournal.springeropen.com/counter/pdf/10.1186/s41239-024-00444-7",
            "abstract": "Abstract While the discussion on generative artificial intelligence, such as ChatGPT, is making waves in academia and the popular press, there is a need for more insight into the use of ChatGPT among students and the potential harmful or beneficial consequences associated with its usage. Using samples from two studies, the current research examined the causes and consequences of ChatGPT usage among university students. Study 1 developed and validated an eight-item scale to measure ChatGPT usage by conducting a survey among university students (N = 165). Study 2 used a three-wave time-lagged design to collect data from university students (N = 494) to further validate the scale and test the study\u2019s hypotheses. Study 2 also examined the effects of academic workload, academic time pressure, sensitivity to rewards, and sensitivity to quality on ChatGPT usage. Study 2 further examined the effects of ChatGPT usage on students\u2019 levels of procrastination, memory loss, and academic performance. Study 1 provided evidence for the validity and reliability of the ChatGPT usage scale. Furthermore, study 2 revealed that when students faced higher academic workload and time pressure, they were more likely to use ChatGPT. In contrast, students who were sensitive to rewards were less likely to use ChatGPT. Not surprisingly, use of ChatGPT was likely to develop tendencies for procrastination and memory loss and dampen the students\u2019 academic performance. Finally, academic workload, time pressure, and sensitivity to rewards had indirect effects on students\u2019 outcomes through ChatGPT usage."
        },
        "https://openalex.org/W4383812952": {
            "title": "Autonomous travel decision-making: An early glimpse into ChatGPT and generative AI",
            "openalex_id": "https://openalex.org/W4383812952",
            "cited_by_count": 185,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W595497825",
                "https://openalex.org/W4205828460",
                "https://openalex.org/W3217058410",
                "https://openalex.org/W2975827637",
                "https://openalex.org/W2365169615",
                "https://openalex.org/W2354089692",
                "https://openalex.org/W2071709014",
                "https://openalex.org/W2042118646",
                "https://openalex.org/W1970538215",
                "https://openalex.org/W173151060"
            ],
            "references": [
                "https://openalex.org/W2019979628",
                "https://openalex.org/W2032973212",
                "https://openalex.org/W2042772616",
                "https://openalex.org/W2099579568",
                "https://openalex.org/W2101117624",
                "https://openalex.org/W2118829576",
                "https://openalex.org/W2130333844",
                "https://openalex.org/W2142285466",
                "https://openalex.org/W2146373331",
                "https://openalex.org/W2165905365"
            ],
            "authors": [
                "IpKin Anthony Wong",
                "Qi Lilith Lian",
                "Danni Sun"
            ],
            "venue": "Journal of Hospitality and Tourism Management",
            "doi": "https://doi.org/10.1016/j.jhtm.2023.06.022",
            "concepts": [
                "Tourism",
                "Fluency",
                "Hospitality",
                "Computer science",
                "Snapshot (computer storage)",
                "Marketing",
                "Process (computing)",
                "Business",
                "Psychology",
                "Geography",
                "Mathematics education",
                "Archaeology",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null
        },
        "https://openalex.org/W4366780972": {
            "title": "Evolution of artificial intelligence research in Technological Forecasting and Social Change: Research topics, trends, and future directions",
            "openalex_id": "https://openalex.org/W4366780972",
            "cited_by_count": 183,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4220822672",
                "https://openalex.org/W3183948672",
                "https://openalex.org/W3173606202",
                "https://openalex.org/W3110381201",
                "https://openalex.org/W2948807893",
                "https://openalex.org/W2778153218",
                "https://openalex.org/W2753626182",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W1959662420",
                "https://openalex.org/W1531601525"
            ],
            "references": [
                "https://openalex.org/W1540841154",
                "https://openalex.org/W1970920490",
                "https://openalex.org/W1971555055",
                "https://openalex.org/W1984656434",
                "https://openalex.org/W1989691100",
                "https://openalex.org/W1989841161",
                "https://openalex.org/W1992760964",
                "https://openalex.org/W1995200506",
                "https://openalex.org/W1996838260",
                "https://openalex.org/W2011958273"
            ],
            "authors": [
                "Yogesh K. Dwivedi",
                "Anuj Sharma",
                "Nripendra P. Rana",
                "Mihalis Giannakis",
                "Pooja Goel",
                "Vincent Dutot"
            ],
            "venue": "Technological Forecasting and Social Change",
            "doi": "https://doi.org/10.1016/j.techfore.2023.122579",
            "concepts": [
                "Big data",
                "Discipline",
                "Diversity (politics)",
                "Artificial intelligence",
                "Knowledge management",
                "Data science",
                "Management science",
                "Computer science",
                "Sociology",
                "Social science",
                "Engineering",
                "Anthropology",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.techfore.2023.122579",
            "abstract": "Artificial intelligence (AI) is a set of rapidly expanding disruptive technologies that are radically transforming various aspects related to people, business, society, and the environment. With the proliferation of digital computing devices and the emergence of big data, AI is increasingly offering significant opportunities for society and business organizations. The growing interest of scholars and practitioners in AI has resulted in the diversity of research topics explored in bulks of scholarly literature published in leading research outlets. This study aims to map the intellectual structure and evolution of the conceptual structure of overall AI research published in Technological Forecasting and Social Change (TF&SC). This study uses machine learning-based structural topic modeling (STM) to extract, report, and visualize the latent topics from the AI research literature. Further, the disciplinary patterns in the intellectual structure of AI research are examined with the additional objective of assessing the disciplinary impact of AI. The results of the topic modeling reveal eight key topics, out of which the topics concerning healthcare, circular economy and sustainable supply chain, adoption of AI by consumers, and AI for decision-making are showing a rising trend over the years. AI research has a significant influence on disciplines such as business, management, and accounting, social science, engineering, computer science, and mathematics. The study provides an insightful agenda for the future based on evidence-based research directions that would benefit future AI scholars to identify contemporary research issues and develop impactful research to solve complex societal problems."
        },
        "https://openalex.org/W4378223984": {
            "title": "The use of ChatGPT in the digital era: Perspectives on chatbot implementation",
            "openalex_id": "https://openalex.org/W4378223984",
            "cited_by_count": 180,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4387007686",
                "https://openalex.org/W4383501580",
                "https://openalex.org/W4382052417",
                "https://openalex.org/W4313813117",
                "https://openalex.org/W4214931137",
                "https://openalex.org/W3192088754",
                "https://openalex.org/W3176146353",
                "https://openalex.org/W3084631705",
                "https://openalex.org/W2997034443",
                "https://openalex.org/W1774155268"
            ],
            "references": [
                "https://openalex.org/W2730196775",
                "https://openalex.org/W2894468641",
                "https://openalex.org/W2981695904",
                "https://openalex.org/W3030253100",
                "https://openalex.org/W3041706899",
                "https://openalex.org/W3158347030",
                "https://openalex.org/W3199263016",
                "https://openalex.org/W4211210290",
                "https://openalex.org/W4307884218",
                "https://openalex.org/W4312135762"
            ],
            "authors": [
                "Pongsakorn Limna",
                "Tanpat Kraiwanit",
                "Kris Jangjarat",
                "Prapasiri Klayklung",
                "Piyawatjana Chocksathaporn"
            ],
            "venue": "Journal of Applied Learning & Teaching",
            "doi": "https://doi.org/10.37074/jalt.2023.6.1.32",
            "concepts": [
                "Chatbot",
                "Nonprobability sampling",
                "Workload",
                "Focus group",
                "Perception",
                "Medical education",
                "Psychology",
                "Qualitative research",
                "Computer science",
                "World Wide Web",
                "Sociology",
                "Medicine",
                "Population",
                "Social science",
                "Demography",
                "Neuroscience",
                "Anthropology",
                "Operating system"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://journals.sfu.ca/jalt/index.php/jalt/article/download/797/593",
            "abstract": "The rapid advancement of technology has led to the integration of ChatGPT, an artificial intelligence (AI)-powered chatbot, in various sectors, including education. This research aims to explore the perceptions of educators and students on the use of ChatGPT in education during the digital era. This study adopted a qualitative research approach, using in-depth interviews to gather data. A purposive sampling technique was used to select ten educators and 15 students from different academic institutions in Krabi, Thailand. The data collected was analysed using content analysis and NVivo. The findings revealed that educators and students generally have a positive perception of using ChatGPT in education. The chatbot was perceived to be a helpful tool for providing immediate feedback, answering questions, and providing support to students. Educators noted that ChatGPT could reduce their workload by answering routine questions and enabling them to focus on higher-order tasks. However, the findings also showed some concerns regarding the use of ChatGPT in education. Participants were worried about the accuracy of information provided by the chatbot and the potential loss of personal interaction with teachers. The need for privacy and data security was also raised as a significant concern. The results of this study could help educators and policymakers make informed decisions about using ChatGPT in education."
        },
        "https://openalex.org/W4383876504": {
            "title": "Generative Artificial Intelligence in the Hospitality and Tourism Industry: Developing a Framework for Future Research",
            "openalex_id": "https://openalex.org/W4383876504",
            "cited_by_count": 180,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4400253744",
                "https://openalex.org/W40154911",
                "https://openalex.org/W3195436559",
                "https://openalex.org/W3171522609",
                "https://openalex.org/W3167248615",
                "https://openalex.org/W250174581",
                "https://openalex.org/W2373766366",
                "https://openalex.org/W2095919374",
                "https://openalex.org/W2057290649",
                "https://openalex.org/W1983398261"
            ],
            "references": [
                "https://openalex.org/W1557992034",
                "https://openalex.org/W1558857973",
                "https://openalex.org/W1733796492",
                "https://openalex.org/W1982795239",
                "https://openalex.org/W1988576693",
                "https://openalex.org/W1999138303",
                "https://openalex.org/W2034644273",
                "https://openalex.org/W2066240089",
                "https://openalex.org/W2093505263",
                "https://openalex.org/W2103197250"
            ],
            "authors": [
                "Tarik Do\u011fru",
                "Nathan Line",
                "Makarand Mody",
                "Lydia Hanks",
                "J\u00e9Anna Abbott",
                "Fulya A\u00e7ikg\u00f6z",
                "A. George Assaf",
                "Selim Bakir",
                "Adiyukh Berbekova",
                "Anil Bilgihan",
                "A M Dalton",
                "Ezgi Erkmen",
                "Mahala Geronasso",
                "Dale Gomez",
                "Sue Graves",
                "Ali \u0130skender",
                "Stanislav Ivanov",
                "Murat Kizildag",
                "Minwoo Lee",
                "Woojin Lee",
                "John Luckett",
                "Sean McGinley",
                "Fevzi Okumu\u015f",
                "\u0130rem \u00d6nder",
                "\u00d6zg\u00fcr \u00d6zdemir",
                "Hyekyung Park",
                "Abhinav Sharma",
                "Courtney Suess",
                "Muzaffer Uysal",
                "Tingting Zhang"
            ],
            "venue": "Journal of Hospitality & Tourism Research",
            "doi": "https://doi.org/10.1177/10963480231188663",
            "concepts": [
                "Tourism",
                "Stakeholder",
                "Hospitality",
                "Context (archaeology)",
                "Marketing",
                "Hospitality industry",
                "Hospitality management studies",
                "Generative grammar",
                "Business",
                "Knowledge management",
                "Economics",
                "Management",
                "Computer science",
                "Political science",
                "Paleontology",
                "Artificial intelligence",
                "Law",
                "Biology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": "https://doi.org/10.1177/10963480231188663",
            "abstract": "Generative artificial intelligence (GAI) offers important opportunities for the hospitality and tourism (HT) industry in the context of operations, design, marketing, destination management, human resources, revenue management, accounting and finance, strategic management, and beyond. However, the implementation of GAI in HT contexts comes with ethical, legal, social, and economic considerations that require careful reflection by HT firms. The purpose of this study is to offer a critical examination of the effects of GAI applications across a broad spectrum of stakeholders in the HT industry, in an effort to integrate practical and academic insights and foresights and drive academic research forward. Through the contributions of a purposeful selection of scholars, educators, and industry-practitioners, along the tenets of the stakeholder theory of the firm, this study highlights the potential challenges and opportunities of GAI and considers how academics can navigate the (research) complexities of this rapidly evolving technological phenomenon."
        },
        "https://openalex.org/W4390266739": {
            "title": "Chat-GPT; validating Technology Acceptance Model (TAM) in education sector via ubiquitous learning mechanism",
            "openalex_id": "https://openalex.org/W4390266739",
            "cited_by_count": 170,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W2959583403",
                "https://openalex.org/W2604550836",
                "https://openalex.org/W2514022516",
                "https://openalex.org/W2322404461",
                "https://openalex.org/W2296672062",
                "https://openalex.org/W2181838399",
                "https://openalex.org/W2130637816",
                "https://openalex.org/W2046492287",
                "https://openalex.org/W2027848165",
                "https://openalex.org/W2006895513"
            ],
            "references": [
                "https://openalex.org/W1410460",
                "https://openalex.org/W1518975438",
                "https://openalex.org/W1605451688",
                "https://openalex.org/W1656258454",
                "https://openalex.org/W1791469321",
                "https://openalex.org/W1791587663",
                "https://openalex.org/W1880304980",
                "https://openalex.org/W1979755600",
                "https://openalex.org/W2038360432",
                "https://openalex.org/W2038539934"
            ],
            "authors": [
                "Naveed Saif",
                "Sajid Ullah Khan",
                "Imrab Shaheen",
                "Faiz Abdullah Alotaibi",
                "Mrim M. Alnfiai",
                "Mohammad Arif"
            ],
            "venue": "Computers in Human Behavior",
            "doi": "https://doi.org/10.1016/j.chb.2023.108097",
            "concepts": [
                "Internship",
                "Technology acceptance model",
                "Structural equation modeling",
                "Context (archaeology)",
                "Psychology",
                "Anxiety",
                "Learning Management",
                "Computer science",
                "Knowledge management",
                "Virtual learning environment",
                "Medical education",
                "Usability",
                "Pedagogy",
                "Mathematics education",
                "Human\u2013computer interaction",
                "Medicine",
                "Paleontology",
                "Machine learning",
                "Psychiatry",
                "Biology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": false,
            "oa_url": null
        },
        "https://openalex.org/W4395483946": {
            "title": "Generative AI and the future of higher education: a threat to academic integrity or reformation? Evidence from multicultural perspectives",
            "openalex_id": "https://openalex.org/W4395483946",
            "cited_by_count": 169,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W636387470",
                "https://openalex.org/W4230550069",
                "https://openalex.org/W2808700786",
                "https://openalex.org/W2791883456",
                "https://openalex.org/W2726589038",
                "https://openalex.org/W2392026046",
                "https://openalex.org/W2376802113",
                "https://openalex.org/W2364219387",
                "https://openalex.org/W2081313298",
                "https://openalex.org/W1621443142"
            ],
            "references": [
                "https://openalex.org/W1980319741",
                "https://openalex.org/W2001501446",
                "https://openalex.org/W2020971864",
                "https://openalex.org/W2022740941",
                "https://openalex.org/W2123848671",
                "https://openalex.org/W2150293387",
                "https://openalex.org/W2285016219",
                "https://openalex.org/W2345458209",
                "https://openalex.org/W2769701102",
                "https://openalex.org/W2809213393"
            ],
            "authors": [
                "Abdullahi Yusuf",
                "Nasrin Pervin",
                "\u202aMarcos Rom\u00e1n-Gonz\u00e1lez\u202c"
            ],
            "venue": "International Journal of Educational Technology in Higher Education",
            "doi": "https://doi.org/10.1186/s41239-024-00453-6",
            "concepts": [
                "Multiculturalism",
                "Higher education",
                "Academic integrity",
                "Generative grammar",
                "Multicultural education",
                "Psychology",
                "Sociology",
                "Political science",
                "Engineering ethics",
                "Pedagogy",
                "Social psychology",
                "Philosophy",
                "Engineering",
                "Linguistics",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1186/s41239-024-00453-6",
            "abstract": "Abstract In recent years, higher education (HE) globally has witnessed extensive adoption of technology, particularly in teaching and research. The emergence of generative Artificial Intelligence (GenAI) further accelerates this trend. However, the increasing sophistication of GenAI tools has raised concerns about their potential to automate teaching and research processes. Despite widespread research on GenAI in various fields, there is a lack of multicultural perspectives on its impact and concerns in HE. This study addresses this gap by examining the usage, benefits, and concerns of GenAI in higher education from a multicultural standpoint. We employed an online survey that collected responses from 1217 participants across 76 countries, encompassing a broad range of gender categories, academic disciplines, geographical locations, and cultural orientations. Our findings revealed a high level of awareness and familiarity with GenAI tools among respondents. A significant portion had prior experience and expressed the intention to continue using these tools, primarily for information retrieval and text paraphrasing. The study emphasizes the importance of GenAI integration in higher education, highlighting both its potential benefits and concerns. Notably, there is a strong correlation between cultural dimensions and respondents\u2019 views on the benefits and concerns related to GenAI, including its potential as academic dishonesty and the need for ethical guidelines. We, therefore, argued that responsible use of GenAI tools can enhance learning processes, but addressing concerns may require robust policies that are responsive to cultural expectations. We discussed the findings and offered recommendations for researchers, educators, and policymakers, aiming to promote the ethical and effective integration of GenAI tools in higher education."
        },
        "https://openalex.org/W4361298749": {
            "title": "ChatGPT and Academic Research: A Review and Recommendations Based on Practical Examples",
            "openalex_id": "https://openalex.org/W4361298749",
            "cited_by_count": 168,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4309121519",
                "https://openalex.org/W4255451594",
                "https://openalex.org/W3112723203",
                "https://openalex.org/W3044958213",
                "https://openalex.org/W2943437879",
                "https://openalex.org/W2801905285",
                "https://openalex.org/W2294677930",
                "https://openalex.org/W2081408389",
                "https://openalex.org/W2028846973",
                "https://openalex.org/W1916135683"
            ],
            "references": [
                "https://openalex.org/W1960525520",
                "https://openalex.org/W4307688794",
                "https://openalex.org/W4312083290",
                "https://openalex.org/W4313262066",
                "https://openalex.org/W4313294616",
                "https://openalex.org/W4313453502",
                "https://openalex.org/W4313564992",
                "https://openalex.org/W4313592949",
                "https://openalex.org/W4315784554",
                "https://openalex.org/W4317390716"
            ],
            "authors": [
                "Md. Mizanur Rahman",
                "Harold Jan R. Terano",
                "Md Nafizur Rahman",
                "Aidin Salamzadeh",
                "Md. Saidur Rahaman"
            ],
            "venue": "Journal of Education Management and Development Studies",
            "doi": "https://doi.org/10.52631/jemds.v3i1.175",
            "concepts": [
                "Academic writing",
                "Academic community",
                "Publishing",
                "Engineering ethics",
                "Computer science",
                "Data science",
                "Management science",
                "Mathematics education",
                "Psychology",
                "Political science",
                "Library science",
                "Engineering",
                "Law"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://journals.cspc.edu.ph/index.php/jemds/article/download/175/62",
            "abstract": "In the academic world, academicians, researchers, and students have already employed Large Language Models (LLMs) such as ChatGPT to complete their various academic and non-academic tasks, including essay writing, different formal and informal speech writing, summarising literature, and generating ideas. However, yet, it is a controversial issue to use ChatGPT in academic research. Recently, its impact on academic research and publication has been scrutinized. The fundamental objective of this study is to highlight the application of ChatGPT in academic research by demonstrating a practical example with some recommendations. Data for this study was gathered using published articles, websites, blogs, and visual and numerical artefacts. We have analyzed, synthesized, and described our gathered data using an \"introductory literature review.\" The findings revealed that for the initial idea generation for academic scientific research, ChatGPT could be an effective tool. However, in the case of literature synthesis, citations, problem statements, research gaps, and data analysis, the researchers might encounter some challenges. Therefore, in these cases, researchers must be cautious about using ChatGPT in academic research. Considering the potential applications and consequences of ChatGPT, it is a must for the academic and scientific community to establish the necessary guidelines for the appropriate use of LLMs, especially ChatGPT, in research and publishing."
        },
        "https://openalex.org/W4381686136": {
            "title": "Enhancing Student Engagement: Harnessing \u201cAIED\u201d\u2019s Power in Hybrid Education\u2014A Review Analysis",
            "openalex_id": "https://openalex.org/W4381686136",
            "cited_by_count": 166,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W3080287063",
                "https://openalex.org/W2914528157",
                "https://openalex.org/W2806361344",
                "https://openalex.org/W2518037665",
                "https://openalex.org/W2368605798",
                "https://openalex.org/W2368049389",
                "https://openalex.org/W2348524959",
                "https://openalex.org/W2344935344",
                "https://openalex.org/W2141967468",
                "https://openalex.org/W1964648952"
            ],
            "references": [
                "https://openalex.org/W2886021756",
                "https://openalex.org/W2888765605",
                "https://openalex.org/W2948358298",
                "https://openalex.org/W2953136281",
                "https://openalex.org/W2972718454",
                "https://openalex.org/W2995065029",
                "https://openalex.org/W3048335295",
                "https://openalex.org/W3095777278",
                "https://openalex.org/W3107425599",
                "https://openalex.org/W3113108648"
            ],
            "authors": [
                "Amjad Almusaed",
                "Asaad Almssad",
                "\u0130brahim Yitmen",
                "Raad Z. Homod"
            ],
            "venue": "Education Sciences",
            "doi": "https://doi.org/10.3390/educsci13070632",
            "concepts": [
                "Popularity",
                "Flexibility (engineering)",
                "Blended learning",
                "Autonomy",
                "Student engagement",
                "Computer science",
                "Higher education",
                "Educational technology",
                "Mathematics education",
                "Psychology",
                "Social psychology",
                "Statistics",
                "Mathematics",
                "Political science",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.mdpi.com/2227-7102/13/7/632/pdf?version=1687687644",
            "abstract": "Hybrid learning is a complex combination of face-to-face and online learning. This model combines the use of multimedia materials with traditional classroom work. Virtual hybrid learning is employed alongside face-to-face methods. That aims to investigate using Artificial Intelligence (AI) to increase student engagement in hybrid learning settings. Educators are confronted with contemporary issues in maintaining their students\u2019 interest and motivation as the popularity of online and hybrid education continues to grow, where many educational institutions are adopting this model due to its flexibility, student-teacher engagement, and peer-to-peer interaction. AI will help students communicate, collaborate, and receive real-time feedback, all of which are challenges in education. This article examines the advantages and disadvantages of hybrid education and the optimal approaches for incorporating Artificial Intelligence (AI) in educational settings. The research findings suggest that using AI can revolutionize hybrid education, as it enhances both student and instructor autonomy while fostering a more engaging and interactive learning environment."
        },
        "https://openalex.org/W4384406891": {
            "title": "Use of ChatGPT at University as a Tool for Complex Thinking: Students\u2019 Perceived Usefulness",
            "openalex_id": "https://openalex.org/W4384406891",
            "cited_by_count": 166,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4388145910",
                "https://openalex.org/W3208008815",
                "https://openalex.org/W2981485204",
                "https://openalex.org/W2947103787",
                "https://openalex.org/W2909329017",
                "https://openalex.org/W2905101614",
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2366107444",
                "https://openalex.org/W2153247772",
                "https://openalex.org/W1976205134"
            ],
            "references": [
                "https://openalex.org/W2000031724",
                "https://openalex.org/W2063849788",
                "https://openalex.org/W2100379340",
                "https://openalex.org/W2607657495",
                "https://openalex.org/W2975893982",
                "https://openalex.org/W2979226802",
                "https://openalex.org/W2995678499",
                "https://openalex.org/W3005988318",
                "https://openalex.org/W3125976894",
                "https://openalex.org/W3137091226"
            ],
            "authors": [
                "Jos\u00e9 Mar\u00eda Romero Rodr\u00edguez",
                "Mar\u00eda Soledad",
                "Mariana Buenestado Fern\u00e1ndez",
                "Fernando Lara Lara"
            ],
            "venue": "Journal of New Approaches in Educational Research",
            "doi": "https://doi.org/10.7821/naer.2023.7.1458",
            "concepts": [
                "Habit",
                "Psychology",
                "Expectancy theory",
                "Construct (python library)",
                "Social psychology",
                "Mathematics education",
                "Applied psychology",
                "Computer science",
                "Programming language"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://naerjournal.ua.es/article/download/v12n2-8/999",
            "abstract": "Abstract Artificial intelligence (AI) and AI-based chatbots, such as ChatGPT, are transforming the approach to education. In particular, ChatGPT\u2019s potential to process large amounts of data and learn from user interactions makes it a beneficial resource for students, albeit with some reluctance from some teachers. This study aimed to explore the acceptance of ChatGPT by university students. The researchers administered an online survey to 400 Spanish university students aged 18\u201364 ( M = 21.80; SD = 6.40). The results of the methodological approach based on the UTAUT2 model for technology adoption showed that: 1) gender was not a determining variable in any construct while the experience of use was a factor conditioning a higher score on all constructs; 2) experience, performance expectancy, hedonic motivation, price value, and habit were influential in behavioral intention to use ChatGPT; 3) facilitating conditions, habit, and behavioral intention were conditioning factors in user behavior. Finally, this report discusses the findings and practical implications of the work and recommends some good uses for ChatGPT."
        },
        "https://openalex.org/W4362716434": {
            "title": "Overview of Early ChatGPT\u2019s Presence in Medical Literature: Insights From a Hybrid Literature Review by ChatGPT and Human Experts",
            "openalex_id": "https://openalex.org/W4362716434",
            "cited_by_count": 164,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W4387007686",
                "https://openalex.org/W4383501580",
                "https://openalex.org/W4382052417",
                "https://openalex.org/W4313813117",
                "https://openalex.org/W4214931137",
                "https://openalex.org/W3192088754",
                "https://openalex.org/W3176146353",
                "https://openalex.org/W3084631705",
                "https://openalex.org/W2965805642",
                "https://openalex.org/W2546519383"
            ],
            "references": [
                "https://openalex.org/W2897827470",
                "https://openalex.org/W2931716593",
                "https://openalex.org/W3028999579",
                "https://openalex.org/W3208833663",
                "https://openalex.org/W4250045111",
                "https://openalex.org/W4310781980",
                "https://openalex.org/W4312083290",
                "https://openalex.org/W4315498620",
                "https://openalex.org/W4315797044",
                "https://openalex.org/W4316671929"
            ],
            "authors": [
                "Omar Temsah",
                "Samina Khan",
                "Yazan Chaiah",
                "Abdulrahman Senjab",
                "Khalid Alhasan",
                "Amr Jamal",
                "Fadi Aljamaan",
                "Khalid H Malki",
                "Rabih Halwani",
                "Jaffar A. Al\u2010Tawfiq",
                "Mohamad\u2010Hani Temsah",
                "Ayman Al\u2010Eyadhy"
            ],
            "venue": "Cureus",
            "doi": "https://doi.org/10.7759/cureus.37281",
            "concepts": [
                "Medicine",
                "Chatbot",
                "Narrative review",
                "Engineering ethics",
                "Narrative",
                "Medical literature",
                "Health care",
                "Medical education",
                "Pathology",
                "Artificial intelligence",
                "Computer science",
                "Linguistics",
                "Philosophy",
                "Intensive care medicine",
                "Engineering",
                "Economics",
                "Economic growth"
            ],
            "type": "review",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://assets.cureus.com/uploads/review_article/pdf/147971/20230408-24888-1uel16b.pdf",
            "abstract": "ChatGPT, an artificial intelligence chatbot, has rapidly gained prominence in various domains, including medical education and healthcare literature. This hybrid narrative review, conducted collaboratively by human authors and ChatGPT, aims to summarize and synthesize the current knowledge of ChatGPT in the indexed medical literature during its initial four months. A search strategy was employed in PubMed and EuropePMC databases, yielding 65 and 110 papers, respectively. These papers focused on ChatGPT's impact on medical education, scientific research, medical writing, ethical considerations, diagnostic decision-making, automation potential, and criticisms. The findings indicate a growing body of literature on ChatGPT's applications and implications in healthcare, highlighting the need for further research to assess its effectiveness and ethical concerns."
        },
        "https://openalex.org/W4392599656": {
            "title": "Generative AI in Medical Practice: In-Depth Exploration of Privacy and Security Challenges",
            "openalex_id": "https://openalex.org/W4392599656",
            "cited_by_count": 163,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W4390718435",
                "https://openalex.org/W4390549206",
                "https://openalex.org/W4383031710",
                "https://openalex.org/W4379540039",
                "https://openalex.org/W4237784285",
                "https://openalex.org/W3211753092",
                "https://openalex.org/W3137171911",
                "https://openalex.org/W2386000789",
                "https://openalex.org/W2380075625",
                "https://openalex.org/W2374712251"
            ],
            "references": [
                "https://openalex.org/W185142374",
                "https://openalex.org/W1854663754",
                "https://openalex.org/W2037881738",
                "https://openalex.org/W2051267297",
                "https://openalex.org/W2053637704",
                "https://openalex.org/W2057993765",
                "https://openalex.org/W2069347668",
                "https://openalex.org/W2486407206",
                "https://openalex.org/W2535690855",
                "https://openalex.org/W2791319131"
            ],
            "authors": [
                "Yan Chen",
                "Pouyan Esmaeilzadeh"
            ],
            "venue": "Journal of Medical Internet Research",
            "doi": "https://doi.org/10.2196/53008",
            "concepts": [
                "Generative grammar",
                "Health care",
                "Computer science",
                "Field (mathematics)",
                "Knowledge management",
                "Data science",
                "Artificial intelligence",
                "Political science",
                "Mathematics",
                "Pure mathematics",
                "Law"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://www.jmir.org/2024/1/e53008/PDF",
            "abstract": "As advances in artificial intelligence (AI) continue to transform and revolutionize the field of medicine, understanding the potential uses of generative AI in health care becomes increasingly important. Generative AI, including models such as generative adversarial networks and large language models, shows promise in transforming medical diagnostics, research, treatment planning, and patient care. However, these data-intensive systems pose new threats to protected health information. This Viewpoint paper aims to explore various categories of generative AI in health care, including medical diagnostics, drug discovery, virtual health assistants, medical research, and clinical decision support, while identifying security and privacy threats within each phase of the life cycle of such systems (ie, data collection, model development, and implementation phases). The objectives of this study were to analyze the current state of generative AI in health care, identify opportunities and privacy and security challenges posed by integrating these technologies into existing health care infrastructure, and propose strategies for mitigating security and privacy risks. This study highlights the importance of addressing the security and privacy threats associated with generative AI in health care to ensure the safe and effective use of these systems. The findings of this study can inform the development of future generative AI systems in health care and help health care organizations better understand the potential benefits and risks associated with these systems. By examining the use cases and benefits of generative AI across diverse domains within health care, this paper contributes to theoretical discussions surrounding AI ethics, security vulnerabilities, and data privacy regulations. In addition, this study provides practical insights for stakeholders looking to adopt generative AI solutions within their organizations."
        },
        "https://openalex.org/W4392932744": {
            "title": "ChatGPT improves creative problem-solving performance in university students: An experimental study",
            "openalex_id": "https://openalex.org/W4392932744",
            "cited_by_count": 159,
            "publication_year": 2024,
            "related_works": [
                "https://openalex.org/W4200071106",
                "https://openalex.org/W3126095231",
                "https://openalex.org/W3045976901",
                "https://openalex.org/W2593155302",
                "https://openalex.org/W2378906650",
                "https://openalex.org/W2360463189",
                "https://openalex.org/W2352855287",
                "https://openalex.org/W2072812638",
                "https://openalex.org/W2067438871",
                "https://openalex.org/W2041415459"
            ],
            "references": [
                "https://openalex.org/W1526122805",
                "https://openalex.org/W1625786276",
                "https://openalex.org/W1967879932",
                "https://openalex.org/W1993870787",
                "https://openalex.org/W1996645202",
                "https://openalex.org/W1996750234",
                "https://openalex.org/W2033428167",
                "https://openalex.org/W2050406213",
                "https://openalex.org/W2052941964",
                "https://openalex.org/W2060906725"
            ],
            "authors": [
                "Marek Urban",
                "Filip D\u011bcht\u011brenko",
                "Ji\u0159\u00ed Lukavsk\u00fd",
                "Veronika Hein",
                "Filip Svacha",
                "Cyril Brom",
                "Kamila Urban"
            ],
            "venue": "Computers & Education",
            "doi": "https://doi.org/10.1016/j.compedu.2024.105031",
            "concepts": [
                "Task (project management)",
                "Metacognition",
                "Psychology",
                "Elaboration",
                "Quality (philosophy)",
                "Cognitive psychology",
                "Resolution (logic)",
                "Originality",
                "Cognition",
                "Mathematics education",
                "Computer science",
                "Artificial intelligence",
                "Social psychology",
                "Creativity",
                "Philosophy",
                "Management",
                "Epistemology",
                "Neuroscience",
                "Humanities",
                "Economics"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://osf.io/9z2tc/download"
        },
        "https://openalex.org/W4385319720": {
            "title": "Generative artificial intelligence (ChatGPT): Implications for management educators",
            "openalex_id": "https://openalex.org/W4385319720",
            "cited_by_count": 157,
            "publication_year": 2023,
            "related_works": [
                "https://openalex.org/W2748952813",
                "https://openalex.org/W2530322880",
                "https://openalex.org/W2390279801",
                "https://openalex.org/W2382290278",
                "https://openalex.org/W2380075625",
                "https://openalex.org/W2376932109",
                "https://openalex.org/W2358668433",
                "https://openalex.org/W2350741829",
                "https://openalex.org/W2001405890",
                "https://openalex.org/W1596801655"
            ],
            "references": [
                "https://openalex.org/W1583636700",
                "https://openalex.org/W2786141192",
                "https://openalex.org/W3048400690",
                "https://openalex.org/W3094104078",
                "https://openalex.org/W3094793347",
                "https://openalex.org/W3095319910",
                "https://openalex.org/W3128384299",
                "https://openalex.org/W3202582446",
                "https://openalex.org/W368346290",
                "https://openalex.org/W4214836316"
            ],
            "authors": [
                "Vanessa Ratten",
                "Paul Jones"
            ],
            "venue": "The International Journal of Management Education",
            "doi": "https://doi.org/10.1016/j.ijme.2023.100857",
            "concepts": [
                "Generative grammar",
                "Artificial intelligence",
                "Computer science",
                "Psychology"
            ],
            "type": "article",
            "language": "en",
            "is_oa": true,
            "oa_url": "https://doi.org/10.1016/j.ijme.2023.100857",
            "abstract": "ChatGPT has been one of the most talked about computer programs amongst management educators in recent weeks due to its transformative ability to change how assessments are undertaken and graded. Unlike other educational technologies that can be tracked when used, ChatGPT has superior abilities that make it virtually untraceable when used. This creates a dilemma for management educators wanting to utilise the technology whilst staying relevant but also interested in authentic learning. Thus, it is critical for management educators to quickly implement policies regarding ChatGPT and subsequent new generative artificial intelligence because of its ease of use and affordability. This article is conceptual in nature and discusses ChatGPT as a generative form of artificial intelligence that presents challenges for management educators that need to be addressed through appropriate strategies. Thereby contributing to the literature on how technological innovations can be included in curriculum design and management learning practices. Practical and managerial implications are stated that highlight the critical need to re-examine existing education practices as a way of incorporating new technological innovation that can be utilised in a beneficial way."
        }
    }
}